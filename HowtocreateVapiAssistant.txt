# Create Assistant

POST https://api.vapi.ai/assistant
Content-Type: application/json

Reference: https://docs.vapi.ai/api-reference/assistants/create

## OpenAPI Specification

```yaml
openapi: 3.1.1
info:
  title: Create Assistant
  version: endpoint_assistants.create
paths:
  /assistant:
    post:
      operationId: create
      summary: Create Assistant
      tags:
        - - subpackage_assistants
      parameters:
        - name: Authorization
          in: header
          description: Retrieve your API Key from [Dashboard](dashboard.vapi.ai).
          required: true
          schema:
            type: string
      responses:
        '201':
          description: Response with status 201
          content:
            application/json:
              schema:
                $ref: '#/components/schemas/Assistant'
      requestBody:
        content:
          application/json:
            schema:
              $ref: '#/components/schemas/CreateAssistantDTO'
components:
  schemas:
    AssemblyAiTranscriberProvider:
      type: string
      enum:
        - value: assembly-ai
    AssemblyAiTranscriberLanguage:
      type: string
      enum:
        - value: multi
        - value: en
    AssemblyAiTranscriberSpeechModel:
      type: string
      enum:
        - value: universal-streaming-english
        - value: universal-streaming-multilingual
    FallbackAssemblyAiTranscriberProvider:
      type: string
      enum:
        - value: assembly-ai
    FallbackAssemblyAiTranscriberLanguage:
      type: string
      enum:
        - value: multi
        - value: en
    FallbackAssemblyAiTranscriberSpeechModel:
      type: string
      enum:
        - value: universal-streaming-english
        - value: universal-streaming-multilingual
    FallbackAssemblyAITranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackAssemblyAiTranscriberProvider'
          description: This is the transcription provider that will be used.
        language:
          $ref: '#/components/schemas/FallbackAssemblyAiTranscriberLanguage'
          description: This is the language that will be set for the transcription.
        confidenceThreshold:
          type: number
          format: double
          description: |-
            Transcripts below this confidence threshold will be discarded.

            @default 0.4
        formatTurns:
          type: boolean
          description: |-
            This enables formatting of transcripts.

            @default true
        endOfTurnConfidenceThreshold:
          type: number
          format: double
          description: >-
            This is the end of turn confidence threshold. The minimum confidence
            that the end of turn is detected.

            Note: Only used if startSpeakingPlan.smartEndpointingPlan is not
            set.

            @min 0

            @max 1

            @default 0.7
        minEndOfTurnSilenceWhenConfident:
          type: number
          format: double
          description: >-
            This is the minimum end of turn silence when confident in
            milliseconds.

            Note: Only used if startSpeakingPlan.smartEndpointingPlan is not
            set.

            @default 160
        wordFinalizationMaxWaitTime:
          type: number
          format: double
        maxTurnSilence:
          type: number
          format: double
          description: >-
            This is the maximum turn silence time in milliseconds.

            Note: Only used if startSpeakingPlan.smartEndpointingPlan is not
            set.

            @default 400
        vadAssistedEndpointingEnabled:
          type: boolean
          description: >-
            Use VAD to assist with endpointing decisions from the transcriber.

            When enabled, transcriber endpointing will be buffered if VAD
            detects the user is still speaking, preventing premature
            turn-taking.

            When disabled, transcriber endpointing will be used immediately
            regardless of VAD state, allowing for quicker but more aggressive
            turn-taking.

            Note: Only used if startSpeakingPlan.smartEndpointingPlan is not
            set.


            @default true
        speechModel:
          $ref: '#/components/schemas/FallbackAssemblyAiTranscriberSpeechModel'
          description: >-
            This is the speech model used for the streaming session.

            Note: Keyterms prompting is not supported with multilingual
            streaming.

            @default 'universal-streaming-english'
        realtimeUrl:
          type: string
          description: The WebSocket URL that the transcriber connects to.
        wordBoost:
          type: array
          items:
            type: string
          description: Add up to 2500 characters of custom vocabulary.
        keytermsPrompt:
          type: array
          items:
            type: string
          description: >-
            Keyterms prompting improves recognition accuracy for specific words
            and phrases.

            Can include up to 100 keyterms, each up to 50 characters.

            Costs an additional $0.04/hour when enabled.
        endUtteranceSilenceThreshold:
          type: number
          format: double
          description: The duration of the end utterance silence threshold in milliseconds.
        disablePartialTranscripts:
          type: boolean
          description: >-
            Disable partial transcripts.

            Set to `true` to not receive partial transcripts. Defaults to
            `false`.
      required:
        - provider
    FallbackAzureSpeechTranscriberProvider:
      type: string
      enum:
        - value: azure
    FallbackAzureSpeechTranscriberLanguage:
      type: string
      enum:
        - value: af-ZA
        - value: am-ET
        - value: ar-AE
        - value: ar-BH
        - value: ar-DZ
        - value: ar-EG
        - value: ar-IL
        - value: ar-IQ
        - value: ar-JO
        - value: ar-KW
        - value: ar-LB
        - value: ar-LY
        - value: ar-MA
        - value: ar-OM
        - value: ar-PS
        - value: ar-QA
        - value: ar-SA
        - value: ar-SY
        - value: ar-TN
        - value: ar-YE
        - value: az-AZ
        - value: bg-BG
        - value: bn-IN
        - value: bs-BA
        - value: ca-ES
        - value: cs-CZ
        - value: cy-GB
        - value: da-DK
        - value: de-AT
        - value: de-CH
        - value: de-DE
        - value: el-GR
        - value: en-AU
        - value: en-CA
        - value: en-GB
        - value: en-GH
        - value: en-HK
        - value: en-IE
        - value: en-IN
        - value: en-KE
        - value: en-NG
        - value: en-NZ
        - value: en-PH
        - value: en-SG
        - value: en-TZ
        - value: en-US
        - value: en-ZA
        - value: es-AR
        - value: es-BO
        - value: es-CL
        - value: es-CO
        - value: es-CR
        - value: es-CU
        - value: es-DO
        - value: es-EC
        - value: es-ES
        - value: es-GQ
        - value: es-GT
        - value: es-HN
        - value: es-MX
        - value: es-NI
        - value: es-PA
        - value: es-PE
        - value: es-PR
        - value: es-PY
        - value: es-SV
        - value: es-US
        - value: es-UY
        - value: es-VE
        - value: et-EE
        - value: eu-ES
        - value: fa-IR
        - value: fi-FI
        - value: fil-PH
        - value: fr-BE
        - value: fr-CA
        - value: fr-CH
        - value: fr-FR
        - value: ga-IE
        - value: gl-ES
        - value: gu-IN
        - value: he-IL
        - value: hi-IN
        - value: hr-HR
        - value: hu-HU
        - value: hy-AM
        - value: id-ID
        - value: is-IS
        - value: it-CH
        - value: it-IT
        - value: ja-JP
        - value: jv-ID
        - value: ka-GE
        - value: kk-KZ
        - value: km-KH
        - value: kn-IN
        - value: ko-KR
        - value: lo-LA
        - value: lt-LT
        - value: lv-LV
        - value: mk-MK
        - value: ml-IN
        - value: mn-MN
        - value: mr-IN
        - value: ms-MY
        - value: mt-MT
        - value: my-MM
        - value: nb-NO
        - value: ne-NP
        - value: nl-BE
        - value: nl-NL
        - value: pa-IN
        - value: pl-PL
        - value: ps-AF
        - value: pt-BR
        - value: pt-PT
        - value: ro-RO
        - value: ru-RU
        - value: si-LK
        - value: sk-SK
        - value: sl-SI
        - value: so-SO
        - value: sq-AL
        - value: sr-RS
        - value: sv-SE
        - value: sw-KE
        - value: sw-TZ
        - value: ta-IN
        - value: te-IN
        - value: th-TH
        - value: tr-TR
        - value: uk-UA
        - value: ur-IN
        - value: uz-UZ
        - value: vi-VN
        - value: wuu-CN
        - value: yue-CN
        - value: zh-CN
        - value: zh-CN-shandong
        - value: zh-CN-sichuan
        - value: zh-HK
        - value: zh-TW
        - value: zu-ZA
    FallbackAzureSpeechTranscriberSegmentationStrategy:
      type: string
      enum:
        - value: Default
        - value: Time
        - value: Semantic
    FallbackAzureSpeechTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackAzureSpeechTranscriberProvider'
          description: This is the transcription provider that will be used.
        language:
          $ref: '#/components/schemas/FallbackAzureSpeechTranscriberLanguage'
          description: >-
            This is the language that will be set for the transcription. The
            list of languages Azure supports can be found here:
            https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=stt
        segmentationStrategy:
          $ref: >-
            #/components/schemas/FallbackAzureSpeechTranscriberSegmentationStrategy
          description: >-
            Controls how phrase boundaries are detected, enabling either simple
            time/silence heuristics or more advanced semantic segmentation.
        segmentationSilenceTimeoutMs:
          type: number
          format: double
          description: >-
            Duration of detected silence after which the service finalizes a
            phrase. Configure to adjust sensitivity to pauses in speech.
        segmentationMaximumTimeMs:
          type: number
          format: double
          description: >-
            Maximum duration a segment can reach before being cut off when using
            time-based segmentation.
      required:
        - provider
    FallbackCustomTranscriberProvider:
      type: string
      enum:
        - value: custom-transcriber
    ServerHeaders:
      type: object
      properties: {}
    BackoffPlanType:
      type: string
      enum:
        - value: fixed
        - value: exponential
    BackoffPlanExcludedStatusCodesItems:
      type: object
      properties: {}
    BackoffPlan:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/BackoffPlanType'
          description: |-
            This is the type of backoff plan to use. Defaults to fixed.

            @default fixed
        maxRetries:
          type: number
          format: double
          description: >-
            This is the maximum number of retries to attempt if the request
            fails. Defaults to 0 (no retries).


            @default 0
        baseDelaySeconds:
          type: number
          format: double
          description: >-
            This is the base delay in seconds. For linear backoff, this is the
            delay between each retry. For exponential backoff, this is the
            initial delay.
        excludedStatusCodes:
          type: array
          items:
            $ref: '#/components/schemas/BackoffPlanExcludedStatusCodesItems'
          description: >-
            This is the excluded status codes. If the response status code is in
            this list, the request will not be retried.

            By default, the request will be retried for any non-2xx status code.
      required:
        - type
        - maxRetries
        - baseDelaySeconds
    Server:
      type: object
      properties:
        timeoutSeconds:
          type: number
          format: double
          description: >-
            This is the timeout in seconds for the request. Defaults to 20
            seconds.


            @default 20
        credentialId:
          type: string
          description: The credential ID for server authentication
        staticIpAddressesEnabled:
          type: boolean
          description: >-
            If enabled, requests will originate from a static set of IPs owned
            and managed by Vapi.


            @default false
        encryptedPaths:
          type: array
          items:
            type: string
          description: >-
            This is the paths to encrypt in the request body if credentialId and
            encryptionPlan are defined.
        url:
          type: string
          description: This is where the request will be sent.
        headers:
          $ref: '#/components/schemas/ServerHeaders'
          description: >-
            These are the headers to include in the request.


            Each key-value pair represents a header name and its value.


            Note: Specifying an Authorization header here will override the
            authorization provided by the `credentialId` (if provided). This is
            an anti-pattern and should be avoided outside of edge case
            scenarios.
        backoffPlan:
          $ref: '#/components/schemas/BackoffPlan'
          description: >-
            This is the backoff plan if the request fails. Defaults to undefined
            (the request will not be retried).


            @default undefined (the request will not be retried)
    FallbackCustomTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackCustomTranscriberProvider'
          description: >-
            This is the transcription provider that will be used. Use
            `custom-transcriber` for providers that are not natively supported.
        server:
          $ref: '#/components/schemas/Server'
          description: >-
            This is where the transcription request will be sent.


            Usage:

            1. Vapi will initiate a websocket connection with `server.url`.


            2. Vapi will send an initial text frame with the sample rate.
            Format:

            ```
                {
                  "type": "start",
                  "encoding": "linear16", // 16-bit raw PCM format
                  "container": "raw",
                  "sampleRate": {{sampleRate}},
                  "channels": 2 // customer is channel 0, assistant is channel 1
                }
            ```


            3. Vapi will send the audio data in 16-bit raw PCM format as binary
            frames.


            4. You can read the messages something like this:

            ```

            ws.on('message', (data, isBinary) => {
              if (isBinary) {
                pcmBuffer = Buffer.concat([pcmBuffer, data]);
                console.log(`Received PCM data, buffer size: ${pcmBuffer.length}`);
              } else {
                console.log('Received message:', JSON.parse(data.toString()));
              }
            });

            ```


            5. You will respond with transcriptions as you have them. Format:

            ```
             {
                "type": "transcriber-response",
                "transcription": "Hello, world!",
                "channel": "customer" | "assistant"
             }
            ```
      required:
        - provider
        - server
    FallbackDeepgramTranscriberProvider:
      type: string
      enum:
        - value: deepgram
    DeepgramTranscriberModel:
      type: string
      enum:
        - value: flux-general-en
        - value: nova-3
        - value: nova-3-general
        - value: nova-3-medical
        - value: nova-2
        - value: nova-2-general
        - value: nova-2-meeting
        - value: nova-2-phonecall
        - value: nova-2-finance
        - value: nova-2-conversationalai
        - value: nova-2-voicemail
        - value: nova-2-video
        - value: nova-2-medical
        - value: nova-2-drivethru
        - value: nova-2-automotive
        - value: nova
        - value: nova-general
        - value: nova-phonecall
        - value: nova-medical
        - value: enhanced
        - value: enhanced-general
        - value: enhanced-meeting
        - value: enhanced-phonecall
        - value: enhanced-finance
        - value: base
        - value: base-general
        - value: base-meeting
        - value: base-phonecall
        - value: base-finance
        - value: base-conversationalai
        - value: base-voicemail
        - value: base-video
        - value: whisper
    DeepgramTranscriberLanguage:
      type: string
      enum:
        - value: bg
        - value: ca
        - value: cs
        - value: da
        - value: da-DK
        - value: de
        - value: de-CH
        - value: el
        - value: en
        - value: en-AU
        - value: en-GB
        - value: en-IN
        - value: en-NZ
        - value: en-US
        - value: es
        - value: es-419
        - value: es-LATAM
        - value: et
        - value: fi
        - value: fr
        - value: fr-CA
        - value: hi
        - value: hi-Latn
        - value: hu
        - value: id
        - value: it
        - value: ja
        - value: ko
        - value: ko-KR
        - value: lt
        - value: lv
        - value: ms
        - value: multi
        - value: nl
        - value: nl-BE
        - value: 'no'
        - value: pl
        - value: pt
        - value: pt-BR
        - value: ro
        - value: ru
        - value: sk
        - value: sv
        - value: sv-SE
        - value: ta
        - value: taq
        - value: th
        - value: th-TH
        - value: tr
        - value: uk
        - value: vi
        - value: zh
        - value: zh-CN
        - value: zh-Hans
        - value: zh-Hant
        - value: zh-TW
    FallbackDeepgramTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackDeepgramTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/DeepgramTranscriberModel'
          description: >-
            This is the Deepgram model that will be used. A list of models can
            be found here:
            https://developers.deepgram.com/docs/models-languages-overview
        language:
          $ref: '#/components/schemas/DeepgramTranscriberLanguage'
          description: >-
            This is the language that will be set for the transcription. The
            list of languages Deepgram supports can be found here:
            https://developers.deepgram.com/docs/models-languages-overview
        smartFormat:
          type: boolean
          description: >-
            This will be use smart format option provided by Deepgram. It's
            default disabled because it can sometimes format numbers as times
            but it's getting better.
        mipOptOut:
          type: boolean
          default: false
          description: >-
            If set to true, this will add mip_opt_out=true as a query parameter
            of all API requests. See
            https://developers.deepgram.com/docs/the-deepgram-model-improvement-partnership-program#want-to-opt-out


            This will only be used if you are using your own Deepgram API key.


            @default false
        numerals:
          type: boolean
          description: >-
            If set to true, this will cause deepgram to convert spoken numbers
            to literal numerals. For example, "my phone number is
            nine-seven-two..." would become "my phone number is 972..."


            @default false
        confidenceThreshold:
          type: number
          format: double
          description: |-
            Transcripts below this confidence threshold will be discarded.

            @default 0.4
        eagerEotThreshold:
          type: number
          format: double
          description: >-
            Eager end-of-turn confidence required to fire a eager end-of-turn
            event. Setting a value here will enable EagerEndOfTurn and
            SpeechResumed events. It is disabled by default. Only used with Flux
            models.
        eotThreshold:
          type: number
          format: double
          description: >-
            End-of-turn confidence required to finish a turn. Only used with
            Flux models.


            @default 0.7
        eotTimeoutMs:
          type: number
          format: double
          description: >-
            A turn will be finished when this much time has passed after speech,
            regardless of EOT confidence. Only used with Flux models.


            @default 5000
        keywords:
          type: array
          items:
            type: string
          description: >-
            These keywords are passed to the transcription model to help it pick
            up use-case specific words. Anything that may not be a common word,
            like your company name, should be added here.
        keyterm:
          type: array
          items:
            type: string
          description: >-
            Keyterm Prompting allows you improve Keyword Recall Rate (KRR) for
            important keyterms or phrases up to 90%.
        endpointing:
          type: number
          format: double
          description: >-
            This is the timeout after which Deepgram will send transcription on
            user silence. You can read in-depth documentation here:
            https://developers.deepgram.com/docs/endpointing.


            Here are the most important bits:

            - Defaults to 10. This is recommended for most use cases to optimize
            for latency.

            - 10 can cause some missing transcriptions since because of the
            shorter context. This mostly happens for one-word utterances. For
            those uses cases, it's recommended to try 300. It will add a bit of
            latency but the quality and reliability of the experience will be
            better.

            - If neither 10 nor 300 work, contact support@vapi.ai and we'll find
            another solution.


            @default 10
      required:
        - provider
    FallbackElevenLabsTranscriberProvider:
      type: string
      enum:
        - value: 11labs
    FallbackElevenLabsTranscriberModel0:
      type: string
      enum:
        - value: scribe_v1
        - value: scribe_v2
        - value: scribe_v2_realtime
    FallbackElevenLabsTranscriberModel:
      oneOf:
        - $ref: '#/components/schemas/FallbackElevenLabsTranscriberModel0'
    FallbackElevenLabsTranscriberLanguage:
      type: string
      enum:
        - value: aa
        - value: ab
        - value: ae
        - value: af
        - value: ak
        - value: am
        - value: an
        - value: ar
        - value: as
        - value: av
        - value: ay
        - value: az
        - value: ba
        - value: be
        - value: bg
        - value: bh
        - value: bi
        - value: bm
        - value: bn
        - value: bo
        - value: br
        - value: bs
        - value: ca
        - value: ce
        - value: ch
        - value: co
        - value: cr
        - value: cs
        - value: cu
        - value: cv
        - value: cy
        - value: da
        - value: de
        - value: dv
        - value: dz
        - value: ee
        - value: el
        - value: en
        - value: eo
        - value: es
        - value: et
        - value: eu
        - value: fa
        - value: ff
        - value: fi
        - value: fj
        - value: fo
        - value: fr
        - value: fy
        - value: ga
        - value: gd
        - value: gl
        - value: gn
        - value: gu
        - value: gv
        - value: ha
        - value: he
        - value: hi
        - value: ho
        - value: hr
        - value: ht
        - value: hu
        - value: hy
        - value: hz
        - value: ia
        - value: id
        - value: ie
        - value: ig
        - value: ii
        - value: ik
        - value: io
        - value: is
        - value: it
        - value: iu
        - value: ja
        - value: jv
        - value: ka
        - value: kg
        - value: ki
        - value: kj
        - value: kk
        - value: kl
        - value: km
        - value: kn
        - value: ko
        - value: kr
        - value: ks
        - value: ku
        - value: kv
        - value: kw
        - value: ky
        - value: la
        - value: lb
        - value: lg
        - value: li
        - value: ln
        - value: lo
        - value: lt
        - value: lu
        - value: lv
        - value: mg
        - value: mh
        - value: mi
        - value: mk
        - value: ml
        - value: mn
        - value: mr
        - value: ms
        - value: mt
        - value: my
        - value: na
        - value: nb
        - value: nd
        - value: ne
        - value: ng
        - value: nl
        - value: nn
        - value: 'no'
        - value: nr
        - value: nv
        - value: ny
        - value: oc
        - value: oj
        - value: om
        - value: or
        - value: os
        - value: pa
        - value: pi
        - value: pl
        - value: ps
        - value: pt
        - value: qu
        - value: rm
        - value: rn
        - value: ro
        - value: ru
        - value: rw
        - value: sa
        - value: sc
        - value: sd
        - value: se
        - value: sg
        - value: si
        - value: sk
        - value: sl
        - value: sm
        - value: sn
        - value: so
        - value: sq
        - value: sr
        - value: ss
        - value: st
        - value: su
        - value: sv
        - value: sw
        - value: ta
        - value: te
        - value: tg
        - value: th
        - value: ti
        - value: tk
        - value: tl
        - value: tn
        - value: to
        - value: tr
        - value: ts
        - value: tt
        - value: tw
        - value: ty
        - value: ug
        - value: uk
        - value: ur
        - value: uz
        - value: ve
        - value: vi
        - value: vo
        - value: wa
        - value: wo
        - value: xh
        - value: yi
        - value: yue
        - value: yo
        - value: za
        - value: zh
        - value: zu
    FallbackElevenLabsTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackElevenLabsTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/FallbackElevenLabsTranscriberModel'
          description: This is the model that will be used for the transcription.
        language:
          $ref: '#/components/schemas/FallbackElevenLabsTranscriberLanguage'
          description: This is the language that will be used for the transcription.
        silenceThresholdSeconds:
          type: number
          format: double
          description: >-
            This is the number of seconds of silence before VAD commits
            (0.3-3.0).
        confidenceThreshold:
          type: number
          format: double
          description: >-
            This is the VAD sensitivity (0.1-0.9, lower indicates more
            sensitive).
        minSpeechDurationMs:
          type: number
          format: double
          description: This is the minimum speech duration for VAD (50-2000ms).
        minSilenceDurationMs:
          type: number
          format: double
          description: This is the minimum silence duration for VAD (50-2000ms).
      required:
        - provider
    FallbackGladiaTranscriberProvider:
      type: string
      enum:
        - value: gladia
    FallbackGladiaTranscriberModel0:
      type: string
      enum:
        - value: fast
        - value: accurate
        - value: solaria-1
    FallbackGladiaTranscriberModel:
      oneOf:
        - $ref: '#/components/schemas/FallbackGladiaTranscriberModel0'
    FallbackGladiaTranscriberLanguageBehaviour0:
      type: string
      enum:
        - value: manual
        - value: automatic single language
        - value: automatic multiple languages
    FallbackGladiaTranscriberLanguageBehaviour:
      oneOf:
        - $ref: '#/components/schemas/FallbackGladiaTranscriberLanguageBehaviour0'
    FallbackGladiaTranscriberLanguage:
      type: string
      enum:
        - value: af
        - value: sq
        - value: am
        - value: ar
        - value: hy
        - value: as
        - value: az
        - value: ba
        - value: eu
        - value: be
        - value: bn
        - value: bs
        - value: br
        - value: bg
        - value: ca
        - value: zh
        - value: hr
        - value: cs
        - value: da
        - value: nl
        - value: en
        - value: et
        - value: fo
        - value: fi
        - value: fr
        - value: gl
        - value: ka
        - value: de
        - value: el
        - value: gu
        - value: ht
        - value: ha
        - value: haw
        - value: he
        - value: hi
        - value: hu
        - value: is
        - value: id
        - value: it
        - value: ja
        - value: jv
        - value: kn
        - value: kk
        - value: km
        - value: ko
        - value: lo
        - value: la
        - value: lv
        - value: ln
        - value: lt
        - value: lb
        - value: mk
        - value: mg
        - value: ms
        - value: ml
        - value: mt
        - value: mi
        - value: mr
        - value: mn
        - value: my
        - value: ne
        - value: 'no'
        - value: nn
        - value: oc
        - value: ps
        - value: fa
        - value: pl
        - value: pt
        - value: pa
        - value: ro
        - value: ru
        - value: sa
        - value: sr
        - value: sn
        - value: sd
        - value: si
        - value: sk
        - value: sl
        - value: so
        - value: es
        - value: su
        - value: sw
        - value: sv
        - value: tl
        - value: tg
        - value: ta
        - value: tt
        - value: te
        - value: th
        - value: bo
        - value: tr
        - value: tk
        - value: uk
        - value: ur
        - value: uz
        - value: vi
        - value: cy
        - value: yi
        - value: yo
    FallbackGladiaTranscriberLanguages:
      type: string
      enum:
        - value: af
        - value: sq
        - value: am
        - value: ar
        - value: hy
        - value: as
        - value: az
        - value: ba
        - value: eu
        - value: be
        - value: bn
        - value: bs
        - value: br
        - value: bg
        - value: ca
        - value: zh
        - value: hr
        - value: cs
        - value: da
        - value: nl
        - value: en
        - value: et
        - value: fo
        - value: fi
        - value: fr
        - value: gl
        - value: ka
        - value: de
        - value: el
        - value: gu
        - value: ht
        - value: ha
        - value: haw
        - value: he
        - value: hi
        - value: hu
        - value: is
        - value: id
        - value: it
        - value: ja
        - value: jv
        - value: kn
        - value: kk
        - value: km
        - value: ko
        - value: lo
        - value: la
        - value: lv
        - value: ln
        - value: lt
        - value: lb
        - value: mk
        - value: mg
        - value: ms
        - value: ml
        - value: mt
        - value: mi
        - value: mr
        - value: mn
        - value: my
        - value: ne
        - value: 'no'
        - value: nn
        - value: oc
        - value: ps
        - value: fa
        - value: pl
        - value: pt
        - value: pa
        - value: ro
        - value: ru
        - value: sa
        - value: sr
        - value: sn
        - value: sd
        - value: si
        - value: sk
        - value: sl
        - value: so
        - value: es
        - value: su
        - value: sw
        - value: sv
        - value: tl
        - value: tg
        - value: ta
        - value: tt
        - value: te
        - value: th
        - value: bo
        - value: tr
        - value: tk
        - value: uk
        - value: ur
        - value: uz
        - value: vi
        - value: cy
        - value: yi
        - value: yo
    GladiaVocabularyItemDTO:
      type: object
      properties:
        value:
          type: string
          description: The vocabulary word or phrase
        pronunciations:
          type: array
          items:
            type: string
          description: Alternative pronunciations for the vocabulary item
        intensity:
          type: number
          format: double
          description: Intensity for this specific vocabulary item (0.0 to 1.0)
        language:
          type: string
          description: Language code for this vocabulary item (ISO 639-1)
      required:
        - value
    GladiaCustomVocabularyConfigDtoVocabularyItems:
      oneOf:
        - type: string
        - $ref: '#/components/schemas/GladiaVocabularyItemDTO'
    GladiaCustomVocabularyConfigDTO:
      type: object
      properties:
        vocabulary:
          type: array
          items:
            $ref: >-
              #/components/schemas/GladiaCustomVocabularyConfigDtoVocabularyItems
          description: >-
            Array of vocabulary items (strings or objects with value,
            pronunciations, intensity, language)
        defaultIntensity:
          type: number
          format: double
          default: 0.5
          description: Default intensity for vocabulary items (0.0 to 1.0)
      required:
        - vocabulary
    FallbackGladiaTranscriberRegion:
      type: string
      enum:
        - value: us-west
        - value: eu-west
    FallbackGladiaTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackGladiaTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/FallbackGladiaTranscriberModel'
          description: This is the Gladia model that will be used. Default is 'fast'
        languageBehaviour:
          $ref: '#/components/schemas/FallbackGladiaTranscriberLanguageBehaviour'
          description: >-
            Defines how the transcription model detects the audio language.
            Default value is 'automatic single language'.
        language:
          $ref: '#/components/schemas/FallbackGladiaTranscriberLanguage'
          description: >-
            Defines the language to use for the transcription. Required when
            languageBehaviour is 'manual'.
        languages:
          $ref: '#/components/schemas/FallbackGladiaTranscriberLanguages'
          description: >-
            Defines the languages to use for the transcription. Required when
            languageBehaviour is 'manual'.
        transcriptionHint:
          type: string
          description: >-
            Provides a custom vocabulary to the model to improve accuracy of
            transcribing context specific words, technical terms, names, etc. If
            empty, this argument is ignored.

            ⚠️ Warning ⚠️: Please be aware that the transcription_hint field has
            a character limit of 600. If you provide a transcription_hint longer
            than 600 characters, it will be automatically truncated to meet this
            limit.
        prosody:
          type: boolean
          description: >-
            If prosody is true, you will get a transcription that can contain
            prosodies i.e. (laugh) (giggles) (malefic laugh) (toss) (music)…
            Default value is false.
        audioEnhancer:
          type: boolean
          description: >-
            If true, audio will be pre-processed to improve accuracy but latency
            will increase. Default value is false.
        confidenceThreshold:
          type: number
          format: double
          description: |-
            Transcripts below this confidence threshold will be discarded.

            @default 0.4
        endpointing:
          type: number
          format: double
          description: >-
            Endpointing time in seconds - time to wait before considering speech
            ended
        speechThreshold:
          type: number
          format: double
          description: >-
            Speech threshold - sensitivity configuration for speech detection
            (0.0 to 1.0)
        customVocabularyEnabled:
          type: boolean
          description: Enable custom vocabulary for improved accuracy
        customVocabularyConfig:
          $ref: '#/components/schemas/GladiaCustomVocabularyConfigDTO'
          description: Custom vocabulary configuration
        region:
          $ref: '#/components/schemas/FallbackGladiaTranscriberRegion'
          description: Region for processing audio (us-west or eu-west)
        receivePartialTranscripts:
          type: boolean
          description: Enable partial transcripts for low-latency streaming transcription
      required:
        - provider
    FallbackGoogleTranscriberProvider:
      type: string
      enum:
        - value: google
    FallbackGoogleTranscriberModel:
      type: string
      enum:
        - value: gemini-3-flash-preview
        - value: gemini-2.5-pro
        - value: gemini-2.5-flash
        - value: gemini-2.5-flash-lite
        - value: gemini-2.0-flash-thinking-exp
        - value: gemini-2.0-pro-exp-02-05
        - value: gemini-2.0-flash
        - value: gemini-2.0-flash-lite
        - value: gemini-2.0-flash-exp
        - value: gemini-2.0-flash-realtime-exp
        - value: gemini-1.5-flash
        - value: gemini-1.5-flash-002
        - value: gemini-1.5-pro
        - value: gemini-1.5-pro-002
        - value: gemini-1.0-pro
    FallbackGoogleTranscriberLanguage:
      type: string
      enum:
        - value: Multilingual
        - value: Arabic
        - value: Bengali
        - value: Bulgarian
        - value: Chinese
        - value: Croatian
        - value: Czech
        - value: Danish
        - value: Dutch
        - value: English
        - value: Estonian
        - value: Finnish
        - value: French
        - value: German
        - value: Greek
        - value: Hebrew
        - value: Hindi
        - value: Hungarian
        - value: Indonesian
        - value: Italian
        - value: Japanese
        - value: Korean
        - value: Latvian
        - value: Lithuanian
        - value: Norwegian
        - value: Polish
        - value: Portuguese
        - value: Romanian
        - value: Russian
        - value: Serbian
        - value: Slovak
        - value: Slovenian
        - value: Spanish
        - value: Swahili
        - value: Swedish
        - value: Thai
        - value: Turkish
        - value: Ukrainian
        - value: Vietnamese
    FallbackGoogleTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackGoogleTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/FallbackGoogleTranscriberModel'
          description: This is the model that will be used for the transcription.
        language:
          $ref: '#/components/schemas/FallbackGoogleTranscriberLanguage'
          description: This is the language that will be set for the transcription.
      required:
        - provider
    FallbackTalkscriberTranscriberProvider:
      type: string
      enum:
        - value: talkscriber
    FallbackTalkscriberTranscriberModel:
      type: string
      enum:
        - value: whisper
    FallbackTalkscriberTranscriberLanguage:
      type: string
      enum:
        - value: en
        - value: zh
        - value: de
        - value: es
        - value: ru
        - value: ko
        - value: fr
        - value: ja
        - value: pt
        - value: tr
        - value: pl
        - value: ca
        - value: nl
        - value: ar
        - value: sv
        - value: it
        - value: id
        - value: hi
        - value: fi
        - value: vi
        - value: he
        - value: uk
        - value: el
        - value: ms
        - value: cs
        - value: ro
        - value: da
        - value: hu
        - value: ta
        - value: 'no'
        - value: th
        - value: ur
        - value: hr
        - value: bg
        - value: lt
        - value: la
        - value: mi
        - value: ml
        - value: cy
        - value: sk
        - value: te
        - value: fa
        - value: lv
        - value: bn
        - value: sr
        - value: az
        - value: sl
        - value: kn
        - value: et
        - value: mk
        - value: br
        - value: eu
        - value: is
        - value: hy
        - value: ne
        - value: mn
        - value: bs
        - value: kk
        - value: sq
        - value: sw
        - value: gl
        - value: mr
        - value: pa
        - value: si
        - value: km
        - value: sn
        - value: yo
        - value: so
        - value: af
        - value: oc
        - value: ka
        - value: be
        - value: tg
        - value: sd
        - value: gu
        - value: am
        - value: yi
        - value: lo
        - value: uz
        - value: fo
        - value: ht
        - value: ps
        - value: tk
        - value: nn
        - value: mt
        - value: sa
        - value: lb
        - value: my
        - value: bo
        - value: tl
        - value: mg
        - value: as
        - value: tt
        - value: haw
        - value: ln
        - value: ha
        - value: ba
        - value: jw
        - value: su
        - value: yue
    FallbackTalkscriberTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackTalkscriberTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/FallbackTalkscriberTranscriberModel'
          description: This is the model that will be used for the transcription.
        language:
          $ref: '#/components/schemas/FallbackTalkscriberTranscriberLanguage'
          description: >-
            This is the language that will be set for the transcription. The
            list of languages Whisper supports can be found here:
            https://github.com/openai/whisper/blob/main/whisper/tokenizer.py
      required:
        - provider
    FallbackSpeechmaticsTranscriberProvider:
      type: string
      enum:
        - value: speechmatics
    FallbackSpeechmaticsTranscriberModel:
      type: string
      enum:
        - value: default
    FallbackSpeechmaticsTranscriberLanguage:
      type: string
      enum:
        - value: auto
        - value: ar
        - value: ba
        - value: eu
        - value: be
        - value: bn
        - value: bg
        - value: yue
        - value: ca
        - value: hr
        - value: cs
        - value: da
        - value: nl
        - value: en
        - value: eo
        - value: et
        - value: fi
        - value: fr
        - value: gl
        - value: de
        - value: el
        - value: he
        - value: hi
        - value: hu
        - value: id
        - value: ia
        - value: ga
        - value: it
        - value: ja
        - value: ko
        - value: lv
        - value: lt
        - value: ms
        - value: en_ms
        - value: mt
        - value: cmn
        - value: cmn_en
        - value: mr
        - value: mn
        - value: 'no'
        - value: fa
        - value: pl
        - value: pt
        - value: ro
        - value: ru
        - value: sk
        - value: sl
        - value: es
        - value: en_es
        - value: sw
        - value: sv
        - value: tl
        - value: ta
        - value: en_ta
        - value: th
        - value: tr
        - value: uk
        - value: ur
        - value: ug
        - value: vi
        - value: cy
    FallbackSpeechmaticsTranscriberOperatingPoint:
      type: string
      enum:
        - value: standard
        - value: enhanced
      default: enhanced
    FallbackSpeechmaticsTranscriberRegion:
      type: string
      enum:
        - value: eu
        - value: us
      default: eu
    SpeechmaticsCustomVocabularyItem:
      type: object
      properties:
        content:
          type: string
          description: The word or phrase to add to the custom vocabulary.
        soundsLike:
          type: array
          items:
            type: string
          description: >-
            Alternative phonetic representations of how the word might sound.
            This helps recognition when the word might be pronounced
            differently.
      required:
        - content
    FallbackSpeechmaticsTranscriberNumeralStyle:
      type: string
      enum:
        - value: written
        - value: spoken
      default: written
    FallbackSpeechmaticsTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackSpeechmaticsTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/FallbackSpeechmaticsTranscriberModel'
          description: This is the model that will be used for the transcription.
        language:
          $ref: '#/components/schemas/FallbackSpeechmaticsTranscriberLanguage'
        operatingPoint:
          $ref: '#/components/schemas/FallbackSpeechmaticsTranscriberOperatingPoint'
          description: >-
            This is the operating point for the transcription. Choose between
            `standard` for faster turnaround with strong accuracy or `enhanced`
            for highest accuracy when precision is critical.


            @default 'enhanced'
        region:
          $ref: '#/components/schemas/FallbackSpeechmaticsTranscriberRegion'
          description: >-
            This is the region for the Speechmatics API. Choose between EU
            (Europe) and US (United States) regions for lower latency and data
            sovereignty compliance.


            @default 'eu'
        enableDiarization:
          type: boolean
          default: false
          description: >-
            This enables speaker diarization, which identifies and separates
            speakers in the transcription. Essential for multi-speaker
            conversations and conference calls.


            @default false
        maxDelay:
          type: number
          format: double
          default: 3000
          description: >-
            This sets the maximum delay in milliseconds for partial transcripts.
            Balances latency and accuracy.


            @default 3000
        customVocabulary:
          type: array
          items:
            $ref: '#/components/schemas/SpeechmaticsCustomVocabularyItem'
        numeralStyle:
          $ref: '#/components/schemas/FallbackSpeechmaticsTranscriberNumeralStyle'
          description: >-
            This controls how numbers, dates, currencies, and other entities are
            formatted in the transcription output.


            @default 'written'
        endOfTurnSensitivity:
          type: number
          format: double
          default: 0.5
          description: >-
            This is the sensitivity level for end-of-turn detection, which
            determines when a speaker has finished talking. Higher values are
            more sensitive.


            @default 0.5
        removeDisfluencies:
          type: boolean
          default: false
          description: >-
            This enables removal of disfluencies (um, uh) from the transcript to
            create cleaner, more professional output.


            This is only supported for the English language transcriber.


            @default false
        minimumSpeechDuration:
          type: number
          format: double
          default: 0
          description: >-
            This is the minimum duration in seconds for speech segments. Shorter
            segments will be filtered out. Helps remove noise and improve
            accuracy.


            @default 0.0
      required:
        - provider
        - customVocabulary
    FallbackOpenAiTranscriberProvider:
      type: string
      enum:
        - value: openai
    FallbackOpenAiTranscriberModel:
      type: string
      enum:
        - value: gpt-4o-transcribe
        - value: gpt-4o-mini-transcribe
    FallbackOpenAiTranscriberLanguage:
      type: string
      enum:
        - value: af
        - value: ar
        - value: hy
        - value: az
        - value: be
        - value: bs
        - value: bg
        - value: ca
        - value: zh
        - value: hr
        - value: cs
        - value: da
        - value: nl
        - value: en
        - value: et
        - value: fi
        - value: fr
        - value: gl
        - value: de
        - value: el
        - value: he
        - value: hi
        - value: hu
        - value: is
        - value: id
        - value: it
        - value: ja
        - value: kn
        - value: kk
        - value: ko
        - value: lv
        - value: lt
        - value: mk
        - value: ms
        - value: mr
        - value: mi
        - value: ne
        - value: 'no'
        - value: fa
        - value: pl
        - value: pt
        - value: ro
        - value: ru
        - value: sr
        - value: sk
        - value: sl
        - value: es
        - value: sw
        - value: sv
        - value: tl
        - value: ta
        - value: th
        - value: tr
        - value: uk
        - value: ur
        - value: vi
        - value: cy
    FallbackOpenAITranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackOpenAiTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/FallbackOpenAiTranscriberModel'
          description: This is the model that will be used for the transcription.
        language:
          $ref: '#/components/schemas/FallbackOpenAiTranscriberLanguage'
          description: This is the language that will be set for the transcription.
      required:
        - provider
        - model
    FallbackCartesiaTranscriberProvider:
      type: string
      enum:
        - value: cartesia
    FallbackCartesiaTranscriberModel:
      type: string
      enum:
        - value: ink-whisper
    FallbackCartesiaTranscriberLanguage:
      type: string
      enum:
        - value: aa
        - value: ab
        - value: ae
        - value: af
        - value: ak
        - value: am
        - value: an
        - value: ar
        - value: as
        - value: av
        - value: ay
        - value: az
        - value: ba
        - value: be
        - value: bg
        - value: bh
        - value: bi
        - value: bm
        - value: bn
        - value: bo
        - value: br
        - value: bs
        - value: ca
        - value: ce
        - value: ch
        - value: co
        - value: cr
        - value: cs
        - value: cu
        - value: cv
        - value: cy
        - value: da
        - value: de
        - value: dv
        - value: dz
        - value: ee
        - value: el
        - value: en
        - value: eo
        - value: es
        - value: et
        - value: eu
        - value: fa
        - value: ff
        - value: fi
        - value: fj
        - value: fo
        - value: fr
        - value: fy
        - value: ga
        - value: gd
        - value: gl
        - value: gn
        - value: gu
        - value: gv
        - value: ha
        - value: he
        - value: hi
        - value: ho
        - value: hr
        - value: ht
        - value: hu
        - value: hy
        - value: hz
        - value: ia
        - value: id
        - value: ie
        - value: ig
        - value: ii
        - value: ik
        - value: io
        - value: is
        - value: it
        - value: iu
        - value: ja
        - value: jv
        - value: ka
        - value: kg
        - value: ki
        - value: kj
        - value: kk
        - value: kl
        - value: km
        - value: kn
        - value: ko
        - value: kr
        - value: ks
        - value: ku
        - value: kv
        - value: kw
        - value: ky
        - value: la
        - value: lb
        - value: lg
        - value: li
        - value: ln
        - value: lo
        - value: lt
        - value: lu
        - value: lv
        - value: mg
        - value: mh
        - value: mi
        - value: mk
        - value: ml
        - value: mn
        - value: mr
        - value: ms
        - value: mt
        - value: my
        - value: na
        - value: nb
        - value: nd
        - value: ne
        - value: ng
        - value: nl
        - value: nn
        - value: 'no'
        - value: nr
        - value: nv
        - value: ny
        - value: oc
        - value: oj
        - value: om
        - value: or
        - value: os
        - value: pa
        - value: pi
        - value: pl
        - value: ps
        - value: pt
        - value: qu
        - value: rm
        - value: rn
        - value: ro
        - value: ru
        - value: rw
        - value: sa
        - value: sc
        - value: sd
        - value: se
        - value: sg
        - value: si
        - value: sk
        - value: sl
        - value: sm
        - value: sn
        - value: so
        - value: sq
        - value: sr
        - value: ss
        - value: st
        - value: su
        - value: sv
        - value: sw
        - value: ta
        - value: te
        - value: tg
        - value: th
        - value: ti
        - value: tk
        - value: tl
        - value: tn
        - value: to
        - value: tr
        - value: ts
        - value: tt
        - value: tw
        - value: ty
        - value: ug
        - value: uk
        - value: ur
        - value: uz
        - value: ve
        - value: vi
        - value: vo
        - value: wa
        - value: wo
        - value: xh
        - value: yi
        - value: yue
        - value: yo
        - value: za
        - value: zh
        - value: zu
    FallbackCartesiaTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/FallbackCartesiaTranscriberProvider'
        model:
          $ref: '#/components/schemas/FallbackCartesiaTranscriberModel'
        language:
          $ref: '#/components/schemas/FallbackCartesiaTranscriberLanguage'
      required:
        - provider
    FallbackTranscriberPlanTranscribersItems:
      oneOf:
        - $ref: '#/components/schemas/FallbackAssemblyAITranscriber'
        - $ref: '#/components/schemas/FallbackAzureSpeechTranscriber'
        - $ref: '#/components/schemas/FallbackCustomTranscriber'
        - $ref: '#/components/schemas/FallbackDeepgramTranscriber'
        - $ref: '#/components/schemas/FallbackElevenLabsTranscriber'
        - $ref: '#/components/schemas/FallbackGladiaTranscriber'
        - $ref: '#/components/schemas/FallbackGoogleTranscriber'
        - $ref: '#/components/schemas/FallbackTalkscriberTranscriber'
        - $ref: '#/components/schemas/FallbackSpeechmaticsTranscriber'
        - $ref: '#/components/schemas/FallbackOpenAITranscriber'
        - $ref: '#/components/schemas/FallbackCartesiaTranscriber'
    FallbackTranscriberPlan:
      type: object
      properties:
        transcribers:
          type: array
          items:
            $ref: '#/components/schemas/FallbackTranscriberPlanTranscribersItems'
      required:
        - transcribers
    AssemblyAITranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/AssemblyAiTranscriberProvider'
          description: This is the transcription provider that will be used.
        language:
          $ref: '#/components/schemas/AssemblyAiTranscriberLanguage'
          description: This is the language that will be set for the transcription.
        confidenceThreshold:
          type: number
          format: double
          description: |-
            Transcripts below this confidence threshold will be discarded.

            @default 0.4
        formatTurns:
          type: boolean
          description: |-
            This enables formatting of transcripts.

            @default true
        endOfTurnConfidenceThreshold:
          type: number
          format: double
          description: >-
            This is the end of turn confidence threshold. The minimum confidence
            that the end of turn is detected.

            Note: Only used if startSpeakingPlan.smartEndpointingPlan is not
            set.

            @min 0

            @max 1

            @default 0.7
        minEndOfTurnSilenceWhenConfident:
          type: number
          format: double
          description: >-
            This is the minimum end of turn silence when confident in
            milliseconds.

            Note: Only used if startSpeakingPlan.smartEndpointingPlan is not
            set.

            @default 160
        wordFinalizationMaxWaitTime:
          type: number
          format: double
        maxTurnSilence:
          type: number
          format: double
          description: >-
            This is the maximum turn silence time in milliseconds.

            Note: Only used if startSpeakingPlan.smartEndpointingPlan is not
            set.

            @default 400
        vadAssistedEndpointingEnabled:
          type: boolean
          description: >-
            Use VAD to assist with endpointing decisions from the transcriber.

            When enabled, transcriber endpointing will be buffered if VAD
            detects the user is still speaking, preventing premature
            turn-taking.

            When disabled, transcriber endpointing will be used immediately
            regardless of VAD state, allowing for quicker but more aggressive
            turn-taking.

            Note: Only used if startSpeakingPlan.smartEndpointingPlan is not
            set.


            @default true
        speechModel:
          $ref: '#/components/schemas/AssemblyAiTranscriberSpeechModel'
          description: >-
            This is the speech model used for the streaming session.

            Note: Keyterms prompting is not supported with multilingual
            streaming.

            @default 'universal-streaming-english'
        realtimeUrl:
          type: string
          description: The WebSocket URL that the transcriber connects to.
        wordBoost:
          type: array
          items:
            type: string
          description: Add up to 2500 characters of custom vocabulary.
        keytermsPrompt:
          type: array
          items:
            type: string
          description: >-
            Keyterms prompting improves recognition accuracy for specific words
            and phrases.

            Can include up to 100 keyterms, each up to 50 characters.

            Costs an additional $0.04/hour when enabled.
        endUtteranceSilenceThreshold:
          type: number
          format: double
          description: The duration of the end utterance silence threshold in milliseconds.
        disablePartialTranscripts:
          type: boolean
          description: >-
            Disable partial transcripts.

            Set to `true` to not receive partial transcripts. Defaults to
            `false`.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
    AzureSpeechTranscriberProvider:
      type: string
      enum:
        - value: azure
    AzureSpeechTranscriberLanguage:
      type: string
      enum:
        - value: af-ZA
        - value: am-ET
        - value: ar-AE
        - value: ar-BH
        - value: ar-DZ
        - value: ar-EG
        - value: ar-IL
        - value: ar-IQ
        - value: ar-JO
        - value: ar-KW
        - value: ar-LB
        - value: ar-LY
        - value: ar-MA
        - value: ar-OM
        - value: ar-PS
        - value: ar-QA
        - value: ar-SA
        - value: ar-SY
        - value: ar-TN
        - value: ar-YE
        - value: az-AZ
        - value: bg-BG
        - value: bn-IN
        - value: bs-BA
        - value: ca-ES
        - value: cs-CZ
        - value: cy-GB
        - value: da-DK
        - value: de-AT
        - value: de-CH
        - value: de-DE
        - value: el-GR
        - value: en-AU
        - value: en-CA
        - value: en-GB
        - value: en-GH
        - value: en-HK
        - value: en-IE
        - value: en-IN
        - value: en-KE
        - value: en-NG
        - value: en-NZ
        - value: en-PH
        - value: en-SG
        - value: en-TZ
        - value: en-US
        - value: en-ZA
        - value: es-AR
        - value: es-BO
        - value: es-CL
        - value: es-CO
        - value: es-CR
        - value: es-CU
        - value: es-DO
        - value: es-EC
        - value: es-ES
        - value: es-GQ
        - value: es-GT
        - value: es-HN
        - value: es-MX
        - value: es-NI
        - value: es-PA
        - value: es-PE
        - value: es-PR
        - value: es-PY
        - value: es-SV
        - value: es-US
        - value: es-UY
        - value: es-VE
        - value: et-EE
        - value: eu-ES
        - value: fa-IR
        - value: fi-FI
        - value: fil-PH
        - value: fr-BE
        - value: fr-CA
        - value: fr-CH
        - value: fr-FR
        - value: ga-IE
        - value: gl-ES
        - value: gu-IN
        - value: he-IL
        - value: hi-IN
        - value: hr-HR
        - value: hu-HU
        - value: hy-AM
        - value: id-ID
        - value: is-IS
        - value: it-CH
        - value: it-IT
        - value: ja-JP
        - value: jv-ID
        - value: ka-GE
        - value: kk-KZ
        - value: km-KH
        - value: kn-IN
        - value: ko-KR
        - value: lo-LA
        - value: lt-LT
        - value: lv-LV
        - value: mk-MK
        - value: ml-IN
        - value: mn-MN
        - value: mr-IN
        - value: ms-MY
        - value: mt-MT
        - value: my-MM
        - value: nb-NO
        - value: ne-NP
        - value: nl-BE
        - value: nl-NL
        - value: pa-IN
        - value: pl-PL
        - value: ps-AF
        - value: pt-BR
        - value: pt-PT
        - value: ro-RO
        - value: ru-RU
        - value: si-LK
        - value: sk-SK
        - value: sl-SI
        - value: so-SO
        - value: sq-AL
        - value: sr-RS
        - value: sv-SE
        - value: sw-KE
        - value: sw-TZ
        - value: ta-IN
        - value: te-IN
        - value: th-TH
        - value: tr-TR
        - value: uk-UA
        - value: ur-IN
        - value: uz-UZ
        - value: vi-VN
        - value: wuu-CN
        - value: yue-CN
        - value: zh-CN
        - value: zh-CN-shandong
        - value: zh-CN-sichuan
        - value: zh-HK
        - value: zh-TW
        - value: zu-ZA
    AzureSpeechTranscriberSegmentationStrategy:
      type: string
      enum:
        - value: Default
        - value: Time
        - value: Semantic
    AzureSpeechTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/AzureSpeechTranscriberProvider'
          description: This is the transcription provider that will be used.
        language:
          $ref: '#/components/schemas/AzureSpeechTranscriberLanguage'
          description: >-
            This is the language that will be set for the transcription. The
            list of languages Azure supports can be found here:
            https://learn.microsoft.com/en-us/azure/ai-services/speech-service/language-support?tabs=stt
        segmentationStrategy:
          $ref: '#/components/schemas/AzureSpeechTranscriberSegmentationStrategy'
          description: >-
            Controls how phrase boundaries are detected, enabling either simple
            time/silence heuristics or more advanced semantic segmentation.
        segmentationSilenceTimeoutMs:
          type: number
          format: double
          description: >-
            Duration of detected silence after which the service finalizes a
            phrase. Configure to adjust sensitivity to pauses in speech.
        segmentationMaximumTimeMs:
          type: number
          format: double
          description: >-
            Maximum duration a segment can reach before being cut off when using
            time-based segmentation.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
    CustomTranscriberProvider:
      type: string
      enum:
        - value: custom-transcriber
    CustomTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/CustomTranscriberProvider'
          description: >-
            This is the transcription provider that will be used. Use
            `custom-transcriber` for providers that are not natively supported.
        server:
          $ref: '#/components/schemas/Server'
          description: >-
            This is where the transcription request will be sent.


            Usage:

            1. Vapi will initiate a websocket connection with `server.url`.


            2. Vapi will send an initial text frame with the sample rate.
            Format:

            ```
                {
                  "type": "start",
                  "encoding": "linear16", // 16-bit raw PCM format
                  "container": "raw",
                  "sampleRate": {{sampleRate}},
                  "channels": 2 // customer is channel 0, assistant is channel 1
                }
            ```


            3. Vapi will send the audio data in 16-bit raw PCM format as binary
            frames.


            4. You can read the messages something like this:

            ```

            ws.on('message', (data, isBinary) => {
              if (isBinary) {
                pcmBuffer = Buffer.concat([pcmBuffer, data]);
                console.log(`Received PCM data, buffer size: ${pcmBuffer.length}`);
              } else {
                console.log('Received message:', JSON.parse(data.toString()));
              }
            });

            ```


            5. You will respond with transcriptions as you have them. Format:

            ```
             {
                "type": "transcriber-response",
                "transcription": "Hello, world!",
                "channel": "customer" | "assistant"
             }
            ```
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
        - server
    DeepgramTranscriberProvider:
      type: string
      enum:
        - value: deepgram
    DeepgramTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/DeepgramTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/DeepgramTranscriberModel'
          description: >-
            This is the Deepgram model that will be used. A list of models can
            be found here:
            https://developers.deepgram.com/docs/models-languages-overview
        language:
          $ref: '#/components/schemas/DeepgramTranscriberLanguage'
          description: >-
            This is the language that will be set for the transcription. The
            list of languages Deepgram supports can be found here:
            https://developers.deepgram.com/docs/models-languages-overview
        smartFormat:
          type: boolean
          description: >-
            This will be use smart format option provided by Deepgram. It's
            default disabled because it can sometimes format numbers as times
            but it's getting better.
        mipOptOut:
          type: boolean
          default: false
          description: >-
            If set to true, this will add mip_opt_out=true as a query parameter
            of all API requests. See
            https://developers.deepgram.com/docs/the-deepgram-model-improvement-partnership-program#want-to-opt-out


            This will only be used if you are using your own Deepgram API key.


            @default false
        numerals:
          type: boolean
          description: >-
            If set to true, this will cause deepgram to convert spoken numbers
            to literal numerals. For example, "my phone number is
            nine-seven-two..." would become "my phone number is 972..."


            @default false
        confidenceThreshold:
          type: number
          format: double
          description: |-
            Transcripts below this confidence threshold will be discarded.

            @default 0.4
        eagerEotThreshold:
          type: number
          format: double
          description: >-
            Eager end-of-turn confidence required to fire a eager end-of-turn
            event. Setting a value here will enable EagerEndOfTurn and
            SpeechResumed events. It is disabled by default. Only used with Flux
            models.
        eotThreshold:
          type: number
          format: double
          description: >-
            End-of-turn confidence required to finish a turn. Only used with
            Flux models.


            @default 0.7
        eotTimeoutMs:
          type: number
          format: double
          description: >-
            A turn will be finished when this much time has passed after speech,
            regardless of EOT confidence. Only used with Flux models.


            @default 5000
        keywords:
          type: array
          items:
            type: string
          description: >-
            These keywords are passed to the transcription model to help it pick
            up use-case specific words. Anything that may not be a common word,
            like your company name, should be added here.
        keyterm:
          type: array
          items:
            type: string
          description: >-
            Keyterm Prompting allows you improve Keyword Recall Rate (KRR) for
            important keyterms or phrases up to 90%.
        endpointing:
          type: number
          format: double
          description: >-
            This is the timeout after which Deepgram will send transcription on
            user silence. You can read in-depth documentation here:
            https://developers.deepgram.com/docs/endpointing.


            Here are the most important bits:

            - Defaults to 10. This is recommended for most use cases to optimize
            for latency.

            - 10 can cause some missing transcriptions since because of the
            shorter context. This mostly happens for one-word utterances. For
            those uses cases, it's recommended to try 300. It will add a bit of
            latency but the quality and reliability of the experience will be
            better.

            - If neither 10 nor 300 work, contact support@vapi.ai and we'll find
            another solution.


            @default 10
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
    ElevenLabsTranscriberProvider:
      type: string
      enum:
        - value: 11labs
    ElevenLabsTranscriberModel0:
      type: string
      enum:
        - value: scribe_v1
        - value: scribe_v2
        - value: scribe_v2_realtime
    ElevenLabsTranscriberModel:
      oneOf:
        - $ref: '#/components/schemas/ElevenLabsTranscriberModel0'
    ElevenLabsTranscriberLanguage:
      type: string
      enum:
        - value: aa
        - value: ab
        - value: ae
        - value: af
        - value: ak
        - value: am
        - value: an
        - value: ar
        - value: as
        - value: av
        - value: ay
        - value: az
        - value: ba
        - value: be
        - value: bg
        - value: bh
        - value: bi
        - value: bm
        - value: bn
        - value: bo
        - value: br
        - value: bs
        - value: ca
        - value: ce
        - value: ch
        - value: co
        - value: cr
        - value: cs
        - value: cu
        - value: cv
        - value: cy
        - value: da
        - value: de
        - value: dv
        - value: dz
        - value: ee
        - value: el
        - value: en
        - value: eo
        - value: es
        - value: et
        - value: eu
        - value: fa
        - value: ff
        - value: fi
        - value: fj
        - value: fo
        - value: fr
        - value: fy
        - value: ga
        - value: gd
        - value: gl
        - value: gn
        - value: gu
        - value: gv
        - value: ha
        - value: he
        - value: hi
        - value: ho
        - value: hr
        - value: ht
        - value: hu
        - value: hy
        - value: hz
        - value: ia
        - value: id
        - value: ie
        - value: ig
        - value: ii
        - value: ik
        - value: io
        - value: is
        - value: it
        - value: iu
        - value: ja
        - value: jv
        - value: ka
        - value: kg
        - value: ki
        - value: kj
        - value: kk
        - value: kl
        - value: km
        - value: kn
        - value: ko
        - value: kr
        - value: ks
        - value: ku
        - value: kv
        - value: kw
        - value: ky
        - value: la
        - value: lb
        - value: lg
        - value: li
        - value: ln
        - value: lo
        - value: lt
        - value: lu
        - value: lv
        - value: mg
        - value: mh
        - value: mi
        - value: mk
        - value: ml
        - value: mn
        - value: mr
        - value: ms
        - value: mt
        - value: my
        - value: na
        - value: nb
        - value: nd
        - value: ne
        - value: ng
        - value: nl
        - value: nn
        - value: 'no'
        - value: nr
        - value: nv
        - value: ny
        - value: oc
        - value: oj
        - value: om
        - value: or
        - value: os
        - value: pa
        - value: pi
        - value: pl
        - value: ps
        - value: pt
        - value: qu
        - value: rm
        - value: rn
        - value: ro
        - value: ru
        - value: rw
        - value: sa
        - value: sc
        - value: sd
        - value: se
        - value: sg
        - value: si
        - value: sk
        - value: sl
        - value: sm
        - value: sn
        - value: so
        - value: sq
        - value: sr
        - value: ss
        - value: st
        - value: su
        - value: sv
        - value: sw
        - value: ta
        - value: te
        - value: tg
        - value: th
        - value: ti
        - value: tk
        - value: tl
        - value: tn
        - value: to
        - value: tr
        - value: ts
        - value: tt
        - value: tw
        - value: ty
        - value: ug
        - value: uk
        - value: ur
        - value: uz
        - value: ve
        - value: vi
        - value: vo
        - value: wa
        - value: wo
        - value: xh
        - value: yi
        - value: yue
        - value: yo
        - value: za
        - value: zh
        - value: zu
    ElevenLabsTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/ElevenLabsTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/ElevenLabsTranscriberModel'
          description: This is the model that will be used for the transcription.
        language:
          $ref: '#/components/schemas/ElevenLabsTranscriberLanguage'
          description: This is the language that will be used for the transcription.
        silenceThresholdSeconds:
          type: number
          format: double
          description: >-
            This is the number of seconds of silence before VAD commits
            (0.3-3.0).
        confidenceThreshold:
          type: number
          format: double
          description: >-
            This is the VAD sensitivity (0.1-0.9, lower indicates more
            sensitive).
        minSpeechDurationMs:
          type: number
          format: double
          description: This is the minimum speech duration for VAD (50-2000ms).
        minSilenceDurationMs:
          type: number
          format: double
          description: This is the minimum silence duration for VAD (50-2000ms).
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
    GladiaTranscriberProvider:
      type: string
      enum:
        - value: gladia
    GladiaTranscriberModel0:
      type: string
      enum:
        - value: fast
        - value: accurate
        - value: solaria-1
    GladiaTranscriberModel:
      oneOf:
        - $ref: '#/components/schemas/GladiaTranscriberModel0'
    GladiaTranscriberLanguageBehaviour0:
      type: string
      enum:
        - value: manual
        - value: automatic single language
        - value: automatic multiple languages
    GladiaTranscriberLanguageBehaviour:
      oneOf:
        - $ref: '#/components/schemas/GladiaTranscriberLanguageBehaviour0'
    GladiaTranscriberLanguage:
      type: string
      enum:
        - value: af
        - value: sq
        - value: am
        - value: ar
        - value: hy
        - value: as
        - value: az
        - value: ba
        - value: eu
        - value: be
        - value: bn
        - value: bs
        - value: br
        - value: bg
        - value: ca
        - value: zh
        - value: hr
        - value: cs
        - value: da
        - value: nl
        - value: en
        - value: et
        - value: fo
        - value: fi
        - value: fr
        - value: gl
        - value: ka
        - value: de
        - value: el
        - value: gu
        - value: ht
        - value: ha
        - value: haw
        - value: he
        - value: hi
        - value: hu
        - value: is
        - value: id
        - value: it
        - value: ja
        - value: jv
        - value: kn
        - value: kk
        - value: km
        - value: ko
        - value: lo
        - value: la
        - value: lv
        - value: ln
        - value: lt
        - value: lb
        - value: mk
        - value: mg
        - value: ms
        - value: ml
        - value: mt
        - value: mi
        - value: mr
        - value: mn
        - value: my
        - value: ne
        - value: 'no'
        - value: nn
        - value: oc
        - value: ps
        - value: fa
        - value: pl
        - value: pt
        - value: pa
        - value: ro
        - value: ru
        - value: sa
        - value: sr
        - value: sn
        - value: sd
        - value: si
        - value: sk
        - value: sl
        - value: so
        - value: es
        - value: su
        - value: sw
        - value: sv
        - value: tl
        - value: tg
        - value: ta
        - value: tt
        - value: te
        - value: th
        - value: bo
        - value: tr
        - value: tk
        - value: uk
        - value: ur
        - value: uz
        - value: vi
        - value: cy
        - value: yi
        - value: yo
    GladiaTranscriberLanguages:
      type: string
      enum:
        - value: af
        - value: sq
        - value: am
        - value: ar
        - value: hy
        - value: as
        - value: az
        - value: ba
        - value: eu
        - value: be
        - value: bn
        - value: bs
        - value: br
        - value: bg
        - value: ca
        - value: zh
        - value: hr
        - value: cs
        - value: da
        - value: nl
        - value: en
        - value: et
        - value: fo
        - value: fi
        - value: fr
        - value: gl
        - value: ka
        - value: de
        - value: el
        - value: gu
        - value: ht
        - value: ha
        - value: haw
        - value: he
        - value: hi
        - value: hu
        - value: is
        - value: id
        - value: it
        - value: ja
        - value: jv
        - value: kn
        - value: kk
        - value: km
        - value: ko
        - value: lo
        - value: la
        - value: lv
        - value: ln
        - value: lt
        - value: lb
        - value: mk
        - value: mg
        - value: ms
        - value: ml
        - value: mt
        - value: mi
        - value: mr
        - value: mn
        - value: my
        - value: ne
        - value: 'no'
        - value: nn
        - value: oc
        - value: ps
        - value: fa
        - value: pl
        - value: pt
        - value: pa
        - value: ro
        - value: ru
        - value: sa
        - value: sr
        - value: sn
        - value: sd
        - value: si
        - value: sk
        - value: sl
        - value: so
        - value: es
        - value: su
        - value: sw
        - value: sv
        - value: tl
        - value: tg
        - value: ta
        - value: tt
        - value: te
        - value: th
        - value: bo
        - value: tr
        - value: tk
        - value: uk
        - value: ur
        - value: uz
        - value: vi
        - value: cy
        - value: yi
        - value: yo
    GladiaTranscriberRegion:
      type: string
      enum:
        - value: us-west
        - value: eu-west
    GladiaTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/GladiaTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/GladiaTranscriberModel'
          description: This is the Gladia model that will be used. Default is 'fast'
        languageBehaviour:
          $ref: '#/components/schemas/GladiaTranscriberLanguageBehaviour'
          description: >-
            Defines how the transcription model detects the audio language.
            Default value is 'automatic single language'.
        language:
          $ref: '#/components/schemas/GladiaTranscriberLanguage'
          description: >-
            Defines the language to use for the transcription. Required when
            languageBehaviour is 'manual'.
        languages:
          $ref: '#/components/schemas/GladiaTranscriberLanguages'
          description: >-
            Defines the languages to use for the transcription. Required when
            languageBehaviour is 'manual'.
        transcriptionHint:
          type: string
          description: >-
            Provides a custom vocabulary to the model to improve accuracy of
            transcribing context specific words, technical terms, names, etc. If
            empty, this argument is ignored.

            ⚠️ Warning ⚠️: Please be aware that the transcription_hint field has
            a character limit of 600. If you provide a transcription_hint longer
            than 600 characters, it will be automatically truncated to meet this
            limit.
        prosody:
          type: boolean
          description: >-
            If prosody is true, you will get a transcription that can contain
            prosodies i.e. (laugh) (giggles) (malefic laugh) (toss) (music)…
            Default value is false.
        audioEnhancer:
          type: boolean
          description: >-
            If true, audio will be pre-processed to improve accuracy but latency
            will increase. Default value is false.
        confidenceThreshold:
          type: number
          format: double
          description: |-
            Transcripts below this confidence threshold will be discarded.

            @default 0.4
        endpointing:
          type: number
          format: double
          description: >-
            Endpointing time in seconds - time to wait before considering speech
            ended
        speechThreshold:
          type: number
          format: double
          description: >-
            Speech threshold - sensitivity configuration for speech detection
            (0.0 to 1.0)
        customVocabularyEnabled:
          type: boolean
          description: Enable custom vocabulary for improved accuracy
        customVocabularyConfig:
          $ref: '#/components/schemas/GladiaCustomVocabularyConfigDTO'
          description: Custom vocabulary configuration
        region:
          $ref: '#/components/schemas/GladiaTranscriberRegion'
          description: Region for processing audio (us-west or eu-west)
        receivePartialTranscripts:
          type: boolean
          description: Enable partial transcripts for low-latency streaming transcription
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
    GoogleTranscriberProvider:
      type: string
      enum:
        - value: google
    GoogleTranscriberModel:
      type: string
      enum:
        - value: gemini-3-flash-preview
        - value: gemini-2.5-pro
        - value: gemini-2.5-flash
        - value: gemini-2.5-flash-lite
        - value: gemini-2.0-flash-thinking-exp
        - value: gemini-2.0-pro-exp-02-05
        - value: gemini-2.0-flash
        - value: gemini-2.0-flash-lite
        - value: gemini-2.0-flash-exp
        - value: gemini-2.0-flash-realtime-exp
        - value: gemini-1.5-flash
        - value: gemini-1.5-flash-002
        - value: gemini-1.5-pro
        - value: gemini-1.5-pro-002
        - value: gemini-1.0-pro
    GoogleTranscriberLanguage:
      type: string
      enum:
        - value: Multilingual
        - value: Arabic
        - value: Bengali
        - value: Bulgarian
        - value: Chinese
        - value: Croatian
        - value: Czech
        - value: Danish
        - value: Dutch
        - value: English
        - value: Estonian
        - value: Finnish
        - value: French
        - value: German
        - value: Greek
        - value: Hebrew
        - value: Hindi
        - value: Hungarian
        - value: Indonesian
        - value: Italian
        - value: Japanese
        - value: Korean
        - value: Latvian
        - value: Lithuanian
        - value: Norwegian
        - value: Polish
        - value: Portuguese
        - value: Romanian
        - value: Russian
        - value: Serbian
        - value: Slovak
        - value: Slovenian
        - value: Spanish
        - value: Swahili
        - value: Swedish
        - value: Thai
        - value: Turkish
        - value: Ukrainian
        - value: Vietnamese
    GoogleTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/GoogleTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/GoogleTranscriberModel'
          description: This is the model that will be used for the transcription.
        language:
          $ref: '#/components/schemas/GoogleTranscriberLanguage'
          description: This is the language that will be set for the transcription.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
    SpeechmaticsTranscriberProvider:
      type: string
      enum:
        - value: speechmatics
    SpeechmaticsTranscriberModel:
      type: string
      enum:
        - value: default
    SpeechmaticsTranscriberLanguage:
      type: string
      enum:
        - value: auto
        - value: ar
        - value: ba
        - value: eu
        - value: be
        - value: bn
        - value: bg
        - value: yue
        - value: ca
        - value: hr
        - value: cs
        - value: da
        - value: nl
        - value: en
        - value: eo
        - value: et
        - value: fi
        - value: fr
        - value: gl
        - value: de
        - value: el
        - value: he
        - value: hi
        - value: hu
        - value: id
        - value: ia
        - value: ga
        - value: it
        - value: ja
        - value: ko
        - value: lv
        - value: lt
        - value: ms
        - value: en_ms
        - value: mt
        - value: cmn
        - value: cmn_en
        - value: mr
        - value: mn
        - value: 'no'
        - value: fa
        - value: pl
        - value: pt
        - value: ro
        - value: ru
        - value: sk
        - value: sl
        - value: es
        - value: en_es
        - value: sw
        - value: sv
        - value: tl
        - value: ta
        - value: en_ta
        - value: th
        - value: tr
        - value: uk
        - value: ur
        - value: ug
        - value: vi
        - value: cy
    SpeechmaticsTranscriberOperatingPoint:
      type: string
      enum:
        - value: standard
        - value: enhanced
      default: enhanced
    SpeechmaticsTranscriberRegion:
      type: string
      enum:
        - value: eu
        - value: us
      default: eu
    SpeechmaticsTranscriberNumeralStyle:
      type: string
      enum:
        - value: written
        - value: spoken
      default: written
    SpeechmaticsTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/SpeechmaticsTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/SpeechmaticsTranscriberModel'
          description: This is the model that will be used for the transcription.
        language:
          $ref: '#/components/schemas/SpeechmaticsTranscriberLanguage'
        operatingPoint:
          $ref: '#/components/schemas/SpeechmaticsTranscriberOperatingPoint'
          description: >-
            This is the operating point for the transcription. Choose between
            `standard` for faster turnaround with strong accuracy or `enhanced`
            for highest accuracy when precision is critical.


            @default 'enhanced'
        region:
          $ref: '#/components/schemas/SpeechmaticsTranscriberRegion'
          description: >-
            This is the region for the Speechmatics API. Choose between EU
            (Europe) and US (United States) regions for lower latency and data
            sovereignty compliance.


            @default 'eu'
        enableDiarization:
          type: boolean
          default: false
          description: >-
            This enables speaker diarization, which identifies and separates
            speakers in the transcription. Essential for multi-speaker
            conversations and conference calls.


            @default false
        maxDelay:
          type: number
          format: double
          default: 3000
          description: >-
            This sets the maximum delay in milliseconds for partial transcripts.
            Balances latency and accuracy.


            @default 3000
        customVocabulary:
          type: array
          items:
            $ref: '#/components/schemas/SpeechmaticsCustomVocabularyItem'
        numeralStyle:
          $ref: '#/components/schemas/SpeechmaticsTranscriberNumeralStyle'
          description: >-
            This controls how numbers, dates, currencies, and other entities are
            formatted in the transcription output.


            @default 'written'
        endOfTurnSensitivity:
          type: number
          format: double
          default: 0.5
          description: >-
            This is the sensitivity level for end-of-turn detection, which
            determines when a speaker has finished talking. Higher values are
            more sensitive.


            @default 0.5
        removeDisfluencies:
          type: boolean
          default: false
          description: >-
            This enables removal of disfluencies (um, uh) from the transcript to
            create cleaner, more professional output.


            This is only supported for the English language transcriber.


            @default false
        minimumSpeechDuration:
          type: number
          format: double
          default: 0
          description: >-
            This is the minimum duration in seconds for speech segments. Shorter
            segments will be filtered out. Helps remove noise and improve
            accuracy.


            @default 0.0
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
        - customVocabulary
    TalkscriberTranscriberProvider:
      type: string
      enum:
        - value: talkscriber
    TalkscriberTranscriberModel:
      type: string
      enum:
        - value: whisper
    TalkscriberTranscriberLanguage:
      type: string
      enum:
        - value: en
        - value: zh
        - value: de
        - value: es
        - value: ru
        - value: ko
        - value: fr
        - value: ja
        - value: pt
        - value: tr
        - value: pl
        - value: ca
        - value: nl
        - value: ar
        - value: sv
        - value: it
        - value: id
        - value: hi
        - value: fi
        - value: vi
        - value: he
        - value: uk
        - value: el
        - value: ms
        - value: cs
        - value: ro
        - value: da
        - value: hu
        - value: ta
        - value: 'no'
        - value: th
        - value: ur
        - value: hr
        - value: bg
        - value: lt
        - value: la
        - value: mi
        - value: ml
        - value: cy
        - value: sk
        - value: te
        - value: fa
        - value: lv
        - value: bn
        - value: sr
        - value: az
        - value: sl
        - value: kn
        - value: et
        - value: mk
        - value: br
        - value: eu
        - value: is
        - value: hy
        - value: ne
        - value: mn
        - value: bs
        - value: kk
        - value: sq
        - value: sw
        - value: gl
        - value: mr
        - value: pa
        - value: si
        - value: km
        - value: sn
        - value: yo
        - value: so
        - value: af
        - value: oc
        - value: ka
        - value: be
        - value: tg
        - value: sd
        - value: gu
        - value: am
        - value: yi
        - value: lo
        - value: uz
        - value: fo
        - value: ht
        - value: ps
        - value: tk
        - value: nn
        - value: mt
        - value: sa
        - value: lb
        - value: my
        - value: bo
        - value: tl
        - value: mg
        - value: as
        - value: tt
        - value: haw
        - value: ln
        - value: ha
        - value: ba
        - value: jw
        - value: su
        - value: yue
    TalkscriberTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/TalkscriberTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/TalkscriberTranscriberModel'
          description: This is the model that will be used for the transcription.
        language:
          $ref: '#/components/schemas/TalkscriberTranscriberLanguage'
          description: >-
            This is the language that will be set for the transcription. The
            list of languages Whisper supports can be found here:
            https://github.com/openai/whisper/blob/main/whisper/tokenizer.py
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
    OpenAiTranscriberProvider:
      type: string
      enum:
        - value: openai
    OpenAiTranscriberModel:
      type: string
      enum:
        - value: gpt-4o-transcribe
        - value: gpt-4o-mini-transcribe
    OpenAiTranscriberLanguage:
      type: string
      enum:
        - value: af
        - value: ar
        - value: hy
        - value: az
        - value: be
        - value: bs
        - value: bg
        - value: ca
        - value: zh
        - value: hr
        - value: cs
        - value: da
        - value: nl
        - value: en
        - value: et
        - value: fi
        - value: fr
        - value: gl
        - value: de
        - value: el
        - value: he
        - value: hi
        - value: hu
        - value: is
        - value: id
        - value: it
        - value: ja
        - value: kn
        - value: kk
        - value: ko
        - value: lv
        - value: lt
        - value: mk
        - value: ms
        - value: mr
        - value: mi
        - value: ne
        - value: 'no'
        - value: fa
        - value: pl
        - value: pt
        - value: ro
        - value: ru
        - value: sr
        - value: sk
        - value: sl
        - value: es
        - value: sw
        - value: sv
        - value: tl
        - value: ta
        - value: th
        - value: tr
        - value: uk
        - value: ur
        - value: vi
        - value: cy
    OpenAITranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/OpenAiTranscriberProvider'
          description: This is the transcription provider that will be used.
        model:
          $ref: '#/components/schemas/OpenAiTranscriberModel'
          description: This is the model that will be used for the transcription.
        language:
          $ref: '#/components/schemas/OpenAiTranscriberLanguage'
          description: This is the language that will be set for the transcription.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
        - model
    CartesiaTranscriberProvider:
      type: string
      enum:
        - value: cartesia
    CartesiaTranscriberModel:
      type: string
      enum:
        - value: ink-whisper
    CartesiaTranscriberLanguage:
      type: string
      enum:
        - value: aa
        - value: ab
        - value: ae
        - value: af
        - value: ak
        - value: am
        - value: an
        - value: ar
        - value: as
        - value: av
        - value: ay
        - value: az
        - value: ba
        - value: be
        - value: bg
        - value: bh
        - value: bi
        - value: bm
        - value: bn
        - value: bo
        - value: br
        - value: bs
        - value: ca
        - value: ce
        - value: ch
        - value: co
        - value: cr
        - value: cs
        - value: cu
        - value: cv
        - value: cy
        - value: da
        - value: de
        - value: dv
        - value: dz
        - value: ee
        - value: el
        - value: en
        - value: eo
        - value: es
        - value: et
        - value: eu
        - value: fa
        - value: ff
        - value: fi
        - value: fj
        - value: fo
        - value: fr
        - value: fy
        - value: ga
        - value: gd
        - value: gl
        - value: gn
        - value: gu
        - value: gv
        - value: ha
        - value: he
        - value: hi
        - value: ho
        - value: hr
        - value: ht
        - value: hu
        - value: hy
        - value: hz
        - value: ia
        - value: id
        - value: ie
        - value: ig
        - value: ii
        - value: ik
        - value: io
        - value: is
        - value: it
        - value: iu
        - value: ja
        - value: jv
        - value: ka
        - value: kg
        - value: ki
        - value: kj
        - value: kk
        - value: kl
        - value: km
        - value: kn
        - value: ko
        - value: kr
        - value: ks
        - value: ku
        - value: kv
        - value: kw
        - value: ky
        - value: la
        - value: lb
        - value: lg
        - value: li
        - value: ln
        - value: lo
        - value: lt
        - value: lu
        - value: lv
        - value: mg
        - value: mh
        - value: mi
        - value: mk
        - value: ml
        - value: mn
        - value: mr
        - value: ms
        - value: mt
        - value: my
        - value: na
        - value: nb
        - value: nd
        - value: ne
        - value: ng
        - value: nl
        - value: nn
        - value: 'no'
        - value: nr
        - value: nv
        - value: ny
        - value: oc
        - value: oj
        - value: om
        - value: or
        - value: os
        - value: pa
        - value: pi
        - value: pl
        - value: ps
        - value: pt
        - value: qu
        - value: rm
        - value: rn
        - value: ro
        - value: ru
        - value: rw
        - value: sa
        - value: sc
        - value: sd
        - value: se
        - value: sg
        - value: si
        - value: sk
        - value: sl
        - value: sm
        - value: sn
        - value: so
        - value: sq
        - value: sr
        - value: ss
        - value: st
        - value: su
        - value: sv
        - value: sw
        - value: ta
        - value: te
        - value: tg
        - value: th
        - value: ti
        - value: tk
        - value: tl
        - value: tn
        - value: to
        - value: tr
        - value: ts
        - value: tt
        - value: tw
        - value: ty
        - value: ug
        - value: uk
        - value: ur
        - value: uz
        - value: ve
        - value: vi
        - value: vo
        - value: wa
        - value: wo
        - value: xh
        - value: yi
        - value: yue
        - value: yo
        - value: za
        - value: zh
        - value: zu
    CartesiaTranscriber:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/CartesiaTranscriberProvider'
        model:
          $ref: '#/components/schemas/CartesiaTranscriberModel'
        language:
          $ref: '#/components/schemas/CartesiaTranscriberLanguage'
        fallbackPlan:
          $ref: '#/components/schemas/FallbackTranscriberPlan'
          description: >-
            This is the plan for transcriber provider fallbacks in the event
            that the primary transcriber provider fails.
      required:
        - provider
    CreateAssistantDtoTranscriber:
      oneOf:
        - $ref: '#/components/schemas/AssemblyAITranscriber'
        - $ref: '#/components/schemas/AzureSpeechTranscriber'
        - $ref: '#/components/schemas/CustomTranscriber'
        - $ref: '#/components/schemas/DeepgramTranscriber'
        - $ref: '#/components/schemas/ElevenLabsTranscriber'
        - $ref: '#/components/schemas/GladiaTranscriber'
        - $ref: '#/components/schemas/GoogleTranscriber'
        - $ref: '#/components/schemas/SpeechmaticsTranscriber'
        - $ref: '#/components/schemas/TalkscriberTranscriber'
        - $ref: '#/components/schemas/OpenAITranscriber'
        - $ref: '#/components/schemas/CartesiaTranscriber'
    OpenAiMessageRole:
      type: string
      enum:
        - value: assistant
        - value: function
        - value: user
        - value: system
        - value: tool
    OpenAIMessage:
      type: object
      properties:
        content:
          type:
            - string
            - 'null'
        role:
          $ref: '#/components/schemas/OpenAiMessageRole'
      required:
        - content
        - role
    TextContentType:
      type: string
      enum:
        - value: text
    TextContentLanguage:
      type: string
      enum:
        - value: aa
        - value: ab
        - value: ae
        - value: af
        - value: ak
        - value: am
        - value: an
        - value: ar
        - value: as
        - value: av
        - value: ay
        - value: az
        - value: ba
        - value: be
        - value: bg
        - value: bh
        - value: bi
        - value: bm
        - value: bn
        - value: bo
        - value: br
        - value: bs
        - value: ca
        - value: ce
        - value: ch
        - value: co
        - value: cr
        - value: cs
        - value: cu
        - value: cv
        - value: cy
        - value: da
        - value: de
        - value: dv
        - value: dz
        - value: ee
        - value: el
        - value: en
        - value: eo
        - value: es
        - value: et
        - value: eu
        - value: fa
        - value: ff
        - value: fi
        - value: fj
        - value: fo
        - value: fr
        - value: fy
        - value: ga
        - value: gd
        - value: gl
        - value: gn
        - value: gu
        - value: gv
        - value: ha
        - value: he
        - value: hi
        - value: ho
        - value: hr
        - value: ht
        - value: hu
        - value: hy
        - value: hz
        - value: ia
        - value: id
        - value: ie
        - value: ig
        - value: ii
        - value: ik
        - value: io
        - value: is
        - value: it
        - value: iu
        - value: ja
        - value: jv
        - value: ka
        - value: kg
        - value: ki
        - value: kj
        - value: kk
        - value: kl
        - value: km
        - value: kn
        - value: ko
        - value: kr
        - value: ks
        - value: ku
        - value: kv
        - value: kw
        - value: ky
        - value: la
        - value: lb
        - value: lg
        - value: li
        - value: ln
        - value: lo
        - value: lt
        - value: lu
        - value: lv
        - value: mg
        - value: mh
        - value: mi
        - value: mk
        - value: ml
        - value: mn
        - value: mr
        - value: ms
        - value: mt
        - value: my
        - value: na
        - value: nb
        - value: nd
        - value: ne
        - value: ng
        - value: nl
        - value: nn
        - value: 'no'
        - value: nr
        - value: nv
        - value: ny
        - value: oc
        - value: oj
        - value: om
        - value: or
        - value: os
        - value: pa
        - value: pi
        - value: pl
        - value: ps
        - value: pt
        - value: qu
        - value: rm
        - value: rn
        - value: ro
        - value: ru
        - value: rw
        - value: sa
        - value: sc
        - value: sd
        - value: se
        - value: sg
        - value: si
        - value: sk
        - value: sl
        - value: sm
        - value: sn
        - value: so
        - value: sq
        - value: sr
        - value: ss
        - value: st
        - value: su
        - value: sv
        - value: sw
        - value: ta
        - value: te
        - value: tg
        - value: th
        - value: ti
        - value: tk
        - value: tl
        - value: tn
        - value: to
        - value: tr
        - value: ts
        - value: tt
        - value: tw
        - value: ty
        - value: ug
        - value: uk
        - value: ur
        - value: uz
        - value: ve
        - value: vi
        - value: vo
        - value: wa
        - value: wo
        - value: xh
        - value: yi
        - value: yue
        - value: yo
        - value: za
        - value: zh
        - value: zu
    TextContent:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/TextContentType'
        text:
          type: string
        language:
          $ref: '#/components/schemas/TextContentLanguage'
      required:
        - type
        - text
        - language
    ToolMessageStartContentsItems:
      oneOf:
        - $ref: '#/components/schemas/TextContent'
    ToolMessageStartType:
      type: string
      enum:
        - value: request-start
    ConditionOperator:
      type: string
      enum:
        - value: eq
        - value: neq
        - value: gt
        - value: gte
        - value: lt
        - value: lte
    Condition:
      type: object
      properties:
        operator:
          $ref: '#/components/schemas/ConditionOperator'
          description: >-
            This is the operator you want to use to compare the parameter and
            value.
        param:
          type: string
          description: This is the name of the parameter that you want to check.
        value:
          type: string
          description: This is the value you want to compare against the parameter.
      required:
        - operator
        - param
        - value
    ToolMessageStart:
      type: object
      properties:
        contents:
          type: array
          items:
            $ref: '#/components/schemas/ToolMessageStartContentsItems'
          description: >-
            This is an alternative to the `content` property. It allows to
            specify variants of the same content, one per language.


            Usage:

            - If your assistants are multilingual, you can provide content for
            each language.

            - If you don't provide content for a language, the first item in the
            array will be automatically translated to the active language at
            that moment.


            This will override the `content` property.
        type:
          $ref: '#/components/schemas/ToolMessageStartType'
          description: >-
            This message is triggered when the tool call starts.


            This message is never triggered for async tools.


            If this message is not provided, one of the default filler messages
            "Hold on a sec", "One moment", "Just a sec", "Give me a moment" or
            "This'll just take a sec" will be used.
        blocking:
          type: boolean
          default: false
          description: >-
            This is an optional boolean that if true, the tool call will only
            trigger after the message is spoken. Default is false.


            @default false
        content:
          type: string
          description: >-
            This is the content that the assistant says when this message is
            triggered.
        conditions:
          type: array
          items:
            $ref: '#/components/schemas/Condition'
          description: >-
            This is an optional array of conditions that the tool call arguments
            must meet in order for this message to be triggered.
      required:
        - type
    ToolMessageCompleteContentsItems:
      oneOf:
        - $ref: '#/components/schemas/TextContent'
    ToolMessageCompleteType:
      type: string
      enum:
        - value: request-complete
    ToolMessageCompleteRole:
      type: string
      enum:
        - value: assistant
        - value: system
    ToolMessageComplete:
      type: object
      properties:
        contents:
          type: array
          items:
            $ref: '#/components/schemas/ToolMessageCompleteContentsItems'
          description: >-
            This is an alternative to the `content` property. It allows to
            specify variants of the same content, one per language.


            Usage:

            - If your assistants are multilingual, you can provide content for
            each language.

            - If you don't provide content for a language, the first item in the
            array will be automatically translated to the active language at
            that moment.


            This will override the `content` property.
        type:
          $ref: '#/components/schemas/ToolMessageCompleteType'
          description: >-
            This message is triggered when the tool call is complete.


            This message is triggered immediately without waiting for your
            server to respond for async tool calls.


            If this message is not provided, the model will be requested to
            respond.


            If this message is provided, only this message will be spoken and
            the model will not be requested to come up with a response. It's an
            exclusive OR.
        role:
          $ref: '#/components/schemas/ToolMessageCompleteRole'
          description: >-
            This is optional and defaults to "assistant".


            When role=assistant, `content` is said out loud.


            When role=system, `content` is passed to the model in a system
            message. Example:
                system: default one
                assistant:
                user:
                assistant:
                user:
                assistant:
                user:
                assistant: tool called
                tool: your server response
                <--- system prompt as hint
                ---> model generates response which is spoken
            This is useful when you want to provide a hint to the model about
            what to say next.
        endCallAfterSpokenEnabled:
          type: boolean
          description: >-
            This is an optional boolean that if true, the call will end after
            the message is spoken. Default is false.


            This is ignored if `role` is set to `system`.


            @default false
        content:
          type: string
          description: >-
            This is the content that the assistant says when this message is
            triggered.
        conditions:
          type: array
          items:
            $ref: '#/components/schemas/Condition'
          description: >-
            This is an optional array of conditions that the tool call arguments
            must meet in order for this message to be triggered.
      required:
        - type
    ToolMessageFailedContentsItems:
      oneOf:
        - $ref: '#/components/schemas/TextContent'
    ToolMessageFailedType:
      type: string
      enum:
        - value: request-failed
    ToolMessageFailed:
      type: object
      properties:
        contents:
          type: array
          items:
            $ref: '#/components/schemas/ToolMessageFailedContentsItems'
          description: >-
            This is an alternative to the `content` property. It allows to
            specify variants of the same content, one per language.


            Usage:

            - If your assistants are multilingual, you can provide content for
            each language.

            - If you don't provide content for a language, the first item in the
            array will be automatically translated to the active language at
            that moment.


            This will override the `content` property.
        type:
          $ref: '#/components/schemas/ToolMessageFailedType'
          description: >-
            This message is triggered when the tool call fails.


            This message is never triggered for async tool calls.


            If this message is not provided, the model will be requested to
            respond.


            If this message is provided, only this message will be spoken and
            the model will not be requested to come up with a response. It's an
            exclusive OR.
        endCallAfterSpokenEnabled:
          type: boolean
          description: >-
            This is an optional boolean that if true, the call will end after
            the message is spoken. Default is false.


            @default false
        content:
          type: string
          description: >-
            This is the content that the assistant says when this message is
            triggered.
        conditions:
          type: array
          items:
            $ref: '#/components/schemas/Condition'
          description: >-
            This is an optional array of conditions that the tool call arguments
            must meet in order for this message to be triggered.
      required:
        - type
    ToolMessageDelayedContentsItems:
      oneOf:
        - $ref: '#/components/schemas/TextContent'
    ToolMessageDelayedType:
      type: string
      enum:
        - value: request-response-delayed
    ToolMessageDelayed:
      type: object
      properties:
        contents:
          type: array
          items:
            $ref: '#/components/schemas/ToolMessageDelayedContentsItems'
          description: >-
            This is an alternative to the `content` property. It allows to
            specify variants of the same content, one per language.


            Usage:

            - If your assistants are multilingual, you can provide content for
            each language.

            - If you don't provide content for a language, the first item in the
            array will be automatically translated to the active language at
            that moment.


            This will override the `content` property.
        type:
          $ref: '#/components/schemas/ToolMessageDelayedType'
          description: >-
            This message is triggered when the tool call is delayed.


            There are the two things that can trigger this message:

            1. The user talks with the assistant while your server is processing
            the request. Default is "Sorry, a few more seconds."

            2. The server doesn't respond within `timingMilliseconds`.


            This message is never triggered for async tool calls.
        timingMilliseconds:
          type: number
          format: double
          description: >-
            The number of milliseconds to wait for the server response before
            saying this message.
        content:
          type: string
          description: >-
            This is the content that the assistant says when this message is
            triggered.
        conditions:
          type: array
          items:
            $ref: '#/components/schemas/Condition'
          description: >-
            This is an optional array of conditions that the tool call arguments
            must meet in order for this message to be triggered.
      required:
        - type
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingApiRequestMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingApiRequestMethod:
      type: string
      enum:
        - value: POST
        - value: GET
        - value: PUT
        - value: PATCH
        - value: DELETE
    JsonSchemaType:
      type: string
      enum:
        - value: string
        - value: number
        - value: integer
        - value: boolean
        - value: array
        - value: object
    JsonSchemaFormat:
      type: string
      enum:
        - value: date-time
        - value: time
        - value: date
        - value: duration
        - value: email
        - value: hostname
        - value: ipv4
        - value: ipv6
        - value: uuid
    JsonSchema:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/JsonSchemaType'
          description: >-
            This is the type of output you'd like.


            `string`, `number`, `integer`, `boolean` are the primitive types and
            should be obvious.


            `array` and `object` are more interesting and quite powerful. They
            allow you to define nested structures.


            For `array`, you can define the schema of the items in the array
            using the `items` property.


            For `object`, you can define the properties of the object using the
            `properties` property.
        items:
          $ref: '#/components/schemas/JsonSchema'
          description: >-
            This is required if the type is "array". This is the schema of the
            items in the array. This is a recursive reference to JsonSchema.
        properties:
          type: object
          additionalProperties:
            $ref: '#/components/schemas/JsonSchema'
          description: >-
            This is required if the type is "object". This specifies the
            properties of the object. This is a map of property names to
            JsonSchema objects.
        description:
          type: string
          description: >-
            This is the description to help the model understand what it needs
            to output.
        pattern:
          type: string
          description: >-
            This is the pattern of the string. This is a regex that will be used
            to validate the data in question. To use a common format, use the
            `format` property instead.


            OpenAI documentation:
            https://platform.openai.com/docs/guides/structured-outputs#supported-properties
        format:
          $ref: '#/components/schemas/JsonSchemaFormat'
          description: >-
            This is the format of the string. To pass a regex, use the `pattern`
            property instead.


            OpenAI documentation:
            https://platform.openai.com/docs/guides/structured-outputs?api-mode=chat&type-restrictions=string-restrictions
        required:
          type: array
          items:
            type: string
          description: |-
            This is a list of properties that are required.

            This only makes sense if the type is "object".
        enum:
          type: array
          items:
            type: string
          description: >-
            This array specifies the allowed values that can be used to restrict
            the output of the model.
        title:
          type: string
          description: This is the title of the schema.
      required:
        - type
    VariableExtractionAlias:
      type: object
      properties:
        key:
          type: string
          description: >-
            This is the key of the variable.


            This variable will be accessible during the call as `{{key}}` and
            stored in `call.artifact.variableValues` after the call.


            Rules:

            - Must start with a letter (a-z, A-Z).

            - Subsequent characters can be letters, numbers, or underscores.

            - Minimum length of 1 and maximum length of 40.
        value:
          type: string
          description: >-
            This is the value of the variable.


            This can reference existing variables, use filters, and perform
            transformations.


            Examples: "{{name}}", "{{customer.email}}", "Hello {{name |
            upcase}}"
      required:
        - key
        - value
    VariableExtractionPlan:
      type: object
      properties:
        schema:
          $ref: '#/components/schemas/JsonSchema'
          description: >-
            This is the schema to extract.


            Examples:

            1. To extract object properties, you can use the following schema:

            ```json

            {
              "type": "object",
              "properties": {
                "name": {
                  "type": "string"
                },
                "age": {
                  "type": "number"
                }
              }
            }

            ```


            These will be extracted as `{{ name }}` and `{{ age }}`
            respectively. To emphasize, object properties are extracted as
            direct global variables.


            2. To extract nested properties, you can use the following schema:

            ```json

            {
              "type": "object",
              "properties": {
                "name": {
                  "type": "object",
                  "properties": {
                    "first": {
                      "type": "string"
                    },
                    "last": {
                      "type": "string"
                    }
                  }
                }
              }
            }

            ```


            These will be extracted as `{{ name }}`. And, `{{ name.first }}` and
            `{{ name.last }}` will be accessible.


            3. To extract array items, you can use the following schema:

            ```json

            {
              "type": "array",
              "title": "zipCodes",
              "items": {
                "type": "string"
              }
            }

            ```


            This will be extracted as `{{ zipCodes }}`. To access the array
            items, you can use `{{ zipCodes[0] }}` and `{{ zipCodes[1] }}`.


            4. To extract array of objects, you can use the following schema:


            ```json

            {
              "type": "array",
              "name": "people",
              "items": {
                "type": "object",
                "properties": {
                  "name": {
                    "type": "string"
                  },
                  "age": {
                    "type": "number"
                  },
                  "zipCodes": {
                    "type": "array",
                    "items": {
                      "type": "string"
                    }
                  }
                }
              }
            }

            ```


            This will be extracted as `{{ people }}`. To access the array items,
            you can use `{{ people[n].name }}`, `{{ people[n].age }}`, `{{
            people[n].zipCodes }}`, `{{ people[n].zipCodes[0] }}` and `{{
            people[n].zipCodes[1] }}`.
        aliases:
          type: array
          items:
            $ref: '#/components/schemas/VariableExtractionAlias'
          description: >-
            These are additional variables to create.


            These will be accessible during the call as `{{key}}` and stored in
            `call.artifact.variableValues` after the call.


            Example:

            ```json

            {
              "aliases": [
                {
                  "key": "customerName",
                  "value": "{{name}}"
                },
                {
                  "key": "fullName",
                  "value": "{{firstName}} {{lastName}}"
                },
                {
                  "key": "greeting",
                  "value": "Hello {{name}}, welcome to {{company}}!"
                },
                {
                  "key": "customerCity",
                  "value": "{{addresses[0].city}}"
                },
                {
                  "key": "something",
                  "value": "{{any liquid}}"
                }
              ]
            }

            ```


            This will create variables `customerName`, `fullName`, `greeting`,
            `customerCity`, and `something`. To access these variables, you can
            reference them as `{{customerName}}`, `{{fullName}}`,
            `{{greeting}}`, `{{customerCity}}`, and `{{something}}`.
    RegexConditionType:
      type: string
      enum:
        - value: regex
    MessageTargetRole:
      type: string
      enum:
        - value: user
        - value: assistant
    MessageTarget:
      type: object
      properties:
        role:
          $ref: '#/components/schemas/MessageTargetRole'
          description: >-
            This is the role of the message to target.


            If not specified, will find the position in the message history
            ignoring role (effectively `any`).
        position:
          type: number
          format: double
          description: >-
            This is the position of the message to target.

            - Negative numbers: Count from end (-1 = most recent, -2 = second
            most recent)

            - 0: First/oldest message in history

            - Positive numbers: Specific position (0-indexed from start)


            @default -1 (most recent message)
    RegexCondition:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/RegexConditionType'
          description: This is the type discriminator for regex condition
        regex:
          type: string
          description: >-
            This is the regular expression pattern to match against message
            content.


            Note:

            - This works by using the RegExp.test method in Node.JS. Eg.
            /hello/.test("hello there") will return true.


            Hot tips:

            - In JavaScript, escape \ when sending the regex pattern. Eg.
            "hello\sthere" will be sent over the wire as "hellosthere". Send
            "hello\\sthere" instead.

            - RegExp.test does substring matching, so /cat/.test("I love cats")
            will return true. To do full string matching, use anchors: /^cat$/
            will only match exactly "cat".

            - Word boundaries \b are useful for matching whole words: /\bcat\b/
            matches "cat" but not "cats" or "category".

            - Use inline flags for portability: (?i) for case insensitive, (?m)
            for multiline
        target:
          $ref: '#/components/schemas/MessageTarget'
          description: >-
            This is the target for messages to check against.

            If not specified, the condition will run on the last message
            (position: -1).

            If role is not specified, it will look at the last message
            regardless of role.

            @default { position: -1 }
        negate:
          type: boolean
          description: >-
            This is the flag that when true, the condition matches if the
            pattern does NOT match.

            Useful for ensuring certain words/phrases are absent.


            @default false
      required:
        - type
        - regex
    LiquidConditionType:
      type: string
      enum:
        - value: liquid
    LiquidCondition:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/LiquidConditionType'
          description: This is the type discriminator for liquid condition
        liquid:
          type: string
          description: >-
            This is the Liquid template that must return exactly "true" or
            "false" as a string.

            The template is evaluated and the entire output must be either
            "true" or "false" - nothing else.


            Available variables:

            - `messages`: Array of recent messages in OpenAI chat completions
            format (ChatCompletionMessageParam[])
              Each message has properties like: role ('user', 'assistant', 'system'), content (string), etc.
            - `now`: Current timestamp in milliseconds (built-in Liquid
            variable)

            - Any assistant variable values (e.g., `userName`, `accountStatus`)


            Useful Liquid filters for messages:

            - `messages | last: 5` - Get the 5 most recent messages

            - `messages | where: 'role', 'user'` - Filter to only user messages

            - `messages | reverse` - Reverse the order of messages
      required:
        - type
        - liquid
    GroupConditionType:
      type: string
      enum:
        - value: group
    GroupConditionOperator:
      type: string
      enum:
        - value: AND
        - value: OR
    GroupConditionConditionsItems:
      oneOf:
        - $ref: '#/components/schemas/RegexCondition'
        - $ref: '#/components/schemas/LiquidCondition'
        - $ref: '#/components/schemas/GroupCondition'
    GroupCondition:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/GroupConditionType'
          description: This is the type discriminator for group condition
        operator:
          $ref: '#/components/schemas/GroupConditionOperator'
          description: This is the logical operator for combining conditions in this group
        conditions:
          type: array
          items:
            $ref: '#/components/schemas/GroupConditionConditionsItems'
          description: |-
            This is the list of nested conditions to evaluate.
            Supports recursive nesting of groups for complex logic.
      required:
        - type
        - operator
        - conditions
    ToolRejectionPlanConditionsItems:
      oneOf:
        - $ref: '#/components/schemas/RegexCondition'
        - $ref: '#/components/schemas/LiquidCondition'
        - $ref: '#/components/schemas/GroupCondition'
    ToolRejectionPlan:
      type: object
      properties:
        conditions:
          type: array
          items:
            $ref: '#/components/schemas/ToolRejectionPlanConditionsItems'
          description: >-
            This is the list of conditions that must be evaluated.


            Usage:

            - If all conditions match (AND logic), the tool call is rejected.

            - For OR logic at the top level, use a single 'group' condition with
            operator: 'OR'.


            @default [] - Empty array means tool always executes
    CreateApiRequestToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingApiRequestMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        method:
          $ref: >-
            #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingApiRequestMethod
        timeoutSeconds:
          type: number
          format: double
          description: >-
            This is the timeout in seconds for the request. Defaults to 20
            seconds.


            @default 20
        credentialId:
          type: string
          description: The credential ID for API request authentication
        encryptedPaths:
          type: array
          items:
            type: string
          description: >-
            This is the paths to encrypt in the request body if credentialId and
            encryptionPlan are defined.
        name:
          type: string
          description: >-
            This is the name of the tool. This will be passed to the model.


            Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a
            maximum length of 40.
        description:
          type: string
          description: >-
            This is the description of the tool. This will be passed to the
            model.
        url:
          type: string
          description: This is where the request will be sent.
        body:
          $ref: '#/components/schemas/JsonSchema'
          description: This is the body of the request.
        headers:
          $ref: '#/components/schemas/JsonSchema'
          description: These are the headers to send with the request.
        backoffPlan:
          $ref: '#/components/schemas/BackoffPlan'
          description: >-
            This is the backoff plan if the request fails. Defaults to undefined
            (the request will not be retried).


            @default undefined (the request will not be retried)
        variableExtractionPlan:
          $ref: '#/components/schemas/VariableExtractionPlan'
          description: >-
            This is the plan to extract variables from the tool's response.
            These will be accessible during the call and stored in
            `call.artifact.variableValues` after the call.


            Usage:

            1. Use `aliases` to extract variables from the tool's response body.
            (Most common case)


            ```json

            {
              "aliases": [
                {
                  "key": "customerName",
                  "value": "{{customer.name}}"
                },
                {
                  "key": "customerAge",
                  "value": "{{customer.age}}"
                }
              ]
            }

            ```


            The tool response body is made available to the liquid template.


            2. Use `aliases` to extract variables from the tool's response body
            if the response is an array.


            ```json

            {
              "aliases": [
                {
                  "key": "customerName",
                  "value": "{{$[0].name}}"
                },
                {
                  "key": "customerAge",
                  "value": "{{$[0].age}}"
                }
              ]
            }

            ```


            $ is a shorthand for the tool's response body. `$[0]` is the first
            item in the array. `$[n]` is the nth item in the array. Note, $ is
            available regardless of the response body type (both object and
            array).


            3. Use `aliases` to extract variables from the tool's response
            headers.


            ```json

            {
              "aliases": [
                {
                  "key": "customerName",
                  "value": "{{tool.response.headers.customer-name}}"
                },
                {
                  "key": "customerAge",
                  "value": "{{tool.response.headers.customer-age}}"
                }
              ]
            }

            ```


            `tool.response` is made available to the liquid template.
            Particularly, both `tool.response.headers` and `tool.response.body`
            are available. Note, `tool.response` is available regardless of the
            response body type (both object and array).


            4. Use `schema` to extract a large portion of the tool's response
            body.


            4.1. If you hit example.com and it returns `{"name": "John", "age":
            30}`, then you can specify the schema as:


            ```json

            {
              "schema": {
                "type": "object",
                "properties": {
                  "name": {
                    "type": "string"
                  },
                  "age": {
                    "type": "number"
                  }
                }
              }
            }

            ```

            These will be extracted as `{{ name }}` and `{{ age }}`
            respectively. To emphasize, object properties are extracted as
            direct global variables.


            4.2. If you hit example.com and it returns `{"name": {"first":
            "John", "last": "Doe"}}`, then you can specify the schema as:


            ```json

            {
              "schema": {
                "type": "object",
                "properties": {
                  "name": {
                    "type": "object",
                    "properties": {
                      "first": {
                        "type": "string"
                      },
                      "last": {
                        "type": "string"
                      }
                    }
                  }
                }
              }
            }

            ```


            These will be extracted as `{{ name }}`. And, `{{ name.first }}` and
            `{{ name.last }}` will be accessible.


            4.3. If you hit example.com and it returns `["94123", "94124"]`,
            then you can specify the schema as:


            ```json

            {
              "schema": {
                "type": "array",
                "title": "zipCodes",
                "items": {
                  "type": "string"
                }
              }
            }

            ```


            This will be extracted as `{{ zipCodes }}`. To access the array
            items, you can use `{{ zipCodes[0] }}` and `{{ zipCodes[1] }}`.


            4.4. If you hit example.com and it returns `[{"name": "John", "age":
            30, "zipCodes": ["94123", "94124"]}, {"name": "Jane", "age": 25,
            "zipCodes": ["94125", "94126"]}]`, then you can specify the schema
            as:


            ```json

            {
              "schema": {
                "type": "array",
                "title": "people",
                "items": {
                  "type": "object",
                  "properties": {
                    "name": {
                      "type": "string"
                    },
                    "age": {
                      "type": "number"
                    },
                    "zipCodes": {
                      "type": "array",
                      "items": {
                        "type": "string"
                      }
                    }
                  }
                }
              }
            }

            ```


            This will be extracted as `{{ people }}`. To access the array items,
            you can use `{{ people[n].name }}`, `{{ people[n].age }}`, `{{
            people[n].zipCodes }}`, `{{ people[n].zipCodes[0] }}` and `{{
            people[n].zipCodes[1] }}`.


            Note: Both `aliases` and `schema` can be used together.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
      required:
        - method
        - url
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingBashMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingBashSubType:
      type: string
      enum:
        - value: bash_20241022
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingBashName:
      type: string
      enum:
        - value: bash
      default: bash
    CreateBashToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingBashMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        subType:
          $ref: >-
            #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingBashSubType
          description: The sub type of tool.
        server:
          $ref: '#/components/schemas/Server'
          description: |2-

              This is the server where a `tool-calls` webhook will be sent.

              Notes:
              - Webhook is sent to this server when a tool call is made.
              - Webhook contains the call, assistant, and phone number objects.
              - Webhook contains the variables set on the assistant.
              - Webhook is sent to the first available URL in this order: {{tool.server.url}}, {{assistant.server.url}}, {{phoneNumber.server.url}}, {{org.server.url}}.
              - Webhook expects a response with tool call result.
        name:
          $ref: >-
            #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingBashName
          description: The name of the tool, fixed to 'bash'
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
      required:
        - subType
        - name
    CreateCodeToolDtoMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateCodeToolDtoType:
      type: string
      enum:
        - value: code
    CodeToolEnvironmentVariable:
      type: object
      properties:
        name:
          type: string
          description: Name of the environment variable
        value:
          type: string
          description: Value of the environment variable. Supports Liquid templates.
      required:
        - name
        - value
    OpenAiFunctionParametersType:
      type: string
      enum:
        - value: object
    OpenAIFunctionParameters:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/OpenAiFunctionParametersType'
          description: >-
            This must be set to 'object'. It instructs the model to return a
            JSON object containing the function call properties.
        properties:
          type: object
          additionalProperties:
            $ref: '#/components/schemas/JsonSchema'
          description: >-
            This provides a description of the properties required by the
            function.

            JSON Schema can be used to specify expectations for each property.

            Refer to [this
            doc](https://ajv.js.org/json-schema.html#json-data-type) for a
            comprehensive guide on JSON Schema.
        required:
          type: array
          items:
            type: string
          description: This specifies the properties that are required by the function.
      required:
        - type
        - properties
    OpenAIFunction:
      type: object
      properties:
        strict:
          type: boolean
          default: false
          description: >-
            This is a boolean that controls whether to enable strict schema
            adherence when generating the function call. If set to true, the
            model will follow the exact schema defined in the parameters field.
            Only a subset of JSON Schema is supported when strict is true. Learn
            more about Structured Outputs in the [OpenAI
            guide](https://openai.com/index/introducing-structured-outputs-in-the-api/).


            @default false
        name:
          type: string
          description: >-
            This is the the name of the function to be called.


            Must be a-z, A-Z, 0-9, or contain underscores and dashes, with a
            maximum length of 64.
        description:
          type: string
          description: >-
            This is the description of what the function does, used by the AI to
            choose when and how to call the function.
        parameters:
          $ref: '#/components/schemas/OpenAIFunctionParameters'
          description: >-
            These are the parameters the functions accepts, described as a JSON
            Schema object.


            See the [OpenAI
            guide](https://platform.openai.com/docs/guides/function-calling) for
            examples, and the [JSON Schema
            reference](https://json-schema.org/understanding-json-schema) for
            documentation about the format.


            Omitting parameters defines a function with an empty parameter list.
      required:
        - name
    CreateCodeToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/CreateCodeToolDtoMessagesItems'
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        type:
          $ref: '#/components/schemas/CreateCodeToolDtoType'
          description: The type of tool. "code" for Code tool.
        async:
          type: boolean
          description: |-
            This determines if the tool is async.

              If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.

              If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.

              Defaults to synchronous (`false`).
        server:
          $ref: '#/components/schemas/Server'
          description: |2-

              This is the server where a `tool-calls` webhook will be sent.

              Notes:
              - Webhook is sent to this server when a tool call is made.
              - Webhook contains the call, assistant, and phone number objects.
              - Webhook contains the variables set on the assistant.
              - Webhook is sent to the first available URL in this order: {{tool.server.url}}, {{assistant.server.url}}, {{phoneNumber.server.url}}, {{org.server.url}}.
              - Webhook expects a response with tool call result.
        code:
          type: string
          description: TypeScript code to execute when the tool is called
        environmentVariables:
          type: array
          items:
            $ref: '#/components/schemas/CodeToolEnvironmentVariable'
          description: Environment variables available in code via `env` object
        timeoutSeconds:
          type: number
          format: double
          description: >-
            This is the timeout in seconds for the code execution. Defaults to
            10 seconds.

            Maximum is 30 seconds to prevent abuse.


            @default 10
        credentialId:
          type: string
          description: Credential ID containing the Val Town API key
        variableExtractionPlan:
          $ref: '#/components/schemas/VariableExtractionPlan'
          description: Plan to extract variables from the tool response
        function:
          $ref: '#/components/schemas/OpenAIFunction'
          description: >-
            This is the function definition of the tool.


            For the Code tool, this defines the name, description, and
            parameters that the model

            will use to understand when and how to call this tool.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
      required:
        - type
        - code
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingComputerMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingComputerSubType:
      type: string
      enum:
        - value: computer_20241022
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingComputerName:
      type: string
      enum:
        - value: computer
      default: computer
    CreateComputerToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingComputerMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        subType:
          $ref: >-
            #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingComputerSubType
          description: The sub type of tool.
        server:
          $ref: '#/components/schemas/Server'
          description: |2-

              This is the server where a `tool-calls` webhook will be sent.

              Notes:
              - Webhook is sent to this server when a tool call is made.
              - Webhook contains the call, assistant, and phone number objects.
              - Webhook contains the variables set on the assistant.
              - Webhook is sent to the first available URL in this order: {{tool.server.url}}, {{assistant.server.url}}, {{phoneNumber.server.url}}, {{org.server.url}}.
              - Webhook expects a response with tool call result.
        name:
          $ref: >-
            #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingComputerName
          description: The name of the tool, fixed to 'computer'
        displayWidthPx:
          type: number
          format: double
          description: The display width in pixels
        displayHeightPx:
          type: number
          format: double
          description: The display height in pixels
        displayNumber:
          type: number
          format: double
          description: Optional display number
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
      required:
        - subType
        - name
        - displayWidthPx
        - displayHeightPx
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingDtmfMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateDtmfToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingDtmfMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        sipInfoDtmfEnabled:
          type: boolean
          default: false
          description: >-
            This enables sending DTMF tones via SIP INFO messages instead of RFC
            2833 (RTP events). When enabled, DTMF digits will be sent using the
            SIP INFO method, which can be more reliable in some network
            configurations. Only relevant when using the `vapi.sip` transport.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingEndCallMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateEndCallToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingEndCallMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingFunctionMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateFunctionToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingFunctionMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        async:
          type: boolean
          description: |-
            This determines if the tool is async.

              If async, the assistant will move forward without waiting for your server to respond. This is useful if you just want to trigger something on your server.

              If sync, the assistant will wait for your server to respond. This is useful if want assistant to respond with the result from your server.

              Defaults to synchronous (`false`).
        server:
          $ref: '#/components/schemas/Server'
          description: |2-

              This is the server where a `tool-calls` webhook will be sent.

              Notes:
              - Webhook is sent to this server when a tool call is made.
              - Webhook contains the call, assistant, and phone number objects.
              - Webhook contains the variables set on the assistant.
              - Webhook is sent to the first available URL in this order: {{tool.server.url}}, {{assistant.server.url}}, {{phoneNumber.server.url}}, {{org.server.url}}.
              - Webhook expects a response with tool call result.
        function:
          $ref: '#/components/schemas/OpenAIFunction'
          description: This is the function definition of the tool.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGohighlevelCalendarAvailabilityCheckMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateGoHighLevelCalendarAvailabilityToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGohighlevelCalendarAvailabilityCheckMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGohighlevelCalendarEventCreateMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateGoHighLevelCalendarEventCreateToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGohighlevelCalendarEventCreateMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGohighlevelContactCreateMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateGoHighLevelContactCreateToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGohighlevelContactCreateMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGohighlevelContactGetMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateGoHighLevelContactGetToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGohighlevelContactGetMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGoogleCalendarAvailabilityCheckMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateGoogleCalendarCheckAvailabilityToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGoogleCalendarAvailabilityCheckMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGoogleCalendarEventCreateMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateGoogleCalendarCreateEventToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGoogleCalendarEventCreateMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGoogleSheetsRowAppendMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateGoogleSheetsRowAppendToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingGoogleSheetsRowAppendMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingHandoffMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    HandoffDestinationAssistantType:
      type: string
      enum:
        - value: assistant
    ContextEngineeringPlanLastNMessagesType:
      type: string
      enum:
        - value: lastNMessages
    ContextEngineeringPlanLastNMessages:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/ContextEngineeringPlanLastNMessagesType'
        maxMessages:
          type: number
          format: double
          description: >-
            This is the maximum number of messages to include in the context
            engineering plan.
      required:
        - type
        - maxMessages
    ContextEngineeringPlanNoneType:
      type: string
      enum:
        - value: none
    ContextEngineeringPlanNone:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/ContextEngineeringPlanNoneType'
      required:
        - type
    ContextEngineeringPlanAllType:
      type: string
      enum:
        - value: all
    ContextEngineeringPlanAll:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/ContextEngineeringPlanAllType'
      required:
        - type
    ContextEngineeringPlanUserAndAssistantMessagesType:
      type: string
      enum:
        - value: userAndAssistantMessages
    ContextEngineeringPlanUserAndAssistantMessages:
      type: object
      properties:
        type:
          $ref: >-
            #/components/schemas/ContextEngineeringPlanUserAndAssistantMessagesType
      required:
        - type
    HandoffDestinationAssistantContextEngineeringPlan:
      oneOf:
        - $ref: '#/components/schemas/ContextEngineeringPlanLastNMessages'
        - $ref: '#/components/schemas/ContextEngineeringPlanNone'
        - $ref: '#/components/schemas/ContextEngineeringPlanAll'
        - $ref: '#/components/schemas/ContextEngineeringPlanUserAndAssistantMessages'
    AssistantOverridesTranscriber:
      oneOf:
        - $ref: '#/components/schemas/AssemblyAITranscriber'
        - $ref: '#/components/schemas/AzureSpeechTranscriber'
        - $ref: '#/components/schemas/CustomTranscriber'
        - $ref: '#/components/schemas/DeepgramTranscriber'
        - $ref: '#/components/schemas/ElevenLabsTranscriber'
        - $ref: '#/components/schemas/GladiaTranscriber'
        - $ref: '#/components/schemas/GoogleTranscriber'
        - $ref: '#/components/schemas/SpeechmaticsTranscriber'
        - $ref: '#/components/schemas/TalkscriberTranscriber'
        - $ref: '#/components/schemas/OpenAITranscriber'
        - $ref: '#/components/schemas/CartesiaTranscriber'
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingMcpMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    McpToolMetadataProtocol:
      type: string
      enum:
        - value: sse
        - value: shttp
    McpToolMetadata:
      type: object
      properties:
        protocol:
          $ref: '#/components/schemas/McpToolMetadataProtocol'
          description: >-
            This is the protocol used for MCP communication. Defaults to
            Streamable HTTP.
    CreateMcpToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingMcpMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        server:
          $ref: '#/components/schemas/Server'
          description: |2-

              This is the server where a `tool-calls` webhook will be sent.

              Notes:
              - Webhook is sent to this server when a tool call is made.
              - Webhook contains the call, assistant, and phone number objects.
              - Webhook contains the variables set on the assistant.
              - Webhook is sent to the first available URL in this order: {{tool.server.url}}, {{assistant.server.url}}, {{phoneNumber.server.url}}, {{org.server.url}}.
              - Webhook expects a response with tool call result.
        metadata:
          $ref: '#/components/schemas/McpToolMetadata'
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingQueryMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    KnowledgeBaseProvider:
      type: string
      enum:
        - value: google
    KnowledgeBaseModel:
      type: string
      enum:
        - value: gemini-3-flash-preview
        - value: gemini-2.5-pro
        - value: gemini-2.5-flash
        - value: gemini-2.5-flash-lite
        - value: gemini-2.0-flash-thinking-exp
        - value: gemini-2.0-pro-exp-02-05
        - value: gemini-2.0-flash
        - value: gemini-2.0-flash-lite
        - value: gemini-2.0-flash-exp
        - value: gemini-2.0-flash-realtime-exp
        - value: gemini-1.5-flash
        - value: gemini-1.5-flash-002
        - value: gemini-1.5-pro
        - value: gemini-1.5-pro-002
        - value: gemini-1.0-pro
    KnowledgeBase:
      type: object
      properties:
        name:
          type: string
          description: The name of the knowledge base
        provider:
          $ref: '#/components/schemas/KnowledgeBaseProvider'
          description: The provider of the knowledge base
        model:
          $ref: '#/components/schemas/KnowledgeBaseModel'
          description: The model to use for the knowledge base
        description:
          type: string
          description: A description of the knowledge base
        fileIds:
          type: array
          items:
            type: string
          description: The file IDs associated with this knowledge base
      required:
        - name
        - provider
        - description
        - fileIds
    CreateQueryToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingQueryMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        knowledgeBases:
          type: array
          items:
            $ref: '#/components/schemas/KnowledgeBase'
          description: The knowledge bases to query
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingSlackMessageSendMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateSlackSendMessageToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingSlackMessageSendMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingSmsMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateSmsToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingSmsMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingTextEditorMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingTextEditorSubType:
      type: string
      enum:
        - value: text_editor_20241022
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingTextEditorName:
      type: string
      enum:
        - value: str_replace_editor
      default: str_replace_editor
    CreateTextEditorToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingTextEditorMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        subType:
          $ref: >-
            #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingTextEditorSubType
          description: The sub type of tool.
        server:
          $ref: '#/components/schemas/Server'
          description: |2-

              This is the server where a `tool-calls` webhook will be sent.

              Notes:
              - Webhook is sent to this server when a tool call is made.
              - Webhook contains the call, assistant, and phone number objects.
              - Webhook contains the variables set on the assistant.
              - Webhook is sent to the first available URL in this order: {{tool.server.url}}, {{assistant.server.url}}, {{phoneNumber.server.url}}, {{org.server.url}}.
              - Webhook expects a response with tool call result.
        name:
          $ref: >-
            #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingTextEditorName
          description: The name of the tool, fixed to 'str_replace_editor'
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
      required:
        - subType
        - name
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingTransferCallMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CustomMessageContentsItems:
      oneOf:
        - $ref: '#/components/schemas/TextContent'
    CustomMessageType:
      type: string
      enum:
        - value: custom-message
    CustomMessage:
      type: object
      properties:
        contents:
          type: array
          items:
            $ref: '#/components/schemas/CustomMessageContentsItems'
          description: >-
            This is an alternative to the `content` property. It allows to
            specify variants of the same content, one per language.


            Usage:

            - If your assistants are multilingual, you can provide content for
            each language.

            - If you don't provide content for a language, the first item in the
            array will be automatically translated to the active language at
            that moment.


            This will override the `content` property.
        type:
          $ref: '#/components/schemas/CustomMessageType'
          description: This is a custom message.
        content:
          type: string
          description: >-
            This is the content that the assistant will say when this message is
            triggered.
      required:
        - type
    TransferDestinationAssistantMessage:
      oneOf:
        - type: string
        - $ref: '#/components/schemas/CustomMessage'
    TransferDestinationAssistantType:
      type: string
      enum:
        - value: assistant
    TransferMode:
      type: string
      enum:
        - value: rolling-history
        - value: swap-system-message-in-history
    TransferDestinationAssistant:
      type: object
      properties:
        message:
          $ref: '#/components/schemas/TransferDestinationAssistantMessage'
          description: >-
            This is spoken to the customer before connecting them to the
            destination.


            Usage:

            - If this is not provided and transfer tool messages is not
            provided, default is "Transferring the call now".

            - If set to "", nothing is spoken. This is useful when you want to
            silently transfer. This is especially useful when transferring
            between assistants in a squad. In this scenario, you likely also
            want to set
            `assistant.firstMessageMode=assistant-speaks-first-with-model-generated-message`
            for the destination assistant.


            This accepts a string or a ToolMessageStart class. Latter is useful
            if you want to specify multiple messages for different languages
            through the `contents` field.
        type:
          $ref: '#/components/schemas/TransferDestinationAssistantType'
        transferMode:
          $ref: '#/components/schemas/TransferMode'
          description: >-
            This is the mode to use for the transfer. Defaults to
            `rolling-history`.


            - `rolling-history`: This is the default mode. It keeps the entire
            conversation history and appends the new assistant's system message
            on transfer.

              Example:

              Pre-transfer:
                system: assistant1 system message
                assistant: assistant1 first message
                user: hey, good morning
                assistant: how can i help?
                user: i need help with my account
                assistant: (destination.message)

              Post-transfer:
                system: assistant1 system message
                assistant: assistant1 first message
                user: hey, good morning
                assistant: how can i help?
                user: i need help with my account
                assistant: (destination.message)
                system: assistant2 system message
                assistant: assistant2 first message (or model generated if firstMessageMode is set to `assistant-speaks-first-with-model-generated-message`)

            - `swap-system-message-in-history`: This replaces the original
            system message with the new assistant's system message on transfer.

              Example:

              Pre-transfer:
                system: assistant1 system message
                assistant: assistant1 first message
                user: hey, good morning
                assistant: how can i help?
                user: i need help with my account
                assistant: (destination.message)

              Post-transfer:
                system: assistant2 system message
                assistant: assistant1 first message
                user: hey, good morning
                assistant: how can i help?
                user: i need help with my account
                assistant: (destination.message)
                assistant: assistant2 first message (or model generated if firstMessageMode is set to `assistant-speaks-first-with-model-generated-message`)

            - `delete-history`: This deletes the entire conversation history on
            transfer.

              Example:

              Pre-transfer:
                system: assistant1 system message
                assistant: assistant1 first message
                user: hey, good morning
                assistant: how can i help?
                user: i need help with my account
                assistant: (destination.message)

              Post-transfer:
                system: assistant2 system message
                assistant: assistant2 first message
                user: Yes, please
                assistant: how can i help?
                user: i need help with my account

            -
            `swap-system-message-in-history-and-remove-transfer-tool-messages`:
            This replaces the original system message with the new assistant's
            system message on transfer and removes transfer tool messages from
            conversation history sent to the LLM.

              Example:

              Pre-transfer:
                system: assistant1 system message
                assistant: assistant1 first message
                user: hey, good morning
                assistant: how can i help?
                user: i need help with my account
                transfer-tool
                transfer-tool-result
                assistant: (destination.message)

              Post-transfer:
                system: assistant2 system message
                assistant: assistant1 first message
                user: hey, good morning
                assistant: how can i help?
                user: i need help with my account
                assistant: (destination.message)
                assistant: assistant2 first message (or model generated if firstMessageMode is set to `assistant-speaks-first-with-model-generated-message`)

            @default 'rolling-history'
        assistantName:
          type: string
          description: This is the assistant to transfer the call to.
        description:
          type: string
          description: >-
            This is the description of the destination, used by the AI to choose
            when and how to transfer the call.
      required:
        - type
        - assistantName
    TransferDestinationNumberMessage:
      oneOf:
        - type: string
        - $ref: '#/components/schemas/CustomMessage'
    TransferDestinationNumberType:
      type: string
      enum:
        - value: number
    TransferPlanMode:
      type: string
      enum:
        - value: blind-transfer
        - value: blind-transfer-add-summary-to-sip-header
        - value: warm-transfer-say-message
        - value: warm-transfer-say-summary
        - value: warm-transfer-twiml
        - value: warm-transfer-wait-for-operator-to-speak-first-and-then-say-message
        - value: warm-transfer-wait-for-operator-to-speak-first-and-then-say-summary
        - value: warm-transfer-experimental
    TransferPlanMessage:
      oneOf:
        - type: string
        - $ref: '#/components/schemas/CustomMessage'
    TransferPlanSipVerb:
      type: string
      enum:
        - value: refer
        - value: bye
        - value: dial
      default: refer
    TransferPlanContextEngineeringPlan:
      oneOf:
        - $ref: '#/components/schemas/ContextEngineeringPlanLastNMessages'
        - $ref: '#/components/schemas/ContextEngineeringPlanNone'
        - $ref: '#/components/schemas/ContextEngineeringPlanAll'
    SummaryPlanMessagesItems:
      type: object
      properties: {}
    SummaryPlan:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/SummaryPlanMessagesItems'
          description: >-
            These are the messages used to generate the summary.


            @default: ```

            [
              {
                "role": "system",
                "content": "You are an expert note-taker. You will be given a transcript of a call. Summarize the call in 2-3 sentences. DO NOT return anything except the summary."
              },
              {
                "role": "user",
                "content": "Here is the transcript:\n\n{{transcript}}\n\n. Here is the ended reason of the call:\n\n{{endedReason}}\n\n"
              }
            ]```


            You can customize by providing any messages you want.


            Here are the template variables available:

            - {{transcript}}: The transcript of the call from
            `call.artifact.transcript` 

            - {{systemPrompt}}: The system prompt of the call from
            `assistant.model.messages[type=system].content` 

            - {{messages}}: The messages of the call from
            `assistant.model.messages` 

            - {{endedReason}}: The ended reason of the call from
            `call.endedReason`
        enabled:
          type: boolean
          description: >-
            This determines whether a summary is generated and stored in
            `call.analysis.summary`. Defaults to true.


            Usage:

            - If you want to disable the summary, set this to false.


            @default true
        timeoutSeconds:
          type: number
          format: double
          description: >-
            This is how long the request is tried before giving up. When request
            times out, `call.analysis.summary` will be empty.


            Usage:

            - To guarantee the summary is generated, set this value high. Note,
            this will delay the end of call report in cases where model is slow
            to respond.


            @default 5 seconds
    TransferFallbackPlanMessage:
      oneOf:
        - type: string
        - $ref: '#/components/schemas/CustomMessage'
    TransferFallbackPlan:
      type: object
      properties:
        message:
          $ref: '#/components/schemas/TransferFallbackPlanMessage'
          description: >-
            This is the message the assistant will deliver to the customer if
            the transfer fails.
        endCallEnabled:
          type: boolean
          default: true
          description: >-
            This controls what happens after delivering the failure message to
            the customer.

            - true: End the call after delivering the failure message (default)

            - false: Keep the assistant on the call to continue handling the
            customer's request


            @default true
      required:
        - message
    TransferPlan:
      type: object
      properties:
        mode:
          $ref: '#/components/schemas/TransferPlanMode'
          description: >-
            This configures how transfer is executed and the experience of the
            destination party receiving the call.


            Usage:

            - `blind-transfer`: The assistant forwards the call to the
            destination without any message or summary.

            - `blind-transfer-add-summary-to-sip-header`: The assistant forwards
            the call to the destination and adds a SIP header X-Transfer-Summary
            to the call to include the summary.

            - `warm-transfer-say-message`: The assistant dials the destination,
            delivers the `message` to the destination party, connects the
            customer, and leaves the call.

            - `warm-transfer-say-summary`: The assistant dials the destination,
            provides a summary of the call to the destination party, connects
            the customer, and leaves the call.

            -
            `warm-transfer-wait-for-operator-to-speak-first-and-then-say-message`:
            The assistant dials the destination, waits for the operator to
            speak, delivers the `message` to the destination party, and then
            connects the customer.

            -
            `warm-transfer-wait-for-operator-to-speak-first-and-then-say-summary`:
            The assistant dials the destination, waits for the operator to
            speak, provides a summary of the call to the destination party, and
            then connects the customer.

            - `warm-transfer-twiml`: The assistant dials the destination,
            executes the twiml instructions on the destination call leg,
            connects the customer, and leaves the call.

            - `warm-transfer-experimental`: The assistant puts the customer on
            hold, dials the destination, and if the destination answers (and is
            human), delivers a message or summary before connecting the
            customer. If the destination is unreachable or not human (e.g., with
            voicemail detection), the assistant delivers the `fallbackMessage`
            to the customer and optionally ends the call.


            @default 'blind-transfer'
        message:
          $ref: '#/components/schemas/TransferPlanMessage'
          description: >-
            This is the message the assistant will deliver to the destination
            party before connecting the customer.


            Usage:

            - Used only when `mode` is
            `blind-transfer-add-summary-to-sip-header`,
            `warm-transfer-say-message`,
            `warm-transfer-wait-for-operator-to-speak-first-and-then-say-message`,
            or `warm-transfer-experimental`.
        timeout:
          type: number
          format: double
          default: 60
          description: >-
            This is the timeout in seconds for the
            warm-transfer-wait-for-operator-to-speak-first-and-then-say-message/summary


            @default 60
        sipVerb:
          $ref: '#/components/schemas/TransferPlanSipVerb'
          description: |-
            This specifies the SIP verb to use while transferring the call.
            - 'refer': Uses SIP REFER to transfer the call (default)
            - 'bye': Ends current call with SIP BYE
            - 'dial': Uses SIP DIAL to transfer the call
        dialTimeout:
          type: number
          format: double
          default: 60
          description: >-
            This sets the timeout for the dial operation in seconds. This is the
            duration the call will ring before timing out.


            Only applicable when `sipVerb='dial'`. Not applicable for SIP REFER
            or BYE.


            @default 60
        holdAudioUrl:
          type: string
          description: >-
            This is the URL to an audio file played while the customer is on
            hold during transfer.


            Usage:

            - Used only when `mode` is `warm-transfer-experimental`.

            - Used when transferring calls to play hold audio for the customer.

            - Must be a publicly accessible URL to an audio file.

            - Supported formats: MP3 and WAV.

            - If not provided, the default hold audio will be used.
        transferCompleteAudioUrl:
          type: string
          description: >-
            This is the URL to an audio file played after the warm transfer
            message or summary is delivered to the destination party.

            It can be used to play a custom sound like 'beep' to notify that the
            transfer is complete.


            Usage:

            - Used only when `mode` is `warm-transfer-experimental`.

            - Used when transferring calls to play hold audio for the
            destination party.

            - Must be a publicly accessible URL to an audio file.

            - Supported formats: MP3 and WAV.
        contextEngineeringPlan:
          $ref: '#/components/schemas/TransferPlanContextEngineeringPlan'
          description: >-
            This is the plan for manipulating the message context before
            initiating the warm transfer.

            Usage:

            - Used only when `mode` is `warm-transfer-experimental`.

            - These messages will automatically be added to the
            transferAssistant's system message.

            - If 'none', we will not add any transcript to the
            transferAssistant's system message.

            - If you want to provide your own messages, use
            transferAssistant.model.messages instead.


            @default { type: 'all' }
        twiml:
          type: string
          description: >-
            This is the TwiML instructions to execute on the destination call
            leg before connecting the customer.


            Usage:

            - Used only when `mode` is `warm-transfer-twiml`.

            - Supports only `Play`, `Say`, `Gather`, `Hangup` and `Pause` verbs.

            - Maximum length is 4096 characters.


            Example:

            ```

            <Say voice="alice" language="en-US">Hello, transferring a customer
            to you.</Say>

            <Pause length="2"/>

            <Say>They called about billing questions.</Say>

            ```
        summaryPlan:
          $ref: '#/components/schemas/SummaryPlan'
          description: >-
            This is the plan for generating a summary of the call to present to
            the destination party.


            Usage:

            - Used only when `mode` is
            `blind-transfer-add-summary-to-sip-header` or
            `warm-transfer-say-summary` or
            `warm-transfer-wait-for-operator-to-speak-first-and-then-say-summary`
            or `warm-transfer-experimental`.
        sipHeadersInReferToEnabled:
          type: boolean
          description: >-
            This flag includes the sipHeaders from above in the refer to sip uri
            as url encoded query params.


            @default false
        fallbackPlan:
          $ref: '#/components/schemas/TransferFallbackPlan'
          description: >-
            This configures the fallback plan when the transfer fails
            (destination unreachable, busy, or not human).


            Usage:

            - Used only when `mode` is `warm-transfer-experimental`.

            - If not provided when using `warm-transfer-experimental`, a default
            message will be used.
      required:
        - mode
    TransferDestinationNumber:
      type: object
      properties:
        message:
          $ref: '#/components/schemas/TransferDestinationNumberMessage'
          description: >-
            This is spoken to the customer before connecting them to the
            destination.


            Usage:

            - If this is not provided and transfer tool messages is not
            provided, default is "Transferring the call now".

            - If set to "", nothing is spoken. This is useful when you want to
            silently transfer. This is especially useful when transferring
            between assistants in a squad. In this scenario, you likely also
            want to set
            `assistant.firstMessageMode=assistant-speaks-first-with-model-generated-message`
            for the destination assistant.


            This accepts a string or a ToolMessageStart class. Latter is useful
            if you want to specify multiple messages for different languages
            through the `contents` field.
        type:
          $ref: '#/components/schemas/TransferDestinationNumberType'
        numberE164CheckEnabled:
          type: boolean
          default: true
          description: >-
            This is the flag to toggle the E164 check for the `number` field.
            This is an advanced property which should be used if you know your
            use case requires it.


            Use cases:

            - `false`: To allow non-E164 numbers like `+001234567890`, `1234`,
            or `abc`. This is useful for dialing out to non-E164 numbers on your
            SIP trunks.

            - `true` (default): To allow only E164 numbers like `+14155551234`.
            This is standard for PSTN calls.


            If `false`, the `number` is still required to only contain
            alphanumeric characters (regex: `/^\+?[a-zA-Z0-9]+$/`).


            @default true (E164 check is enabled)
        number:
          type: string
          description: This is the phone number to transfer the call to.
        extension:
          type: string
          description: >-
            This is the extension to dial after transferring the call to the
            `number`.
        callerId:
          type: string
          description: >-
            This is the caller ID to use when transferring the call to the
            `number`.


            Usage:

            - If not provided, the caller ID will be the number the call is
            coming **from**.
              Example: a customer with number +14151111111 calls in to and the assistant transfers out to +16470000000. +16470000000 will see +14151111111 as the caller.
              For inbound calls, the caller ID is the customer's number. For outbound calls, the caller ID is the phone number of the assistant.
            - To change this behavior, provide a `callerId`.

            - Set to '{{customer.number}}' to always use the customer's number
            as the caller ID.

            - Set to '{{phoneNumber.number}}' to always use the phone number of
            the assistant as the caller ID.

            - Set to any E164 number to always use that number as the caller ID.
            This needs to be a number that is owned or verified by your
            Transport provider like Twilio.


            For Twilio, you can read up more here:
            https://www.twilio.com/docs/voice/twiml/dial#callerid
        transferPlan:
          $ref: '#/components/schemas/TransferPlan'
          description: >-
            This configures how transfer is executed and the experience of the
            destination party receiving the call. Defaults to `blind-transfer`.


            @default `transferPlan.mode='blind-transfer'`
        description:
          type: string
          description: >-
            This is the description of the destination, used by the AI to choose
            when and how to transfer the call.
      required:
        - type
        - number
    TransferDestinationSipMessage:
      oneOf:
        - type: string
        - $ref: '#/components/schemas/CustomMessage'
    TransferDestinationSipType:
      type: string
      enum:
        - value: sip
    TransferDestinationSipSipHeaders:
      type: object
      properties: {}
    TransferDestinationSip:
      type: object
      properties:
        message:
          $ref: '#/components/schemas/TransferDestinationSipMessage'
          description: >-
            This is spoken to the customer before connecting them to the
            destination.


            Usage:

            - If this is not provided and transfer tool messages is not
            provided, default is "Transferring the call now".

            - If set to "", nothing is spoken. This is useful when you want to
            silently transfer. This is especially useful when transferring
            between assistants in a squad. In this scenario, you likely also
            want to set
            `assistant.firstMessageMode=assistant-speaks-first-with-model-generated-message`
            for the destination assistant.


            This accepts a string or a ToolMessageStart class. Latter is useful
            if you want to specify multiple messages for different languages
            through the `contents` field.
        type:
          $ref: '#/components/schemas/TransferDestinationSipType'
        sipUri:
          type: string
          description: This is the SIP URI to transfer the call to.
        transferPlan:
          $ref: '#/components/schemas/TransferPlan'
          description: >-
            This configures how transfer is executed and the experience of the
            destination party receiving the call. Defaults to `blind-transfer`.


            @default `transferPlan.mode='blind-transfer'`
        sipHeaders:
          $ref: '#/components/schemas/TransferDestinationSipSipHeaders'
          description: >-
            These are custom headers to be added to SIP refer during transfer
            call.
        description:
          type: string
          description: >-
            This is the description of the destination, used by the AI to choose
            when and how to transfer the call.
      required:
        - type
        - sipUri
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingTransferCallDestinationsItems:
      oneOf:
        - $ref: '#/components/schemas/TransferDestinationAssistant'
        - $ref: '#/components/schemas/TransferDestinationNumber'
        - $ref: '#/components/schemas/TransferDestinationSip'
    CreateTransferCallToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingTransferCallMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        destinations:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingTransferCallDestinationsItems
          description: >-
            These are the destinations that the call can be transferred to. If
            no destinations are provided, server.url will be used to get the
            transfer destination once the tool is called.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingSipRequestMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingSipRequestVerb:
      type: string
      enum:
        - value: INFO
        - value: MESSAGE
        - value: NOTIFY
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingSipRequestBody:
      oneOf:
        - type: string
        - $ref: '#/components/schemas/JsonSchema'
    CreateSipRequestToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingSipRequestMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        verb:
          $ref: >-
            #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingSipRequestVerb
          description: The SIP method to send.
        headers:
          $ref: '#/components/schemas/JsonSchema'
          description: >-
            JSON schema for headers the model should populate when sending the
            SIP request.
        body:
          $ref: >-
            #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingSipRequestBody
          description: >-
            Body to include in the SIP request. Either a literal string body, or
            a JSON schema describing a structured body that the model should
            populate.
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
      required:
        - verb
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingVoicemailMessagesItems:
      oneOf:
        - $ref: '#/components/schemas/ToolMessageStart'
        - $ref: '#/components/schemas/ToolMessageComplete'
        - $ref: '#/components/schemas/ToolMessageFailed'
        - $ref: '#/components/schemas/ToolMessageDelayed'
    CreateVoicemailToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingVoicemailMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        beepDetectionEnabled:
          type: boolean
          default: false
          description: >-
            This is the flag that enables beep detection for voicemail detection
            and applies only for twilio based calls.


            @default false
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    AnthropicBedrockModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    CreateCustomKnowledgeBaseDtoProvider:
      type: string
      enum:
        - value: custom-knowledge-base
    CreateCustomKnowledgeBaseDTO:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/CreateCustomKnowledgeBaseDtoProvider'
          description: This knowledge base is bring your own knowledge base implementation.
        server:
          $ref: '#/components/schemas/Server'
          description: |-
            This is where the knowledge base request will be sent.

            Request Example:

            POST https://{server.url}
            Content-Type: application/json

            {
              "messsage": {
                "type": "knowledge-base-request",
                "messages": [
                  {
                    "role": "user",
                    "content": "Why is ocean blue?"
                  }
                ],
                ...other metadata about the call...
              }
            }

            Response Expected:
            ```
            {
              "message": {
                 "role": "assistant",
                 "content": "The ocean is blue because water absorbs everything but blue.",
              }, // YOU CAN RETURN THE EXACT RESPONSE TO SPEAK
              "documents": [
                {
                  "content": "The ocean is blue primarily because water absorbs colors in the red part of the light spectrum and scatters the blue light, making it more visible to our eyes.",
                  "similarity": 1
                },
                {
                  "content": "Blue light is scattered more by the water molecules than other colors, enhancing the blue appearance of the ocean.",
                  "similarity": .5
                }
              ] // OR, YOU CAN RETURN AN ARRAY OF DOCUMENTS THAT WILL BE SENT TO THE MODEL
            }
            ```
      required:
        - provider
        - server
    AnthropicBedrockModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    AnthropicBedrockModelProvider:
      type: string
      enum:
        - value: anthropic-bedrock
    AnthropicBedrockModelModel:
      type: string
      enum:
        - value: claude-3-opus-20240229
        - value: claude-3-sonnet-20240229
        - value: claude-3-haiku-20240307
        - value: claude-3-5-sonnet-20240620
        - value: claude-3-5-sonnet-20241022
        - value: claude-3-5-haiku-20241022
        - value: claude-3-7-sonnet-20250219
        - value: claude-opus-4-20250514
        - value: claude-opus-4-5-20251101
        - value: claude-opus-4-6
        - value: claude-sonnet-4-20250514
        - value: claude-sonnet-4-5-20250929
        - value: claude-haiku-4-5-20251001
    AnthropicThinkingConfigType:
      type: string
      enum:
        - value: enabled
    AnthropicThinkingConfig:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/AnthropicThinkingConfigType'
        budgetTokens:
          type: number
          format: double
          description: |-
            The maximum number of tokens to allocate for thinking.
            Must be between 1024 and 100000 tokens.
      required:
        - type
        - budgetTokens
    AnthropicBedrockModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/AnthropicBedrockModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/AnthropicBedrockModelKnowledgeBase'
          description: These are the options for the knowledge base.
        provider:
          $ref: '#/components/schemas/AnthropicBedrockModelProvider'
          description: The provider identifier for Anthropic via AWS Bedrock.
        model:
          $ref: '#/components/schemas/AnthropicBedrockModelModel'
          description: The specific Anthropic/Claude model that will be used via Bedrock.
        thinking:
          $ref: '#/components/schemas/AnthropicThinkingConfig'
          description: |-
            Optional configuration for Anthropic's thinking feature.
            Only applicable for claude-3-7-sonnet-20250219 model.
            If provided, maxTokens must be greater than thinking.budgetTokens.
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - provider
        - model
    AnyscaleModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    AnyscaleModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    AnyscaleModelProvider:
      type: string
      enum:
        - value: anyscale
    AnyscaleModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/AnyscaleModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/AnyscaleModelKnowledgeBase'
          description: These are the options for the knowledge base.
        provider:
          $ref: '#/components/schemas/AnyscaleModelProvider'
        model:
          type: string
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - provider
        - model
    CerebrasModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    CerebrasModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    CerebrasModelModel:
      type: string
      enum:
        - value: llama3.1-8b
        - value: llama-3.3-70b
    CerebrasModelProvider:
      type: string
      enum:
        - value: cerebras
    CerebrasModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/CerebrasModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/CerebrasModelKnowledgeBase'
          description: These are the options for the knowledge base.
        model:
          $ref: '#/components/schemas/CerebrasModelModel'
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        provider:
          $ref: '#/components/schemas/CerebrasModelProvider'
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - model
        - provider
    CustomLlmModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    CustomLlmModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    CustomLlmModelProvider:
      type: string
      enum:
        - value: custom-llm
    CustomLlmModelMetadataSendMode:
      type: string
      enum:
        - value: 'off'
        - value: variable
        - value: destructured
    CustomLLMModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/CustomLlmModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/CustomLlmModelKnowledgeBase'
          description: These are the options for the knowledge base.
        provider:
          $ref: '#/components/schemas/CustomLlmModelProvider'
          description: >-
            This is the provider that will be used for the model. Any service,
            including your own server, that is compatible with the OpenAI API
            can be used.
        metadataSendMode:
          $ref: '#/components/schemas/CustomLlmModelMetadataSendMode'
          description: >-
            This determines whether metadata is sent in requests to the custom
            provider.


            - `off` will not send any metadata. payload will look like `{
            messages }`

            - `variable` will send `assistant.metadata` as a variable on the
            payload. payload will look like `{ messages, metadata }`

            - `destructured` will send `assistant.metadata` fields directly on
            the payload. payload will look like `{ messages, ...metadata }`


            Further, `variable` and `destructured` will send `call`,
            `phoneNumber`, and `customer` objects in the payload.


            Default is `variable`.
        headers:
          type: object
          additionalProperties:
            type: string
          description: >-
            Custom headers to send with requests. These headers can override
            default OpenAI headers except for Authorization (which should be
            specified using a custom-llm credential).
        url:
          type: string
          description: >-
            These is the URL we'll use for the OpenAI client's `baseURL`. Ex.
            https://openrouter.ai/api/v1
        wordLevelConfidenceEnabled:
          type: boolean
          description: >-
            This determines whether the transcriber's word level confidence is
            sent in requests to the custom provider. Default is false.

            This only works for Deepgram transcribers.
        timeoutSeconds:
          type: number
          format: double
          description: >-
            This sets the timeout for the connection to the custom provider
            without needing to stream any tokens back. Default is 20 seconds.
        model:
          type: string
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - provider
        - url
        - model
    DeepInfraModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    DeepInfraModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    DeepInfraModelProvider:
      type: string
      enum:
        - value: deepinfra
    DeepInfraModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/DeepInfraModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/DeepInfraModelKnowledgeBase'
          description: These are the options for the knowledge base.
        provider:
          $ref: '#/components/schemas/DeepInfraModelProvider'
        model:
          type: string
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - provider
        - model
    DeepSeekModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    DeepSeekModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    DeepSeekModelModel:
      type: string
      enum:
        - value: deepseek-chat
        - value: deepseek-reasoner
    DeepSeekModelProvider:
      type: string
      enum:
        - value: deep-seek
    DeepSeekModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/DeepSeekModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/DeepSeekModelKnowledgeBase'
          description: These are the options for the knowledge base.
        model:
          $ref: '#/components/schemas/DeepSeekModelModel'
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        provider:
          $ref: '#/components/schemas/DeepSeekModelProvider'
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - model
        - provider
    GoogleModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    GoogleModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    GoogleModelModel:
      type: string
      enum:
        - value: gemini-3-flash-preview
        - value: gemini-2.5-pro
        - value: gemini-2.5-flash
        - value: gemini-2.5-flash-lite
        - value: gemini-2.0-flash-thinking-exp
        - value: gemini-2.0-pro-exp-02-05
        - value: gemini-2.0-flash
        - value: gemini-2.0-flash-lite
        - value: gemini-2.0-flash-exp
        - value: gemini-2.0-flash-realtime-exp
        - value: gemini-1.5-flash
        - value: gemini-1.5-flash-002
        - value: gemini-1.5-pro
        - value: gemini-1.5-pro-002
        - value: gemini-1.0-pro
    GoogleModelProvider:
      type: string
      enum:
        - value: google
    GeminiMultimodalLivePrebuiltVoiceConfigVoiceName:
      type: string
      enum:
        - value: Puck
        - value: Charon
        - value: Kore
        - value: Fenrir
        - value: Aoede
    GeminiMultimodalLivePrebuiltVoiceConfig:
      type: object
      properties:
        voiceName:
          $ref: >-
            #/components/schemas/GeminiMultimodalLivePrebuiltVoiceConfigVoiceName
      required:
        - voiceName
    GeminiMultimodalLiveVoiceConfig:
      type: object
      properties:
        prebuiltVoiceConfig:
          $ref: '#/components/schemas/GeminiMultimodalLivePrebuiltVoiceConfig'
      required:
        - prebuiltVoiceConfig
    GeminiMultimodalLiveSpeechConfig:
      type: object
      properties:
        voiceConfig:
          $ref: '#/components/schemas/GeminiMultimodalLiveVoiceConfig'
      required:
        - voiceConfig
    GoogleRealtimeConfig:
      type: object
      properties:
        topP:
          type: number
          format: double
          description: >-
            This is the nucleus sampling parameter that controls the cumulative
            probability of tokens considered during text generation.

            Only applicable with the Gemini Flash 2.0 Multimodal Live API.
        topK:
          type: number
          format: double
          description: >-
            This is the top-k sampling parameter that limits the number of
            highest probability tokens considered during text generation.

            Only applicable with the Gemini Flash 2.0 Multimodal Live API.
        presencePenalty:
          type: number
          format: double
          description: >-
            This is the presence penalty parameter that influences the model's
            likelihood to repeat information by penalizing tokens based on their
            presence in the text.

            Only applicable with the Gemini Flash 2.0 Multimodal Live API.
        frequencyPenalty:
          type: number
          format: double
          description: >-
            This is the frequency penalty parameter that influences the model's
            likelihood to repeat tokens by penalizing them based on their
            frequency in the text.

            Only applicable with the Gemini Flash 2.0 Multimodal Live API.
        speechConfig:
          $ref: '#/components/schemas/GeminiMultimodalLiveSpeechConfig'
          description: >-
            This is the speech configuration object that defines the voice
            settings to be used for the model's speech output.

            Only applicable with the Gemini Flash 2.0 Multimodal Live API.
    GoogleModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/GoogleModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/GoogleModelKnowledgeBase'
          description: These are the options for the knowledge base.
        model:
          $ref: '#/components/schemas/GoogleModelModel'
          description: This is the Google model that will be used.
        provider:
          $ref: '#/components/schemas/GoogleModelProvider'
        realtimeConfig:
          $ref: '#/components/schemas/GoogleRealtimeConfig'
          description: >-
            This is the session configuration for the Gemini Flash 2.0
            Multimodal Live API.

            Only applicable if the model `gemini-2.0-flash-realtime-exp` is
            selected.
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - model
        - provider
    GroqModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    GroqModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    GroqModelModel:
      type: string
      enum:
        - value: openai/gpt-oss-20b
        - value: openai/gpt-oss-120b
        - value: deepseek-r1-distill-llama-70b
        - value: llama-3.3-70b-versatile
        - value: llama-3.1-405b-reasoning
        - value: llama-3.1-8b-instant
        - value: llama3-8b-8192
        - value: llama3-70b-8192
        - value: gemma2-9b-it
        - value: moonshotai/kimi-k2-instruct-0905
        - value: meta-llama/llama-4-maverick-17b-128e-instruct
        - value: meta-llama/llama-4-scout-17b-16e-instruct
        - value: mistral-saba-24b
        - value: compound-beta
        - value: compound-beta-mini
    GroqModelProvider:
      type: string
      enum:
        - value: groq
    GroqModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/GroqModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/GroqModelKnowledgeBase'
          description: These are the options for the knowledge base.
        model:
          $ref: '#/components/schemas/GroqModelModel'
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        provider:
          $ref: '#/components/schemas/GroqModelProvider'
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - model
        - provider
    InflectionAiModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    InflectionAiModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    InflectionAiModelModel:
      type: string
      enum:
        - value: inflection_3_pi
    InflectionAiModelProvider:
      type: string
      enum:
        - value: inflection-ai
    InflectionAIModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/InflectionAiModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/InflectionAiModelKnowledgeBase'
          description: These are the options for the knowledge base.
        model:
          $ref: '#/components/schemas/InflectionAiModelModel'
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        provider:
          $ref: '#/components/schemas/InflectionAiModelProvider'
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - model
        - provider
    OpenAiModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    OpenAiModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    OpenAiModelProvider:
      type: string
      enum:
        - value: openai
    OpenAiModelModel:
      type: string
      enum:
        - value: gpt-5.2
        - value: gpt-5.2-chat-latest
        - value: gpt-5.1
        - value: gpt-5.1-chat-latest
        - value: gpt-5
        - value: gpt-5-chat-latest
        - value: gpt-5-mini
        - value: gpt-5-nano
        - value: gpt-4.1-2025-04-14
        - value: gpt-4.1-mini-2025-04-14
        - value: gpt-4.1-nano-2025-04-14
        - value: gpt-4.1
        - value: gpt-4.1-mini
        - value: gpt-4.1-nano
        - value: chatgpt-4o-latest
        - value: o3
        - value: o3-mini
        - value: o4-mini
        - value: o1-mini
        - value: o1-mini-2024-09-12
        - value: gpt-4o-realtime-preview-2024-10-01
        - value: gpt-4o-realtime-preview-2024-12-17
        - value: gpt-4o-mini-realtime-preview-2024-12-17
        - value: gpt-realtime-2025-08-28
        - value: gpt-realtime-mini-2025-12-15
        - value: gpt-4o-mini-2024-07-18
        - value: gpt-4o-mini
        - value: gpt-4o
        - value: gpt-4o-2024-05-13
        - value: gpt-4o-2024-08-06
        - value: gpt-4o-2024-11-20
        - value: gpt-4-turbo
        - value: gpt-4-turbo-2024-04-09
        - value: gpt-4-turbo-preview
        - value: gpt-4-0125-preview
        - value: gpt-4-1106-preview
        - value: gpt-4
        - value: gpt-4-0613
        - value: gpt-3.5-turbo
        - value: gpt-3.5-turbo-0125
        - value: gpt-3.5-turbo-1106
        - value: gpt-3.5-turbo-16k
        - value: gpt-3.5-turbo-0613
        - value: gpt-4.1-2025-04-14:westus
        - value: gpt-4.1-2025-04-14:eastus2
        - value: gpt-4.1-2025-04-14:eastus
        - value: gpt-4.1-2025-04-14:westus3
        - value: gpt-4.1-2025-04-14:northcentralus
        - value: gpt-4.1-2025-04-14:southcentralus
        - value: gpt-4.1-2025-04-14:westeurope
        - value: gpt-4.1-2025-04-14:germanywestcentral
        - value: gpt-4.1-2025-04-14:polandcentral
        - value: gpt-4.1-2025-04-14:spaincentral
        - value: gpt-4.1-mini-2025-04-14:westus
        - value: gpt-4.1-mini-2025-04-14:eastus2
        - value: gpt-4.1-mini-2025-04-14:eastus
        - value: gpt-4.1-mini-2025-04-14:westus3
        - value: gpt-4.1-mini-2025-04-14:northcentralus
        - value: gpt-4.1-mini-2025-04-14:southcentralus
        - value: gpt-4.1-mini-2025-04-14:westeurope
        - value: gpt-4.1-mini-2025-04-14:germanywestcentral
        - value: gpt-4.1-mini-2025-04-14:polandcentral
        - value: gpt-4.1-mini-2025-04-14:spaincentral
        - value: gpt-4.1-nano-2025-04-14:westus
        - value: gpt-4.1-nano-2025-04-14:eastus2
        - value: gpt-4.1-nano-2025-04-14:westus3
        - value: gpt-4.1-nano-2025-04-14:northcentralus
        - value: gpt-4.1-nano-2025-04-14:southcentralus
        - value: gpt-4o-2024-11-20:swedencentral
        - value: gpt-4o-2024-11-20:westus
        - value: gpt-4o-2024-11-20:eastus2
        - value: gpt-4o-2024-11-20:eastus
        - value: gpt-4o-2024-11-20:westus3
        - value: gpt-4o-2024-11-20:southcentralus
        - value: gpt-4o-2024-11-20:westeurope
        - value: gpt-4o-2024-11-20:germanywestcentral
        - value: gpt-4o-2024-11-20:polandcentral
        - value: gpt-4o-2024-11-20:spaincentral
        - value: gpt-4o-2024-08-06:westus
        - value: gpt-4o-2024-08-06:westus3
        - value: gpt-4o-2024-08-06:eastus
        - value: gpt-4o-2024-08-06:eastus2
        - value: gpt-4o-2024-08-06:northcentralus
        - value: gpt-4o-2024-08-06:southcentralus
        - value: gpt-4o-mini-2024-07-18:westus
        - value: gpt-4o-mini-2024-07-18:westus3
        - value: gpt-4o-mini-2024-07-18:eastus
        - value: gpt-4o-mini-2024-07-18:eastus2
        - value: gpt-4o-mini-2024-07-18:northcentralus
        - value: gpt-4o-mini-2024-07-18:southcentralus
        - value: gpt-4o-2024-05-13:eastus2
        - value: gpt-4o-2024-05-13:eastus
        - value: gpt-4o-2024-05-13:northcentralus
        - value: gpt-4o-2024-05-13:southcentralus
        - value: gpt-4o-2024-05-13:westus3
        - value: gpt-4o-2024-05-13:westus
        - value: gpt-4-turbo-2024-04-09:eastus2
        - value: gpt-4-0125-preview:eastus
        - value: gpt-4-0125-preview:northcentralus
        - value: gpt-4-0125-preview:southcentralus
        - value: gpt-4-1106-preview:australia
        - value: gpt-4-1106-preview:canadaeast
        - value: gpt-4-1106-preview:france
        - value: gpt-4-1106-preview:india
        - value: gpt-4-1106-preview:norway
        - value: gpt-4-1106-preview:swedencentral
        - value: gpt-4-1106-preview:uk
        - value: gpt-4-1106-preview:westus
        - value: gpt-4-1106-preview:westus3
        - value: gpt-4-0613:canadaeast
        - value: gpt-3.5-turbo-0125:canadaeast
        - value: gpt-3.5-turbo-0125:northcentralus
        - value: gpt-3.5-turbo-0125:southcentralus
        - value: gpt-3.5-turbo-1106:canadaeast
        - value: gpt-3.5-turbo-1106:westus
    OpenAiModelFallbackModelsItems:
      type: string
      enum:
        - value: gpt-5.2
        - value: gpt-5.2-chat-latest
        - value: gpt-5.1
        - value: gpt-5.1-chat-latest
        - value: gpt-5
        - value: gpt-5-chat-latest
        - value: gpt-5-mini
        - value: gpt-5-nano
        - value: gpt-4.1-2025-04-14
        - value: gpt-4.1-mini-2025-04-14
        - value: gpt-4.1-nano-2025-04-14
        - value: gpt-4.1
        - value: gpt-4.1-mini
        - value: gpt-4.1-nano
        - value: chatgpt-4o-latest
        - value: o3
        - value: o3-mini
        - value: o4-mini
        - value: o1-mini
        - value: o1-mini-2024-09-12
        - value: gpt-4o-realtime-preview-2024-10-01
        - value: gpt-4o-realtime-preview-2024-12-17
        - value: gpt-4o-mini-realtime-preview-2024-12-17
        - value: gpt-realtime-2025-08-28
        - value: gpt-realtime-mini-2025-12-15
        - value: gpt-4o-mini-2024-07-18
        - value: gpt-4o-mini
        - value: gpt-4o
        - value: gpt-4o-2024-05-13
        - value: gpt-4o-2024-08-06
        - value: gpt-4o-2024-11-20
        - value: gpt-4-turbo
        - value: gpt-4-turbo-2024-04-09
        - value: gpt-4-turbo-preview
        - value: gpt-4-0125-preview
        - value: gpt-4-1106-preview
        - value: gpt-4
        - value: gpt-4-0613
        - value: gpt-3.5-turbo
        - value: gpt-3.5-turbo-0125
        - value: gpt-3.5-turbo-1106
        - value: gpt-3.5-turbo-16k
        - value: gpt-3.5-turbo-0613
        - value: gpt-4.1-2025-04-14:westus
        - value: gpt-4.1-2025-04-14:eastus2
        - value: gpt-4.1-2025-04-14:eastus
        - value: gpt-4.1-2025-04-14:westus3
        - value: gpt-4.1-2025-04-14:northcentralus
        - value: gpt-4.1-2025-04-14:southcentralus
        - value: gpt-4.1-2025-04-14:westeurope
        - value: gpt-4.1-2025-04-14:germanywestcentral
        - value: gpt-4.1-2025-04-14:polandcentral
        - value: gpt-4.1-2025-04-14:spaincentral
        - value: gpt-4.1-mini-2025-04-14:westus
        - value: gpt-4.1-mini-2025-04-14:eastus2
        - value: gpt-4.1-mini-2025-04-14:eastus
        - value: gpt-4.1-mini-2025-04-14:westus3
        - value: gpt-4.1-mini-2025-04-14:northcentralus
        - value: gpt-4.1-mini-2025-04-14:southcentralus
        - value: gpt-4.1-mini-2025-04-14:westeurope
        - value: gpt-4.1-mini-2025-04-14:germanywestcentral
        - value: gpt-4.1-mini-2025-04-14:polandcentral
        - value: gpt-4.1-mini-2025-04-14:spaincentral
        - value: gpt-4.1-nano-2025-04-14:westus
        - value: gpt-4.1-nano-2025-04-14:eastus2
        - value: gpt-4.1-nano-2025-04-14:westus3
        - value: gpt-4.1-nano-2025-04-14:northcentralus
        - value: gpt-4.1-nano-2025-04-14:southcentralus
        - value: gpt-4o-2024-11-20:swedencentral
        - value: gpt-4o-2024-11-20:westus
        - value: gpt-4o-2024-11-20:eastus2
        - value: gpt-4o-2024-11-20:eastus
        - value: gpt-4o-2024-11-20:westus3
        - value: gpt-4o-2024-11-20:southcentralus
        - value: gpt-4o-2024-11-20:westeurope
        - value: gpt-4o-2024-11-20:germanywestcentral
        - value: gpt-4o-2024-11-20:polandcentral
        - value: gpt-4o-2024-11-20:spaincentral
        - value: gpt-4o-2024-08-06:westus
        - value: gpt-4o-2024-08-06:westus3
        - value: gpt-4o-2024-08-06:eastus
        - value: gpt-4o-2024-08-06:eastus2
        - value: gpt-4o-2024-08-06:northcentralus
        - value: gpt-4o-2024-08-06:southcentralus
        - value: gpt-4o-mini-2024-07-18:westus
        - value: gpt-4o-mini-2024-07-18:westus3
        - value: gpt-4o-mini-2024-07-18:eastus
        - value: gpt-4o-mini-2024-07-18:eastus2
        - value: gpt-4o-mini-2024-07-18:northcentralus
        - value: gpt-4o-mini-2024-07-18:southcentralus
        - value: gpt-4o-2024-05-13:eastus2
        - value: gpt-4o-2024-05-13:eastus
        - value: gpt-4o-2024-05-13:northcentralus
        - value: gpt-4o-2024-05-13:southcentralus
        - value: gpt-4o-2024-05-13:westus3
        - value: gpt-4o-2024-05-13:westus
        - value: gpt-4-turbo-2024-04-09:eastus2
        - value: gpt-4-0125-preview:eastus
        - value: gpt-4-0125-preview:northcentralus
        - value: gpt-4-0125-preview:southcentralus
        - value: gpt-4-1106-preview:australia
        - value: gpt-4-1106-preview:canadaeast
        - value: gpt-4-1106-preview:france
        - value: gpt-4-1106-preview:india
        - value: gpt-4-1106-preview:norway
        - value: gpt-4-1106-preview:swedencentral
        - value: gpt-4-1106-preview:uk
        - value: gpt-4-1106-preview:westus
        - value: gpt-4-1106-preview:westus3
        - value: gpt-4-0613:canadaeast
        - value: gpt-3.5-turbo-0125:canadaeast
        - value: gpt-3.5-turbo-0125:northcentralus
        - value: gpt-3.5-turbo-0125:southcentralus
        - value: gpt-3.5-turbo-1106:canadaeast
        - value: gpt-3.5-turbo-1106:westus
    OpenAiModelToolStrictCompatibilityMode:
      type: string
      enum:
        - value: strip-parameters-with-unsupported-validation
        - value: strip-unsupported-validation
    OpenAiModelPromptCacheRetention:
      type: string
      enum:
        - value: in_memory
        - value: 24h
    OpenAIModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/OpenAiModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/OpenAiModelKnowledgeBase'
          description: These are the options for the knowledge base.
        provider:
          $ref: '#/components/schemas/OpenAiModelProvider'
          description: This is the provider that will be used for the model.
        model:
          $ref: '#/components/schemas/OpenAiModelModel'
          description: >-
            This is the OpenAI model that will be used.


            When using Vapi OpenAI or your own Azure Credentials, you have the
            option to specify the region for the selected model. This shouldn't
            be specified unless you have a specific reason to do so. Vapi will
            automatically find the fastest region that make sense.

            This is helpful when you are required to comply with Data Residency
            rules. Learn more about Azure regions here
            https://azure.microsoft.com/en-us/explore/global-infrastructure/data-residency/.


            @default undefined
        fallbackModels:
          type: array
          items:
            $ref: '#/components/schemas/OpenAiModelFallbackModelsItems'
          description: >-
            These are the fallback models that will be used if the primary model
            fails. This shouldn't be specified unless you have a specific reason
            to do so. Vapi will automatically find the fastest fallbacks that
            make sense.
        toolStrictCompatibilityMode:
          $ref: '#/components/schemas/OpenAiModelToolStrictCompatibilityMode'
          description: >-
            Azure OpenAI doesn't support `maxLength` right now
            https://learn.microsoft.com/en-us/azure/ai-services/openai/how-to/structured-outputs?tabs=python-secure%2Cdotnet-entra-id&pivots=programming-language-csharp#unsupported-type-specific-keywords.
            Need to strip.


            - `strip-parameters-with-unsupported-validation` will strip
            parameters with unsupported validation.

            - `strip-unsupported-validation` will keep the parameters but strip
            unsupported validation.


            @default `strip-unsupported-validation`
        promptCacheRetention:
          $ref: '#/components/schemas/OpenAiModelPromptCacheRetention'
          description: >-
            This controls the prompt cache retention policy for models that
            support extended caching (GPT-4.1, GPT-5 series).


            - `in_memory`: Default behavior, cache retained in GPU memory only

            - `24h`: Extended caching, keeps cached prefixes active for up to 24
            hours by offloading to GPU-local storage


            Only applies to models: gpt-5.2, gpt-5.1, gpt-5.1-codex,
            gpt-5.1-codex-mini, gpt-5.1-chat-latest, gpt-5, gpt-5-codex, gpt-4.1


            @default undefined (uses API default which is 'in_memory')
        promptCacheKey:
          type: string
          description: >-
            This is the prompt cache key for models that support extended
            caching (GPT-4.1, GPT-5 series).


            Providing a cache key allows you to share cached prefixes across
            requests.


            @default undefined
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - provider
        - model
    OpenRouterModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    OpenRouterModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    OpenRouterModelProvider:
      type: string
      enum:
        - value: openrouter
    OpenRouterModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/OpenRouterModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/OpenRouterModelKnowledgeBase'
          description: These are the options for the knowledge base.
        provider:
          $ref: '#/components/schemas/OpenRouterModelProvider'
        model:
          type: string
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - provider
        - model
    PerplexityAiModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    PerplexityAiModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    PerplexityAiModelProvider:
      type: string
      enum:
        - value: perplexity-ai
    PerplexityAIModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/PerplexityAiModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/PerplexityAiModelKnowledgeBase'
          description: These are the options for the knowledge base.
        provider:
          $ref: '#/components/schemas/PerplexityAiModelProvider'
        model:
          type: string
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - provider
        - model
    TogetherAiModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    TogetherAiModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    TogetherAiModelProvider:
      type: string
      enum:
        - value: together-ai
    TogetherAIModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/TogetherAiModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/TogetherAiModelKnowledgeBase'
          description: These are the options for the knowledge base.
        provider:
          $ref: '#/components/schemas/TogetherAiModelProvider'
        model:
          type: string
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - provider
        - model
    XaiModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    XaiModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    XaiModelModel:
      type: string
      enum:
        - value: grok-beta
        - value: grok-2
        - value: grok-3
        - value: grok-4-fast-reasoning
        - value: grok-4-fast-non-reasoning
    XaiModelProvider:
      type: string
      enum:
        - value: xai
    XaiModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/XaiModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/XaiModelKnowledgeBase'
          description: These are the options for the knowledge base.
        model:
          $ref: '#/components/schemas/XaiModelModel'
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        provider:
          $ref: '#/components/schemas/XaiModelProvider'
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - model
        - provider
    AssistantOverridesModel:
      oneOf:
        - $ref: '#/components/schemas/AnthropicModel'
        - $ref: '#/components/schemas/AnthropicBedrockModel'
        - $ref: '#/components/schemas/AnyscaleModel'
        - $ref: '#/components/schemas/CerebrasModel'
        - $ref: '#/components/schemas/CustomLLMModel'
        - $ref: '#/components/schemas/DeepInfraModel'
        - $ref: '#/components/schemas/DeepSeekModel'
        - $ref: '#/components/schemas/GoogleModel'
        - $ref: '#/components/schemas/GroqModel'
        - $ref: '#/components/schemas/InflectionAIModel'
        - $ref: '#/components/schemas/OpenAIModel'
        - $ref: '#/components/schemas/OpenRouterModel'
        - $ref: '#/components/schemas/PerplexityAIModel'
        - $ref: '#/components/schemas/TogetherAIModel'
        - $ref: '#/components/schemas/XaiModel'
    AzureVoiceProvider:
      type: string
      enum:
        - value: azure
    AzureVoiceId0:
      type: string
      enum:
        - value: andrew
        - value: brian
        - value: emma
    AzureVoiceId:
      oneOf:
        - $ref: '#/components/schemas/AzureVoiceId0'
        - type: string
    ChunkPlanPunctuationBoundaries:
      type: string
      enum:
        - value: 。
        - value: ，
        - value: .
        - value: '!'
        - value: '?'
        - value: ;
        - value: )
        - value: ،
        - value: ۔
        - value: ।
        - value: ॥
        - value: '|'
        - value: '||'
        - value: ','
        - value: ':'
    ExactReplacementType:
      type: string
      enum:
        - value: exact
    ExactReplacement:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/ExactReplacementType'
          description: >-
            This is the exact replacement type. You can use this to replace a
            specific word or phrase with a different word or phrase.


            Usage:

            - Replace "hello" with "hi": { type: 'exact', key: 'hello', value:
            'hi' }

            - Replace "good morning" with "good day": { type: 'exact', key:
            'good morning', value: 'good day' }

            - Replace a specific name: { type: 'exact', key: 'John Doe', value:
            'Jane Smith' }

            - Replace an acronym: { type: 'exact', key: 'AI', value: 'Artificial
            Intelligence' }

            - Replace a company name with its phonetic pronunciation: { type:
            'exact', key: 'Vapi', value: 'Vappy' }
        replaceAllEnabled:
          type: boolean
          default: false
          description: >-
            This option let's you control whether to replace all instances of
            the key or only the first one. By default, it only replaces the
            first instance.

            Examples:

            - For { type: 'exact', key: 'hello', value: 'hi', replaceAllEnabled:
            false }. Before: "hello world, hello universe" | After: "hi world,
            hello universe"

            - For { type: 'exact', key: 'hello', value: 'hi', replaceAllEnabled:
            true }. Before: "hello world, hello universe" | After: "hi world, hi
            universe"

            @default false
        key:
          type: string
          description: This is the key to replace.
        value:
          type: string
          description: This is the value that will replace the match.
      required:
        - type
        - key
        - value
    RegexReplacementType:
      type: string
      enum:
        - value: regex
    RegexOptionType:
      type: string
      enum:
        - value: ignore-case
        - value: whole-word
        - value: multi-line
    RegexOption:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/RegexOptionType'
          description: |-
            This is the type of the regex option. Options are:
            - `ignore-case`: Ignores the case of the text being matched. Add
            - `whole-word`: Matches whole words only.
            - `multi-line`: Matches across multiple lines.
        enabled:
          type: boolean
          description: |-
            This is whether to enable the option.

            @default false
      required:
        - type
        - enabled
    RegexReplacement:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/RegexReplacementType'
          description: >-
            This is the regex replacement type. You can use this to replace a
            word or phrase that matches a pattern.


            Usage:

            - Replace all numbers with "some number": { type: 'regex', regex:
            '\\d+', value: 'some number' }

            - Replace email addresses with "[EMAIL]": { type: 'regex', regex:
            '\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', value:
            '[EMAIL]' }

            - Replace phone numbers with a formatted version: { type: 'regex',
            regex: '(\\d{3})(\\d{3})(\\d{4})', value: '($1) $2-$3' }

            - Replace all instances of "color" or "colour" with "hue": { type:
            'regex', regex: 'colou?r', value: 'hue' }

            - Capitalize the first letter of every sentence: { type: 'regex',
            regex: '(?<=\\. |^)[a-z]', value: (match) => match.toUpperCase() }
        regex:
          type: string
          description: >-
            This is the regex pattern to replace.


            Note:

            - This works by using the `string.replace` method in Node.JS. Eg.
            `"hello there".replace(/hello/g, "hi")` will return `"hi there"`.


            Hot tip:

            - In JavaScript, escape `\` when sending the regex pattern. Eg.
            `"hello\sthere"` will be sent over the wire as `"hellosthere"`. Send
            `"hello\\sthere"` instead.
        options:
          type: array
          items:
            $ref: '#/components/schemas/RegexOption'
          description: >-
            These are the options for the regex replacement. Defaults to all
            disabled.


            @default []
        value:
          type: string
          description: This is the value that will replace the match.
      required:
        - type
        - regex
        - value
    FormatPlanReplacementsItems:
      oneOf:
        - $ref: '#/components/schemas/ExactReplacement'
        - $ref: '#/components/schemas/RegexReplacement'
    FormatPlanFormattersEnabled:
      type: string
      enum:
        - value: markdown
        - value: asterisk
        - value: quote
        - value: dash
        - value: newline
        - value: colon
        - value: acronym
        - value: dollarAmount
        - value: email
        - value: date
        - value: time
        - value: distance
        - value: unit
        - value: percentage
        - value: phoneNumber
        - value: number
        - value: stripAsterisk
    FormatPlan:
      type: object
      properties:
        enabled:
          type: boolean
          description: >-
            This determines whether the chunk is formatted before being sent to
            the voice provider. This helps with enunciation. This includes phone
            numbers, emails and addresses. Default `true`.


            Usage:

            - To rely on the voice provider's formatting logic, set this to
            `false`.


            If `voice.chunkPlan.enabled` is `false`, this is automatically
            `false` since there's no chunk to format.


            @default true
        numberToDigitsCutoff:
          type: number
          format: double
          description: >-
            This is the cutoff after which a number is converted to individual
            digits instead of being spoken as words.


            Example:

            - If cutoff 2025, "12345" is converted to "1 2 3 4 5" while "1200"
            is converted to "twelve hundred".


            Usage:

            - If your use case doesn't involve IDs like zip codes, set this to a
            high value.

            - If your use case involves IDs that are shorter than 5 digits, set
            this to a lower value.


            @default 2025
        replacements:
          type: array
          items:
            $ref: '#/components/schemas/FormatPlanReplacementsItems'
          description: >-
            These are the custom replacements you can make to the chunk before
            it is sent to the voice provider.


            Usage:

            - To replace a specific word or phrase with a different word or
            phrase, use the `ExactReplacement` type. Eg. `{ type: 'exact', key:
            'hello', value: 'hi' }`

            - To replace a word or phrase that matches a pattern, use the
            `RegexReplacement` type. Eg. `{ type: 'regex', regex:
            '\\b[a-zA-Z]{5}\\b', value: 'hi' }`


            @default []
        formattersEnabled:
          $ref: '#/components/schemas/FormatPlanFormattersEnabled'
          description: >-
            List of formatters to apply. If not provided, all default formatters
            will be applied.

            If provided, only the specified formatters will be applied.

            Note: Some essential formatters like angle bracket removal will
            always be applied.

            @default undefined
    ChunkPlan:
      type: object
      properties:
        enabled:
          type: boolean
          description: >-
            This determines whether the model output is chunked before being
            sent to the voice provider. Default `true`.


            Usage:

            - To rely on the voice provider's audio generation logic, set this
            to `false`.

            - If seeing issues with quality, set this to `true`.


            If disabled, Vapi-provided audio control tokens like <flush /> will
            not work.


            @default true
        minCharacters:
          type: number
          format: double
          description: |-
            This is the minimum number of characters in a chunk.

            Usage:
            - To increase quality, set this to a higher value.
            - To decrease latency, set this to a lower value.

            @default 30
        punctuationBoundaries:
          $ref: '#/components/schemas/ChunkPlanPunctuationBoundaries'
          description: >-
            These are the punctuations that are considered valid boundaries for
            a chunk to be created.


            Usage:

            - To increase quality, constrain to fewer boundaries.

            - To decrease latency, enable all.


            Default is automatically set to balance the trade-off between
            quality and latency based on the provider.
        formatPlan:
          $ref: '#/components/schemas/FormatPlan'
          description: >-
            This is the plan for formatting the chunk before it is sent to the
            voice provider.
    FallbackAzureVoiceProvider:
      type: string
      enum:
        - value: azure
    FallbackAzureVoiceId0:
      type: string
      enum:
        - value: andrew
        - value: brian
        - value: emma
    FallbackAzureVoiceId:
      oneOf:
        - $ref: '#/components/schemas/FallbackAzureVoiceId0'
        - type: string
    FallbackAzureVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackAzureVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackAzureVoiceId'
          description: This is the provider-specific ID that will be used.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        oneOf:
          description: Any type
      required:
        - provider
        - voiceId
    FallbackCartesiaVoiceProvider:
      type: string
      enum:
        - value: cartesia
    FallbackCartesiaVoiceModel:
      type: string
      enum:
        - value: sonic-3
        - value: sonic-2
        - value: sonic-english
        - value: sonic-multilingual
        - value: sonic-preview
        - value: sonic
    FallbackCartesiaVoiceLanguage:
      type: string
      enum:
        - value: ar
        - value: bg
        - value: bn
        - value: cs
        - value: da
        - value: de
        - value: el
        - value: en
        - value: es
        - value: fi
        - value: fr
        - value: gu
        - value: he
        - value: hi
        - value: hr
        - value: hu
        - value: id
        - value: it
        - value: ja
        - value: ka
        - value: kn
        - value: ko
        - value: ml
        - value: mr
        - value: ms
        - value: nl
        - value: 'no'
        - value: pa
        - value: pl
        - value: pt
        - value: ro
        - value: ru
        - value: sk
        - value: sv
        - value: ta
        - value: te
        - value: th
        - value: tl
        - value: tr
        - value: uk
        - value: vi
        - value: zh
    CartesiaSpeedControl0:
      type: string
      enum:
        - value: slowest
        - value: slow
        - value: normal
        - value: fast
        - value: fastest
    CartesiaSpeedControl:
      oneOf:
        - $ref: '#/components/schemas/CartesiaSpeedControl0'
        - type: number
          format: double
    CartesiaExperimentalControlsEmotion:
      type: string
      enum:
        - value: anger:lowest
        - value: anger:low
        - value: anger:high
        - value: anger:highest
        - value: positivity:lowest
        - value: positivity:low
        - value: positivity:high
        - value: positivity:highest
        - value: surprise:lowest
        - value: surprise:low
        - value: surprise:high
        - value: surprise:highest
        - value: sadness:lowest
        - value: sadness:low
        - value: sadness:high
        - value: sadness:highest
        - value: curiosity:lowest
        - value: curiosity:low
        - value: curiosity:high
        - value: curiosity:highest
    CartesiaExperimentalControls:
      type: object
      properties:
        speed:
          $ref: '#/components/schemas/CartesiaSpeedControl'
        emotion:
          $ref: '#/components/schemas/CartesiaExperimentalControlsEmotion'
    CartesiaGenerationConfigExperimental:
      type: object
      properties:
        accentLocalization:
          type: integer
          default: 0
          description: >-
            Toggle accent localization for sonic-3: 0 (disabled, default) or 1
            (enabled). When enabled, the voice adapts to match the transcript
            language accent while preserving vocal characteristics.
    CartesiaGenerationConfig:
      type: object
      properties:
        speed:
          type: number
          format: double
          default: 1
          description: >-
            Fine-grained speed control for sonic-3. Only available for sonic-3
            model.
        volume:
          type: number
          format: double
          default: 1
          description: >-
            Fine-grained volume control for sonic-3. Only available for sonic-3
            model.
        experimental:
          $ref: '#/components/schemas/CartesiaGenerationConfigExperimental'
          description: >-
            Experimental model controls for sonic-3. These are subject to
            breaking changes.
    FallbackCartesiaVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackCartesiaVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          type: string
          description: The ID of the particular voice you want to use.
        model:
          $ref: '#/components/schemas/FallbackCartesiaVoiceModel'
          description: >-
            This is the model that will be used. This is optional and will
            default to the correct model for the voiceId.
        language:
          $ref: '#/components/schemas/FallbackCartesiaVoiceLanguage'
          description: >-
            This is the language that will be used. This is optional and will
            default to the correct language for the voiceId.
        experimentalControls:
          $ref: '#/components/schemas/CartesiaExperimentalControls'
          description: Experimental controls for Cartesia voice generation
        generationConfig:
          $ref: '#/components/schemas/CartesiaGenerationConfig'
          description: >-
            Generation config for fine-grained control of sonic-3 voice output
            (speed, volume, and experimental controls). Only available for
            sonic-3 model.
        pronunciationDictId:
          type: string
          description: >-
            Pronunciation dictionary ID for sonic-3. Allows custom
            pronunciations for specific words. Only available for sonic-3 model.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackHumeVoiceProvider:
      type: string
      enum:
        - value: hume
    FallbackHumeVoiceModel:
      type: string
      enum:
        - value: octave
        - value: octave2
    FallbackHumeVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackHumeVoiceProvider'
          description: This is the voice provider that will be used.
        model:
          $ref: '#/components/schemas/FallbackHumeVoiceModel'
          description: This is the model that will be used.
        voiceId:
          type: string
          description: The ID of the particular voice you want to use.
        isCustomHumeVoice:
          type: boolean
          description: >-
            Indicates whether the chosen voice is a preset Hume AI voice or a
            custom voice.
        description:
          type: string
          description: >-
            Natural language instructions describing how the synthesized speech
            should sound, including but not limited to tone, intonation, pacing,
            and accent (e.g., 'a soft, gentle voice with a strong British
            accent').


            If a Voice is specified in the request, this description serves as
            acting instructions.

            If no Voice is specified, a new voice is generated based on this
            description.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackCustomVoiceProvider:
      type: string
      enum:
        - value: custom-voice
    FallbackCustomVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackCustomVoiceProvider'
          description: >-
            This is the voice provider that will be used. Use `custom-voice` for
            providers that are not natively supported.
        server:
          $ref: '#/components/schemas/Server'
          description: >-
            This is where the voice request will be sent.


            Request Example:


            POST https://{server.url}

            Content-Type: application/json


            {
              "message": {
                "type": "voice-request",
                "text": "Hello, world!",
                "sampleRate": 24000,
                ...other metadata about the call...
              }
            }


            Response Expected: 1-channel 16-bit raw PCM audio at the sample rate
            specified in the request. Here is how the response will be piped to
            the transport:

            ```

            response.on('data', (chunk: Buffer) => {
              outputStream.write(chunk);
            });

            ```
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - server
    FallbackDeepgramVoiceProvider:
      type: string
      enum:
        - value: deepgram
    FallbackDeepgramVoiceId:
      type: string
      enum:
        - value: asteria
        - value: luna
        - value: stella
        - value: athena
        - value: hera
        - value: orion
        - value: arcas
        - value: perseus
        - value: angus
        - value: orpheus
        - value: helios
        - value: zeus
        - value: thalia
        - value: andromeda
        - value: helena
        - value: apollo
        - value: arcas
        - value: aries
        - value: amalthea
        - value: asteria
        - value: athena
        - value: atlas
        - value: aurora
        - value: callista
        - value: cora
        - value: cordelia
        - value: delia
        - value: draco
        - value: electra
        - value: harmonia
        - value: hera
        - value: hermes
        - value: hyperion
        - value: iris
        - value: janus
        - value: juno
        - value: jupiter
        - value: luna
        - value: mars
        - value: minerva
        - value: neptune
        - value: odysseus
        - value: ophelia
        - value: orion
        - value: orpheus
        - value: pandora
        - value: phoebe
        - value: pluto
        - value: saturn
        - value: selene
        - value: theia
        - value: vesta
        - value: zeus
        - value: celeste
        - value: estrella
        - value: nestor
        - value: sirio
        - value: carina
        - value: alvaro
        - value: diana
        - value: aquila
        - value: selena
        - value: javier
    FallbackDeepgramVoiceModel:
      type: string
      enum:
        - value: aura
        - value: aura-2
    FallbackDeepgramVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackDeepgramVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackDeepgramVoiceId'
          description: This is the provider-specific ID that will be used.
        model:
          $ref: '#/components/schemas/FallbackDeepgramVoiceModel'
          description: >-
            This is the model that will be used. Defaults to 'aura-2' when not
            specified.
        mipOptOut:
          type: boolean
          default: false
          description: >-
            If set to true, this will add mip_opt_out=true as a query parameter
            of all API requests. See
            https://developers.deepgram.com/docs/the-deepgram-model-improvement-partnership-program#want-to-opt-out


            This will only be used if you are using your own Deepgram API key.


            @default false
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackElevenLabsVoiceId0:
      type: string
      enum:
        - value: burt
        - value: marissa
        - value: andrea
        - value: sarah
        - value: phillip
        - value: steve
        - value: joseph
        - value: myra
        - value: paula
        - value: ryan
        - value: drew
        - value: paul
        - value: mrb
        - value: matilda
        - value: mark
    FallbackElevenLabsVoiceId:
      oneOf:
        - $ref: '#/components/schemas/FallbackElevenLabsVoiceId0'
        - type: string
    FallbackElevenLabsVoiceModel:
      type: string
      enum:
        - value: eleven_multilingual_v2
        - value: eleven_turbo_v2
        - value: eleven_turbo_v2_5
        - value: eleven_flash_v2
        - value: eleven_flash_v2_5
        - value: eleven_monolingual_v1
        - value: eleven_v3
    ElevenLabsPronunciationDictionaryLocator:
      type: object
      properties:
        pronunciationDictionaryId:
          type: string
          description: This is the ID of the pronunciation dictionary to use.
        versionId:
          type: string
          description: This is the version ID of the pronunciation dictionary to use.
      required:
        - pronunciationDictionaryId
        - versionId
    FallbackElevenLabsVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          type: string
          enum:
            - type: stringLiteral
              value: 11labs
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackElevenLabsVoiceId'
          description: >-
            This is the provider-specific ID that will be used. Ensure the Voice
            is present in your 11Labs Voice Library.
        stability:
          type: number
          format: double
          description: Defines the stability for voice settings.
        similarityBoost:
          type: number
          format: double
          description: Defines the similarity boost for voice settings.
        style:
          type: number
          format: double
          description: Defines the style for voice settings.
        useSpeakerBoost:
          type: boolean
          description: Defines the use speaker boost for voice settings.
        speed:
          type: number
          format: double
          description: Defines the speed for voice settings.
        optimizeStreamingLatency:
          type: number
          format: double
          description: >-
            Defines the optimize streaming latency for voice settings. Defaults
            to 3.
        enableSsmlParsing:
          type: boolean
          description: >-
            This enables the use of
            https://elevenlabs.io/docs/speech-synthesis/prompting#pronunciation.
            Defaults to false to save latency.


            @default false
        autoMode:
          type: boolean
          description: Defines the auto mode for voice settings. Defaults to false.
        model:
          $ref: '#/components/schemas/FallbackElevenLabsVoiceModel'
          description: >-
            This is the model that will be used. Defaults to 'eleven_turbo_v2'
            if not specified.
        language:
          type: string
          description: >-
            This is the language (ISO 639-1) that is enforced for the model.
            Currently only Turbo v2.5 supports language enforcement. For other
            models, an error will be returned if language code is provided.
        pronunciationDictionaryLocators:
          type: array
          items:
            $ref: '#/components/schemas/ElevenLabsPronunciationDictionaryLocator'
          description: This is the pronunciation dictionary locators to use.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackVapiVoiceProvider:
      type: string
      enum:
        - value: vapi
    FallbackVapiVoiceVoiceId:
      type: string
      enum:
        - value: Elliot
        - value: Kylie
        - value: Rohan
        - value: Lily
        - value: Savannah
        - value: Hana
        - value: Neha
        - value: Cole
        - value: Harry
        - value: Paige
        - value: Spencer
        - value: Leah
        - value: Tara
        - value: Jess
        - value: Leo
        - value: Dan
        - value: Mia
        - value: Zac
        - value: Zoe
    VapiPronunciationDictionaryLocator:
      type: object
      properties:
        pronunciationDictId:
          type: string
          description: The pronunciation dictionary ID
        versionId:
          type: string
          description: Version ID (only required for ElevenLabs, ignored for Cartesia)
      required:
        - pronunciationDictId
    FallbackVapiVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackVapiVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackVapiVoiceVoiceId'
          description: The voices provided by Vapi
        speed:
          type: number
          format: double
          default: 1
          description: |-
            This is the speed multiplier that will be used.

            @default 1
        pronunciationDictionary:
          type: array
          items:
            $ref: '#/components/schemas/VapiPronunciationDictionaryLocator'
          description: >-
            List of pronunciation dictionary locators for custom word
            pronunciations.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackLmntVoiceProvider:
      type: string
      enum:
        - value: lmnt
    FallbackLmntVoiceId0:
      type: string
      enum:
        - value: amy
        - value: ansel
        - value: autumn
        - value: ava
        - value: brandon
        - value: caleb
        - value: cassian
        - value: chloe
        - value: dalton
        - value: daniel
        - value: dustin
        - value: elowen
        - value: evander
        - value: huxley
        - value: james
        - value: juniper
        - value: kennedy
        - value: lauren
        - value: leah
        - value: lily
        - value: lucas
        - value: magnus
        - value: miles
        - value: morgan
        - value: natalie
        - value: nathan
        - value: noah
        - value: nyssa
        - value: oliver
        - value: paige
        - value: ryan
        - value: sadie
        - value: sophie
        - value: stella
        - value: terrence
        - value: tyler
        - value: vesper
        - value: violet
        - value: warrick
        - value: zain
        - value: zeke
        - value: zoe
    FallbackLMNTVoiceId:
      oneOf:
        - $ref: '#/components/schemas/FallbackLmntVoiceId0'
        - type: string
    FallbackLmntVoiceLanguage:
      type: string
      enum:
        - value: aa
        - value: ab
        - value: ae
        - value: af
        - value: ak
        - value: am
        - value: an
        - value: ar
        - value: as
        - value: av
        - value: ay
        - value: az
        - value: ba
        - value: be
        - value: bg
        - value: bh
        - value: bi
        - value: bm
        - value: bn
        - value: bo
        - value: br
        - value: bs
        - value: ca
        - value: ce
        - value: ch
        - value: co
        - value: cr
        - value: cs
        - value: cu
        - value: cv
        - value: cy
        - value: da
        - value: de
        - value: dv
        - value: dz
        - value: ee
        - value: el
        - value: en
        - value: eo
        - value: es
        - value: et
        - value: eu
        - value: fa
        - value: ff
        - value: fi
        - value: fj
        - value: fo
        - value: fr
        - value: fy
        - value: ga
        - value: gd
        - value: gl
        - value: gn
        - value: gu
        - value: gv
        - value: ha
        - value: he
        - value: hi
        - value: ho
        - value: hr
        - value: ht
        - value: hu
        - value: hy
        - value: hz
        - value: ia
        - value: id
        - value: ie
        - value: ig
        - value: ii
        - value: ik
        - value: io
        - value: is
        - value: it
        - value: iu
        - value: ja
        - value: jv
        - value: ka
        - value: kg
        - value: ki
        - value: kj
        - value: kk
        - value: kl
        - value: km
        - value: kn
        - value: ko
        - value: kr
        - value: ks
        - value: ku
        - value: kv
        - value: kw
        - value: ky
        - value: la
        - value: lb
        - value: lg
        - value: li
        - value: ln
        - value: lo
        - value: lt
        - value: lu
        - value: lv
        - value: mg
        - value: mh
        - value: mi
        - value: mk
        - value: ml
        - value: mn
        - value: mr
        - value: ms
        - value: mt
        - value: my
        - value: na
        - value: nb
        - value: nd
        - value: ne
        - value: ng
        - value: nl
        - value: nn
        - value: 'no'
        - value: nr
        - value: nv
        - value: ny
        - value: oc
        - value: oj
        - value: om
        - value: or
        - value: os
        - value: pa
        - value: pi
        - value: pl
        - value: ps
        - value: pt
        - value: qu
        - value: rm
        - value: rn
        - value: ro
        - value: ru
        - value: rw
        - value: sa
        - value: sc
        - value: sd
        - value: se
        - value: sg
        - value: si
        - value: sk
        - value: sl
        - value: sm
        - value: sn
        - value: so
        - value: sq
        - value: sr
        - value: ss
        - value: st
        - value: su
        - value: sv
        - value: sw
        - value: ta
        - value: te
        - value: tg
        - value: th
        - value: ti
        - value: tk
        - value: tl
        - value: tn
        - value: to
        - value: tr
        - value: ts
        - value: tt
        - value: tw
        - value: ty
        - value: ug
        - value: uk
        - value: ur
        - value: uz
        - value: ve
        - value: vi
        - value: vo
        - value: wa
        - value: wo
        - value: xh
        - value: yi
        - value: yue
        - value: yo
        - value: za
        - value: zh
        - value: zu
        - value: auto
    FallbackLMNTVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackLmntVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackLMNTVoiceId'
          description: This is the provider-specific ID that will be used.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        language:
          $ref: '#/components/schemas/FallbackLmntVoiceLanguage'
          description: Two letter ISO 639-1 language code. Use "auto" for auto-detection.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackOpenAiVoiceProvider:
      type: string
      enum:
        - value: openai
    FallbackOpenAiVoiceId0:
      type: string
      enum:
        - value: alloy
        - value: echo
        - value: fable
        - value: onyx
        - value: nova
        - value: shimmer
        - value: marin
        - value: cedar
    FallbackOpenAIVoiceId:
      oneOf:
        - $ref: '#/components/schemas/FallbackOpenAiVoiceId0'
        - type: string
    FallbackOpenAiVoiceModel:
      type: string
      enum:
        - value: tts-1
        - value: tts-1-hd
        - value: gpt-4o-mini-tts
    FallbackOpenAIVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackOpenAiVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackOpenAIVoiceId'
          description: >-
            This is the provider-specific ID that will be used.

            Please note that ash, ballad, coral, sage, and verse may only be
            used with realtime models.
        model:
          $ref: '#/components/schemas/FallbackOpenAiVoiceModel'
          description: This is the model that will be used for text-to-speech.
        instructions:
          type: string
          description: >-
            This is a prompt that allows you to control the voice of your
            generated audio.

            Does not work with 'tts-1' or 'tts-1-hd' models.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackPlayHtVoiceProvider:
      type: string
      enum:
        - value: playht
    FallbackPlayHtVoiceId0:
      type: string
      enum:
        - value: jennifer
        - value: melissa
        - value: will
        - value: chris
        - value: matt
        - value: jack
        - value: ruby
        - value: davis
        - value: donna
        - value: michael
    FallbackPlayHTVoiceId:
      oneOf:
        - $ref: '#/components/schemas/FallbackPlayHtVoiceId0'
        - type: string
    FallbackPlayHtVoiceEmotion:
      type: string
      enum:
        - value: female_happy
        - value: female_sad
        - value: female_angry
        - value: female_fearful
        - value: female_disgust
        - value: female_surprised
        - value: male_happy
        - value: male_sad
        - value: male_angry
        - value: male_fearful
        - value: male_disgust
        - value: male_surprised
    FallbackPlayHtVoiceModel:
      type: string
      enum:
        - value: PlayHT2.0
        - value: PlayHT2.0-turbo
        - value: Play3.0-mini
        - value: PlayDialog
    FallbackPlayHtVoiceLanguage:
      type: string
      enum:
        - value: afrikaans
        - value: albanian
        - value: amharic
        - value: arabic
        - value: bengali
        - value: bulgarian
        - value: catalan
        - value: croatian
        - value: czech
        - value: danish
        - value: dutch
        - value: english
        - value: french
        - value: galician
        - value: german
        - value: greek
        - value: hebrew
        - value: hindi
        - value: hungarian
        - value: indonesian
        - value: italian
        - value: japanese
        - value: korean
        - value: malay
        - value: mandarin
        - value: polish
        - value: portuguese
        - value: russian
        - value: serbian
        - value: spanish
        - value: swedish
        - value: tagalog
        - value: thai
        - value: turkish
        - value: ukrainian
        - value: urdu
        - value: xhosa
    FallbackPlayHTVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackPlayHtVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackPlayHTVoiceId'
          description: This is the provider-specific ID that will be used.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        temperature:
          type: number
          format: double
          description: >-
            A floating point number between 0, exclusive, and 2, inclusive. If
            equal to null or not provided, the model's default temperature will
            be used. The temperature parameter controls variance. Lower
            temperatures result in more predictable results, higher temperatures
            allow each run to vary more, so the voice may sound less like the
            baseline voice.
        emotion:
          $ref: '#/components/schemas/FallbackPlayHtVoiceEmotion'
          description: An emotion to be applied to the speech.
        voiceGuidance:
          type: number
          format: double
          description: >-
            A number between 1 and 6. Use lower numbers to reduce how unique
            your chosen voice will be compared to other voices.
        styleGuidance:
          type: number
          format: double
          description: >-
            A number between 1 and 30. Use lower numbers to to reduce how strong
            your chosen emotion will be. Higher numbers will create a very
            emotional performance.
        textGuidance:
          type: number
          format: double
          description: >-
            A number between 1 and 2. This number influences how closely the
            generated speech adheres to the input text. Use lower values to
            create more fluid speech, but with a higher chance of deviating from
            the input text. Higher numbers will make the generated speech more
            accurate to the input text, ensuring that the words spoken align
            closely with the provided text.
        model:
          $ref: '#/components/schemas/FallbackPlayHtVoiceModel'
          description: Playht voice model/engine to use.
        language:
          $ref: '#/components/schemas/FallbackPlayHtVoiceLanguage'
          description: The language to use for the speech.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackWellSaidVoiceProvider:
      type: string
      enum:
        - value: wellsaid
    FallbackWellSaidVoiceModel:
      type: string
      enum:
        - value: caruso
        - value: legacy
    FallbackWellSaidVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackWellSaidVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          type: string
          description: The WellSaid speaker ID to synthesize.
        model:
          $ref: '#/components/schemas/FallbackWellSaidVoiceModel'
          description: This is the model that will be used.
        enableSsml:
          type: boolean
          description: Enables limited SSML translation for input text.
        libraryIds:
          type: array
          items:
            type: string
          description: Array of library IDs to use for voice synthesis.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackRimeAiVoiceProvider:
      type: string
      enum:
        - value: rime-ai
    FallbackRimeAiVoiceId0:
      type: string
      enum:
        - value: abbie
        - value: allison
        - value: ally
        - value: alona
        - value: amber
        - value: ana
        - value: antoine
        - value: armon
        - value: brenda
        - value: brittany
        - value: carol
        - value: colin
        - value: courtney
        - value: elena
        - value: elliot
        - value: eva
        - value: geoff
        - value: gerald
        - value: hank
        - value: helen
        - value: hera
        - value: jen
        - value: joe
        - value: joy
        - value: juan
        - value: kendra
        - value: kendrick
        - value: kenneth
        - value: kevin
        - value: kris
        - value: linda
        - value: madison
        - value: marge
        - value: marina
        - value: marissa
        - value: marta
        - value: maya
        - value: nicholas
        - value: nyles
        - value: phil
        - value: reba
        - value: rex
        - value: rick
        - value: ritu
        - value: rob
        - value: rodney
        - value: rohan
        - value: rosco
        - value: samantha
        - value: sandy
        - value: selena
        - value: seth
        - value: sharon
        - value: stan
        - value: tamra
        - value: tanya
        - value: tibur
        - value: tj
        - value: tyler
        - value: viv
        - value: yadira
        - value: marsh
        - value: bayou
        - value: creek
        - value: brook
        - value: flower
        - value: spore
        - value: glacier
        - value: gulch
        - value: alpine
        - value: cove
        - value: lagoon
        - value: tundra
        - value: steppe
        - value: mesa
        - value: grove
        - value: rainforest
        - value: moraine
        - value: wildflower
        - value: peak
        - value: boulder
        - value: gypsum
        - value: zest
        - value: luna
        - value: celeste
        - value: orion
        - value: ursa
        - value: astra
        - value: esther
        - value: estelle
        - value: andromeda
    FallbackRimeAIVoiceId:
      oneOf:
        - $ref: '#/components/schemas/FallbackRimeAiVoiceId0'
        - type: string
    FallbackRimeAiVoiceModel:
      type: string
      enum:
        - value: arcana
        - value: mistv2
        - value: mist
    FallbackRimeAIVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackRimeAiVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackRimeAIVoiceId'
          description: This is the provider-specific ID that will be used.
        model:
          $ref: '#/components/schemas/FallbackRimeAiVoiceModel'
          description: >-
            This is the model that will be used. Defaults to 'arcana' when not
            specified.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        pauseBetweenBrackets:
          type: boolean
          description: >-
            This is a flag that controls whether to add slight pauses using
            angle brackets. Example: "Hi. <200> I'd love to have a conversation
            with you." adds a 200ms pause between the first and second
            sentences.
        phonemizeBetweenBrackets:
          type: boolean
          description: >-
            This is a flag that controls whether text inside brackets should be
            phonemized (converted to phonetic pronunciation) - Example:
            "{h'El.o} World" will pronounce "Hello" as expected.
        reduceLatency:
          type: boolean
          description: >-
            This is a flag that controls whether to optimize for reduced latency
            in streaming.
            https://docs.rime.ai/api-reference/endpoint/websockets#param-reduce-latency
        inlineSpeedAlpha:
          type: string
          description: >-
            This is a string that allows inline speed control using alpha
            notation.
            https://docs.rime.ai/api-reference/endpoint/websockets#param-inline-speed-alpha
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackSmallestAiVoiceProvider:
      type: string
      enum:
        - value: smallest-ai
    FallbackSmallestAiVoiceId0:
      type: string
      enum:
        - value: emily
        - value: jasmine
        - value: arman
        - value: james
        - value: mithali
        - value: aravind
        - value: raj
        - value: diya
        - value: raman
        - value: ananya
        - value: isha
        - value: william
        - value: aarav
        - value: monika
        - value: niharika
        - value: deepika
        - value: raghav
        - value: kajal
        - value: radhika
        - value: mansi
        - value: nisha
        - value: saurabh
        - value: pooja
        - value: saina
        - value: sanya
    FallbackSmallestAIVoiceId:
      oneOf:
        - $ref: '#/components/schemas/FallbackSmallestAiVoiceId0'
        - type: string
    FallbackSmallestAiVoiceModel:
      type: string
      enum:
        - value: lightning
    FallbackSmallestAIVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackSmallestAiVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackSmallestAIVoiceId'
          description: This is the provider-specific ID that will be used.
        model:
          $ref: '#/components/schemas/FallbackSmallestAiVoiceModel'
          description: >-
            Smallest AI voice model to use. Defaults to 'lightning' when not
            specified.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackTavusVoiceProvider:
      type: string
      enum:
        - value: tavus
    FallbackTavusVoiceVoiceId0:
      type: string
      enum:
        - value: r52da2535a
    FallbackTavusVoiceVoiceId:
      oneOf:
        - $ref: '#/components/schemas/FallbackTavusVoiceVoiceId0'
        - type: string
    TavusConversationProperties:
      type: object
      properties:
        maxCallDuration:
          type: number
          format: double
          description: >-
            The maximum duration of the call in seconds. The default
            `maxCallDuration` is 3600 seconds (1 hour).

            Once the time limit specified by this parameter has been reached,
            the conversation will automatically shut down.
        participantLeftTimeout:
          type: number
          format: double
          description: >-
            The duration in seconds after which the call will be automatically
            shut down once the last participant leaves.
        participantAbsentTimeout:
          type: number
          format: double
          description: >-
            Starting from conversation creation, the duration in seconds after
            which the call will be automatically shut down if no participant
            joins the call.

            Default is 300 seconds (5 minutes).
        enableRecording:
          type: boolean
          description: If true, the user will be able to record the conversation.
        enableTranscription:
          type: boolean
          description: >-
            If true, the user will be able to transcribe the conversation.

            You can find more instructions on displaying transcriptions if you
            are using your custom DailyJS components here.

            You need to have an event listener on Daily that listens for
            `app-messages`.
        applyGreenscreen:
          type: boolean
          description: >-
            If true, the background will be replaced with a greenscreen (RGB
            values: `[0, 255, 155]`).

            You can use WebGL on the frontend to make the greenscreen
            transparent or change its color.
        language:
          type: string
          description: >-
            The language of the conversation. Please provide the **full language
            name**, not the two-letter code.

            If you are using your own TTS voice, please ensure it supports the
            language you provide.

            If you are using a stock replica or default persona, please note
            that only ElevenLabs and Cartesia supported languages are available.

            You can find a full list of supported languages for Cartesia here,
            for ElevenLabs here, and for PlayHT here.
        recordingS3BucketName:
          type: string
          description: The name of the S3 bucket where the recording will be stored.
        recordingS3BucketRegion:
          type: string
          description: The region of the S3 bucket where the recording will be stored.
        awsAssumeRoleArn:
          type: string
          description: The ARN of the role that will be assumed to access the S3 bucket.
    FallbackTavusVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackTavusVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackTavusVoiceVoiceId'
          description: This is the provider-specific ID that will be used.
        personaId:
          type: string
          description: >-
            This is the unique identifier for the persona that the replica will
            use in the conversation.
        callbackUrl:
          type: string
          description: >-
            This is the url that will receive webhooks with updates regarding
            the conversation state.
        conversationName:
          type: string
          description: This is the name for the conversation.
        conversationalContext:
          type: string
          description: >-
            This is the context that will be appended to any context provided in
            the persona, if one is provided.
        customGreeting:
          type: string
          description: >-
            This is the custom greeting that the replica will give once a
            participant joines the conversation.
        properties:
          $ref: '#/components/schemas/TavusConversationProperties'
          description: These are optional properties used to customize the conversation.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackNeuphonicVoiceProvider:
      type: string
      enum:
        - value: neuphonic
    FallbackNeuphonicVoiceModel:
      type: string
      enum:
        - value: neu_hq
        - value: neu_fast
    FallbackNeuphonicVoiceLanguage:
      type: object
      properties: {}
    FallbackNeuphonicVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackNeuphonicVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          type: string
          description: This is the provider-specific ID that will be used.
        model:
          $ref: '#/components/schemas/FallbackNeuphonicVoiceModel'
          description: >-
            This is the model that will be used. Defaults to 'neu_fast' if not
            specified.
        language:
          $ref: '#/components/schemas/FallbackNeuphonicVoiceLanguage'
          description: This is the language (ISO 639-1) that is enforced for the model.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
        - language
    FallbackSesameVoiceProvider:
      type: string
      enum:
        - value: sesame
    FallbackSesameVoiceModel:
      type: string
      enum:
        - value: csm-1b
    FallbackSesameVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackSesameVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          type: string
          description: This is the provider-specific ID that will be used.
        model:
          $ref: '#/components/schemas/FallbackSesameVoiceModel'
          description: This is the model that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
        - model
    FallbackInworldVoiceProvider:
      type: string
      enum:
        - value: inworld
    FallbackInworldVoiceVoiceId:
      type: string
      enum:
        - value: Alex
        - value: Ashley
        - value: Craig
        - value: Deborah
        - value: Dennis
        - value: Edward
        - value: Elizabeth
        - value: Hades
        - value: Julia
        - value: Pixie
        - value: Mark
        - value: Olivia
        - value: Priya
        - value: Ronald
        - value: Sarah
        - value: Shaun
        - value: Theodore
        - value: Timothy
        - value: Wendy
        - value: Dominus
        - value: Hana
        - value: Clive
        - value: Carter
        - value: Blake
        - value: Luna
        - value: Yichen
        - value: Xiaoyin
        - value: Xinyi
        - value: Jing
        - value: Erik
        - value: Katrien
        - value: Lennart
        - value: Lore
        - value: Alain
        - value: Hélène
        - value: Mathieu
        - value: Étienne
        - value: Johanna
        - value: Josef
        - value: Gianni
        - value: Orietta
        - value: Asuka
        - value: Satoshi
        - value: Hyunwoo
        - value: Minji
        - value: Seojun
        - value: Yoona
        - value: Szymon
        - value: Wojciech
        - value: Heitor
        - value: Maitê
        - value: Diego
        - value: Lupita
        - value: Miguel
        - value: Rafael
        - value: Svetlana
        - value: Elena
        - value: Dmitry
        - value: Nikolai
        - value: Riya
        - value: Manoj
        - value: Yael
        - value: Oren
        - value: Nour
        - value: Omar
    FallbackInworldVoiceModel:
      type: string
      enum:
        - value: inworld-tts-1
      default: inworld-tts-1
    FallbackInworldVoiceLanguageCode:
      type: string
      enum:
        - value: en
        - value: zh
        - value: ko
        - value: nl
        - value: fr
        - value: es
        - value: ja
        - value: de
        - value: it
        - value: pl
        - value: pt
        - value: ru
        - value: hi
        - value: he
        - value: ar
      default: en
    FallbackInworldVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/FallbackInworldVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/FallbackInworldVoiceVoiceId'
          description: >-
            Available voices by language:

            • en: Alex, Ashley, Craig, Deborah, Dennis, Edward, Elizabeth,
            Hades, Julia, Pixie, Mark, Olivia, Priya, Ronald, Sarah, Shaun,
            Theodore, Timothy, Wendy, Dominus, Hana, Clive, Carter, Blake, Luna

            • zh: Yichen, Xiaoyin, Xinyi, Jing

            • nl: Erik, Katrien, Lennart, Lore

            • fr: Alain, Hélène, Mathieu, Étienne

            • de: Johanna, Josef

            • it: Gianni, Orietta

            • ja: Asuka, Satoshi

            • ko: Hyunwoo, Minji, Seojun, Yoona

            • pl: Szymon, Wojciech

            • pt: Heitor, Maitê

            • es: Diego, Lupita, Miguel, Rafael

            • ru: Svetlana, Elena, Dmitry, Nikolai

            • hi: Riya, Manoj

            • he: Yael, Oren

            • ar: Nour, Omar
        model:
          $ref: '#/components/schemas/FallbackInworldVoiceModel'
          description: This is the model that will be used.
        languageCode:
          $ref: '#/components/schemas/FallbackInworldVoiceLanguageCode'
          description: Language code for Inworld TTS synthesis
        temperature:
          type: number
          format: double
          default: 1.1
          description: >-
            A floating point number between 0, exclusive, and 2, inclusive. If
            equal to null or not provided, the model's default temperature of
            1.1 will be used. The temperature parameter controls variance.

            Higher values will make the output more random and can lead to more
            expressive results. Lower values will make it more deterministic.

            See
            https://docs.inworld.ai/docs/tts/capabilities/generating-audio#additional-configurations
            for more details.
        speakingRate:
          type: number
          format: double
          default: 1
          description: >-
            A floating point number between 0.5, inclusive, and 1.5, inclusive.
            If equal to null or not provided, the model's default speaking speed
            of 1.0 will be used.

            Values above 0.8 are recommended for higher quality.

            See
            https://docs.inworld.ai/docs/tts/capabilities/generating-audio#additional-configurations
            for more details.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
      required:
        - provider
        - voiceId
    FallbackPlanVoicesItems:
      oneOf:
        - $ref: '#/components/schemas/FallbackAzureVoice'
        - $ref: '#/components/schemas/FallbackCartesiaVoice'
        - $ref: '#/components/schemas/FallbackHumeVoice'
        - $ref: '#/components/schemas/FallbackCustomVoice'
        - $ref: '#/components/schemas/FallbackDeepgramVoice'
        - $ref: '#/components/schemas/FallbackElevenLabsVoice'
        - $ref: '#/components/schemas/FallbackVapiVoice'
        - $ref: '#/components/schemas/FallbackLMNTVoice'
        - $ref: '#/components/schemas/FallbackOpenAIVoice'
        - $ref: '#/components/schemas/FallbackPlayHTVoice'
        - $ref: '#/components/schemas/FallbackWellSaidVoice'
        - $ref: '#/components/schemas/FallbackRimeAIVoice'
        - $ref: '#/components/schemas/FallbackSmallestAIVoice'
        - $ref: '#/components/schemas/FallbackTavusVoice'
        - $ref: '#/components/schemas/FallbackNeuphonicVoice'
        - $ref: '#/components/schemas/FallbackSesameVoice'
        - $ref: '#/components/schemas/FallbackInworldVoice'
    FallbackPlan:
      type: object
      properties:
        voices:
          type: array
          items:
            $ref: '#/components/schemas/FallbackPlanVoicesItems'
          description: >-
            This is the list of voices to fallback to in the event that the
            primary voice provider fails.
      required:
        - voices
    AzureVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/AzureVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/AzureVoiceId'
          description: This is the provider-specific ID that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    CartesiaVoiceProvider:
      type: string
      enum:
        - value: cartesia
    CartesiaVoiceModel:
      type: string
      enum:
        - value: sonic-3
        - value: sonic-2
        - value: sonic-english
        - value: sonic-multilingual
        - value: sonic-preview
        - value: sonic
    CartesiaVoiceLanguage:
      type: string
      enum:
        - value: ar
        - value: bg
        - value: bn
        - value: cs
        - value: da
        - value: de
        - value: el
        - value: en
        - value: es
        - value: fi
        - value: fr
        - value: gu
        - value: he
        - value: hi
        - value: hr
        - value: hu
        - value: id
        - value: it
        - value: ja
        - value: ka
        - value: kn
        - value: ko
        - value: ml
        - value: mr
        - value: ms
        - value: nl
        - value: 'no'
        - value: pa
        - value: pl
        - value: pt
        - value: ro
        - value: ru
        - value: sk
        - value: sv
        - value: ta
        - value: te
        - value: th
        - value: tl
        - value: tr
        - value: uk
        - value: vi
        - value: zh
    CartesiaVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/CartesiaVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          type: string
          description: The ID of the particular voice you want to use.
        model:
          $ref: '#/components/schemas/CartesiaVoiceModel'
          description: >-
            This is the model that will be used. This is optional and will
            default to the correct model for the voiceId.
        language:
          $ref: '#/components/schemas/CartesiaVoiceLanguage'
          description: >-
            This is the language that will be used. This is optional and will
            default to the correct language for the voiceId.
        experimentalControls:
          $ref: '#/components/schemas/CartesiaExperimentalControls'
          description: Experimental controls for Cartesia voice generation
        generationConfig:
          $ref: '#/components/schemas/CartesiaGenerationConfig'
          description: >-
            Generation config for fine-grained control of sonic-3 voice output
            (speed, volume, and experimental controls). Only available for
            sonic-3 model.
        pronunciationDictId:
          type: string
          description: >-
            Pronunciation dictionary ID for sonic-3. Allows custom
            pronunciations for specific words. Only available for sonic-3 model.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    CustomVoiceProvider:
      type: string
      enum:
        - value: custom-voice
    CustomVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/CustomVoiceProvider'
          description: >-
            This is the voice provider that will be used. Use `custom-voice` for
            providers that are not natively supported.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        server:
          $ref: '#/components/schemas/Server'
          description: >-
            This is where the voice request will be sent.


            Request Example:


            POST https://{server.url}

            Content-Type: application/json


            {
              "message": {
                "type": "voice-request",
                "text": "Hello, world!",
                "sampleRate": 24000,
                ...other metadata about the call...
              }
            }


            Response Expected: 1-channel 16-bit raw PCM audio at the sample rate
            specified in the request. Here is how the response will be piped to
            the transport:

            ```

            response.on('data', (chunk: Buffer) => {
              outputStream.write(chunk);
            });

            ```
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - server
    DeepgramVoiceProvider:
      type: string
      enum:
        - value: deepgram
    DeepgramVoiceId:
      type: string
      enum:
        - value: asteria
        - value: luna
        - value: stella
        - value: athena
        - value: hera
        - value: orion
        - value: arcas
        - value: perseus
        - value: angus
        - value: orpheus
        - value: helios
        - value: zeus
        - value: thalia
        - value: andromeda
        - value: helena
        - value: apollo
        - value: arcas
        - value: aries
        - value: amalthea
        - value: asteria
        - value: athena
        - value: atlas
        - value: aurora
        - value: callista
        - value: cora
        - value: cordelia
        - value: delia
        - value: draco
        - value: electra
        - value: harmonia
        - value: hera
        - value: hermes
        - value: hyperion
        - value: iris
        - value: janus
        - value: juno
        - value: jupiter
        - value: luna
        - value: mars
        - value: minerva
        - value: neptune
        - value: odysseus
        - value: ophelia
        - value: orion
        - value: orpheus
        - value: pandora
        - value: phoebe
        - value: pluto
        - value: saturn
        - value: selene
        - value: theia
        - value: vesta
        - value: zeus
        - value: celeste
        - value: estrella
        - value: nestor
        - value: sirio
        - value: carina
        - value: alvaro
        - value: diana
        - value: aquila
        - value: selena
        - value: javier
    DeepgramVoiceModel:
      type: string
      enum:
        - value: aura
        - value: aura-2
    DeepgramVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/DeepgramVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/DeepgramVoiceId'
          description: This is the provider-specific ID that will be used.
        model:
          $ref: '#/components/schemas/DeepgramVoiceModel'
          description: >-
            This is the model that will be used. Defaults to 'aura-2' when not
            specified.
        mipOptOut:
          type: boolean
          default: false
          description: >-
            If set to true, this will add mip_opt_out=true as a query parameter
            of all API requests. See
            https://developers.deepgram.com/docs/the-deepgram-model-improvement-partnership-program#want-to-opt-out


            This will only be used if you are using your own Deepgram API key.


            @default false
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    ElevenLabsVoiceId0:
      type: string
      enum:
        - value: burt
        - value: marissa
        - value: andrea
        - value: sarah
        - value: phillip
        - value: steve
        - value: joseph
        - value: myra
        - value: paula
        - value: ryan
        - value: drew
        - value: paul
        - value: mrb
        - value: matilda
        - value: mark
    ElevenLabsVoiceId:
      oneOf:
        - $ref: '#/components/schemas/ElevenLabsVoiceId0'
        - type: string
    ElevenLabsVoiceModel:
      type: string
      enum:
        - value: eleven_multilingual_v2
        - value: eleven_turbo_v2
        - value: eleven_turbo_v2_5
        - value: eleven_flash_v2
        - value: eleven_flash_v2_5
        - value: eleven_monolingual_v1
        - value: eleven_v3
    ElevenLabsVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          type: string
          enum:
            - type: stringLiteral
              value: 11labs
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/ElevenLabsVoiceId'
          description: >-
            This is the provider-specific ID that will be used. Ensure the Voice
            is present in your 11Labs Voice Library.
        stability:
          type: number
          format: double
          description: Defines the stability for voice settings.
        similarityBoost:
          type: number
          format: double
          description: Defines the similarity boost for voice settings.
        style:
          type: number
          format: double
          description: Defines the style for voice settings.
        useSpeakerBoost:
          type: boolean
          description: Defines the use speaker boost for voice settings.
        speed:
          type: number
          format: double
          description: Defines the speed for voice settings.
        optimizeStreamingLatency:
          type: number
          format: double
          description: >-
            Defines the optimize streaming latency for voice settings. Defaults
            to 3.
        enableSsmlParsing:
          type: boolean
          description: >-
            This enables the use of
            https://elevenlabs.io/docs/speech-synthesis/prompting#pronunciation.
            Defaults to false to save latency.


            @default false
        autoMode:
          type: boolean
          description: Defines the auto mode for voice settings. Defaults to false.
        model:
          $ref: '#/components/schemas/ElevenLabsVoiceModel'
          description: >-
            This is the model that will be used. Defaults to 'eleven_turbo_v2'
            if not specified.
        language:
          type: string
          description: >-
            This is the language (ISO 639-1) that is enforced for the model.
            Currently only Turbo v2.5 supports language enforcement. For other
            models, an error will be returned if language code is provided.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        pronunciationDictionaryLocators:
          type: array
          items:
            $ref: '#/components/schemas/ElevenLabsPronunciationDictionaryLocator'
          description: This is the pronunciation dictionary locators to use.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    HumeVoiceProvider:
      type: string
      enum:
        - value: hume
    HumeVoiceModel:
      type: string
      enum:
        - value: octave
        - value: octave2
    HumeVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/HumeVoiceProvider'
          description: This is the voice provider that will be used.
        model:
          $ref: '#/components/schemas/HumeVoiceModel'
          description: This is the model that will be used.
        voiceId:
          type: string
          description: The ID of the particular voice you want to use.
        isCustomHumeVoice:
          type: boolean
          description: >-
            Indicates whether the chosen voice is a preset Hume AI voice or a
            custom voice.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        description:
          type: string
          description: >-
            Natural language instructions describing how the synthesized speech
            should sound, including but not limited to tone, intonation, pacing,
            and accent (e.g., 'a soft, gentle voice with a strong British
            accent').


            If a Voice is specified in the request, this description serves as
            acting instructions.

            If no Voice is specified, a new voice is generated based on this
            description.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    LmntVoiceProvider:
      type: string
      enum:
        - value: lmnt
    LmntVoiceId0:
      type: string
      enum:
        - value: amy
        - value: ansel
        - value: autumn
        - value: ava
        - value: brandon
        - value: caleb
        - value: cassian
        - value: chloe
        - value: dalton
        - value: daniel
        - value: dustin
        - value: elowen
        - value: evander
        - value: huxley
        - value: james
        - value: juniper
        - value: kennedy
        - value: lauren
        - value: leah
        - value: lily
        - value: lucas
        - value: magnus
        - value: miles
        - value: morgan
        - value: natalie
        - value: nathan
        - value: noah
        - value: nyssa
        - value: oliver
        - value: paige
        - value: ryan
        - value: sadie
        - value: sophie
        - value: stella
        - value: terrence
        - value: tyler
        - value: vesper
        - value: violet
        - value: warrick
        - value: zain
        - value: zeke
        - value: zoe
    LMNTVoiceId:
      oneOf:
        - $ref: '#/components/schemas/LmntVoiceId0'
        - type: string
    LmntVoiceLanguage:
      type: string
      enum:
        - value: aa
        - value: ab
        - value: ae
        - value: af
        - value: ak
        - value: am
        - value: an
        - value: ar
        - value: as
        - value: av
        - value: ay
        - value: az
        - value: ba
        - value: be
        - value: bg
        - value: bh
        - value: bi
        - value: bm
        - value: bn
        - value: bo
        - value: br
        - value: bs
        - value: ca
        - value: ce
        - value: ch
        - value: co
        - value: cr
        - value: cs
        - value: cu
        - value: cv
        - value: cy
        - value: da
        - value: de
        - value: dv
        - value: dz
        - value: ee
        - value: el
        - value: en
        - value: eo
        - value: es
        - value: et
        - value: eu
        - value: fa
        - value: ff
        - value: fi
        - value: fj
        - value: fo
        - value: fr
        - value: fy
        - value: ga
        - value: gd
        - value: gl
        - value: gn
        - value: gu
        - value: gv
        - value: ha
        - value: he
        - value: hi
        - value: ho
        - value: hr
        - value: ht
        - value: hu
        - value: hy
        - value: hz
        - value: ia
        - value: id
        - value: ie
        - value: ig
        - value: ii
        - value: ik
        - value: io
        - value: is
        - value: it
        - value: iu
        - value: ja
        - value: jv
        - value: ka
        - value: kg
        - value: ki
        - value: kj
        - value: kk
        - value: kl
        - value: km
        - value: kn
        - value: ko
        - value: kr
        - value: ks
        - value: ku
        - value: kv
        - value: kw
        - value: ky
        - value: la
        - value: lb
        - value: lg
        - value: li
        - value: ln
        - value: lo
        - value: lt
        - value: lu
        - value: lv
        - value: mg
        - value: mh
        - value: mi
        - value: mk
        - value: ml
        - value: mn
        - value: mr
        - value: ms
        - value: mt
        - value: my
        - value: na
        - value: nb
        - value: nd
        - value: ne
        - value: ng
        - value: nl
        - value: nn
        - value: 'no'
        - value: nr
        - value: nv
        - value: ny
        - value: oc
        - value: oj
        - value: om
        - value: or
        - value: os
        - value: pa
        - value: pi
        - value: pl
        - value: ps
        - value: pt
        - value: qu
        - value: rm
        - value: rn
        - value: ro
        - value: ru
        - value: rw
        - value: sa
        - value: sc
        - value: sd
        - value: se
        - value: sg
        - value: si
        - value: sk
        - value: sl
        - value: sm
        - value: sn
        - value: so
        - value: sq
        - value: sr
        - value: ss
        - value: st
        - value: su
        - value: sv
        - value: sw
        - value: ta
        - value: te
        - value: tg
        - value: th
        - value: ti
        - value: tk
        - value: tl
        - value: tn
        - value: to
        - value: tr
        - value: ts
        - value: tt
        - value: tw
        - value: ty
        - value: ug
        - value: uk
        - value: ur
        - value: uz
        - value: ve
        - value: vi
        - value: vo
        - value: wa
        - value: wo
        - value: xh
        - value: yi
        - value: yue
        - value: yo
        - value: za
        - value: zh
        - value: zu
        - value: auto
    LMNTVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/LmntVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/LMNTVoiceId'
          description: This is the provider-specific ID that will be used.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        language:
          $ref: '#/components/schemas/LmntVoiceLanguage'
          description: Two letter ISO 639-1 language code. Use "auto" for auto-detection.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    NeuphonicVoiceProvider:
      type: string
      enum:
        - value: neuphonic
    NeuphonicVoiceModel:
      type: string
      enum:
        - value: neu_hq
        - value: neu_fast
    NeuphonicVoiceLanguage:
      type: object
      properties: {}
    NeuphonicVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/NeuphonicVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          type: string
          description: This is the provider-specific ID that will be used.
        model:
          $ref: '#/components/schemas/NeuphonicVoiceModel'
          description: >-
            This is the model that will be used. Defaults to 'neu_fast' if not
            specified.
        language:
          $ref: '#/components/schemas/NeuphonicVoiceLanguage'
          description: This is the language (ISO 639-1) that is enforced for the model.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
        - language
    OpenAiVoiceProvider:
      type: string
      enum:
        - value: openai
    OpenAiVoiceId0:
      type: string
      enum:
        - value: alloy
        - value: echo
        - value: fable
        - value: onyx
        - value: nova
        - value: shimmer
        - value: marin
        - value: cedar
    OpenAIVoiceId:
      oneOf:
        - $ref: '#/components/schemas/OpenAiVoiceId0'
        - type: string
    OpenAiVoiceModel:
      type: string
      enum:
        - value: tts-1
        - value: tts-1-hd
        - value: gpt-4o-mini-tts
    OpenAIVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/OpenAiVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/OpenAIVoiceId'
          description: >-
            This is the provider-specific ID that will be used.

            Please note that ash, ballad, coral, sage, and verse may only be
            used with realtime models.
        model:
          $ref: '#/components/schemas/OpenAiVoiceModel'
          description: This is the model that will be used for text-to-speech.
        instructions:
          type: string
          description: >-
            This is a prompt that allows you to control the voice of your
            generated audio.

            Does not work with 'tts-1' or 'tts-1-hd' models.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    PlayHtVoiceProvider:
      type: string
      enum:
        - value: playht
    PlayHtVoiceId0:
      type: string
      enum:
        - value: jennifer
        - value: melissa
        - value: will
        - value: chris
        - value: matt
        - value: jack
        - value: ruby
        - value: davis
        - value: donna
        - value: michael
    PlayHTVoiceId:
      oneOf:
        - $ref: '#/components/schemas/PlayHtVoiceId0'
        - type: string
    PlayHtVoiceEmotion:
      type: string
      enum:
        - value: female_happy
        - value: female_sad
        - value: female_angry
        - value: female_fearful
        - value: female_disgust
        - value: female_surprised
        - value: male_happy
        - value: male_sad
        - value: male_angry
        - value: male_fearful
        - value: male_disgust
        - value: male_surprised
    PlayHtVoiceModel:
      type: string
      enum:
        - value: PlayHT2.0
        - value: PlayHT2.0-turbo
        - value: Play3.0-mini
        - value: PlayDialog
    PlayHtVoiceLanguage:
      type: string
      enum:
        - value: afrikaans
        - value: albanian
        - value: amharic
        - value: arabic
        - value: bengali
        - value: bulgarian
        - value: catalan
        - value: croatian
        - value: czech
        - value: danish
        - value: dutch
        - value: english
        - value: french
        - value: galician
        - value: german
        - value: greek
        - value: hebrew
        - value: hindi
        - value: hungarian
        - value: indonesian
        - value: italian
        - value: japanese
        - value: korean
        - value: malay
        - value: mandarin
        - value: polish
        - value: portuguese
        - value: russian
        - value: serbian
        - value: spanish
        - value: swedish
        - value: tagalog
        - value: thai
        - value: turkish
        - value: ukrainian
        - value: urdu
        - value: xhosa
    PlayHTVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/PlayHtVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/PlayHTVoiceId'
          description: This is the provider-specific ID that will be used.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        temperature:
          type: number
          format: double
          description: >-
            A floating point number between 0, exclusive, and 2, inclusive. If
            equal to null or not provided, the model's default temperature will
            be used. The temperature parameter controls variance. Lower
            temperatures result in more predictable results, higher temperatures
            allow each run to vary more, so the voice may sound less like the
            baseline voice.
        emotion:
          $ref: '#/components/schemas/PlayHtVoiceEmotion'
          description: An emotion to be applied to the speech.
        voiceGuidance:
          type: number
          format: double
          description: >-
            A number between 1 and 6. Use lower numbers to reduce how unique
            your chosen voice will be compared to other voices.
        styleGuidance:
          type: number
          format: double
          description: >-
            A number between 1 and 30. Use lower numbers to to reduce how strong
            your chosen emotion will be. Higher numbers will create a very
            emotional performance.
        textGuidance:
          type: number
          format: double
          description: >-
            A number between 1 and 2. This number influences how closely the
            generated speech adheres to the input text. Use lower values to
            create more fluid speech, but with a higher chance of deviating from
            the input text. Higher numbers will make the generated speech more
            accurate to the input text, ensuring that the words spoken align
            closely with the provided text.
        model:
          $ref: '#/components/schemas/PlayHtVoiceModel'
          description: Playht voice model/engine to use.
        language:
          $ref: '#/components/schemas/PlayHtVoiceLanguage'
          description: The language to use for the speech.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    WellSaidVoiceProvider:
      type: string
      enum:
        - value: wellsaid
    WellSaidVoiceModel:
      type: string
      enum:
        - value: caruso
        - value: legacy
    WellSaidVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/WellSaidVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          type: string
          description: The WellSaid speaker ID to synthesize.
        model:
          $ref: '#/components/schemas/WellSaidVoiceModel'
          description: This is the model that will be used.
        enableSsml:
          type: boolean
          description: Enables limited SSML translation for input text.
        libraryIds:
          type: array
          items:
            type: string
          description: Array of library IDs to use for voice synthesis.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    RimeAiVoiceProvider:
      type: string
      enum:
        - value: rime-ai
    RimeAiVoiceId0:
      type: string
      enum:
        - value: abbie
        - value: allison
        - value: ally
        - value: alona
        - value: amber
        - value: ana
        - value: antoine
        - value: armon
        - value: brenda
        - value: brittany
        - value: carol
        - value: colin
        - value: courtney
        - value: elena
        - value: elliot
        - value: eva
        - value: geoff
        - value: gerald
        - value: hank
        - value: helen
        - value: hera
        - value: jen
        - value: joe
        - value: joy
        - value: juan
        - value: kendra
        - value: kendrick
        - value: kenneth
        - value: kevin
        - value: kris
        - value: linda
        - value: madison
        - value: marge
        - value: marina
        - value: marissa
        - value: marta
        - value: maya
        - value: nicholas
        - value: nyles
        - value: phil
        - value: reba
        - value: rex
        - value: rick
        - value: ritu
        - value: rob
        - value: rodney
        - value: rohan
        - value: rosco
        - value: samantha
        - value: sandy
        - value: selena
        - value: seth
        - value: sharon
        - value: stan
        - value: tamra
        - value: tanya
        - value: tibur
        - value: tj
        - value: tyler
        - value: viv
        - value: yadira
        - value: marsh
        - value: bayou
        - value: creek
        - value: brook
        - value: flower
        - value: spore
        - value: glacier
        - value: gulch
        - value: alpine
        - value: cove
        - value: lagoon
        - value: tundra
        - value: steppe
        - value: mesa
        - value: grove
        - value: rainforest
        - value: moraine
        - value: wildflower
        - value: peak
        - value: boulder
        - value: gypsum
        - value: zest
        - value: luna
        - value: celeste
        - value: orion
        - value: ursa
        - value: astra
        - value: esther
        - value: estelle
        - value: andromeda
    RimeAIVoiceId:
      oneOf:
        - $ref: '#/components/schemas/RimeAiVoiceId0'
        - type: string
    RimeAiVoiceModel:
      type: string
      enum:
        - value: arcana
        - value: mistv2
        - value: mist
    RimeAIVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/RimeAiVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/RimeAIVoiceId'
          description: This is the provider-specific ID that will be used.
        model:
          $ref: '#/components/schemas/RimeAiVoiceModel'
          description: >-
            This is the model that will be used. Defaults to 'arcana' when not
            specified.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        pauseBetweenBrackets:
          type: boolean
          description: >-
            This is a flag that controls whether to add slight pauses using
            angle brackets. Example: "Hi. <200> I'd love to have a conversation
            with you." adds a 200ms pause between the first and second
            sentences.
        phonemizeBetweenBrackets:
          type: boolean
          description: >-
            This is a flag that controls whether text inside brackets should be
            phonemized (converted to phonetic pronunciation) - Example:
            "{h'El.o} World" will pronounce "Hello" as expected.
        reduceLatency:
          type: boolean
          description: >-
            This is a flag that controls whether to optimize for reduced latency
            in streaming.
            https://docs.rime.ai/api-reference/endpoint/websockets#param-reduce-latency
        inlineSpeedAlpha:
          type: string
          description: >-
            This is a string that allows inline speed control using alpha
            notation.
            https://docs.rime.ai/api-reference/endpoint/websockets#param-inline-speed-alpha
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    SmallestAiVoiceProvider:
      type: string
      enum:
        - value: smallest-ai
    SmallestAiVoiceId0:
      type: string
      enum:
        - value: emily
        - value: jasmine
        - value: arman
        - value: james
        - value: mithali
        - value: aravind
        - value: raj
        - value: diya
        - value: raman
        - value: ananya
        - value: isha
        - value: william
        - value: aarav
        - value: monika
        - value: niharika
        - value: deepika
        - value: raghav
        - value: kajal
        - value: radhika
        - value: mansi
        - value: nisha
        - value: saurabh
        - value: pooja
        - value: saina
        - value: sanya
    SmallestAIVoiceId:
      oneOf:
        - $ref: '#/components/schemas/SmallestAiVoiceId0'
        - type: string
    SmallestAiVoiceModel:
      type: string
      enum:
        - value: lightning
    SmallestAIVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/SmallestAiVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/SmallestAIVoiceId'
          description: This is the provider-specific ID that will be used.
        model:
          $ref: '#/components/schemas/SmallestAiVoiceModel'
          description: >-
            Smallest AI voice model to use. Defaults to 'lightning' when not
            specified.
        speed:
          type: number
          format: double
          description: This is the speed multiplier that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    TavusVoiceProvider:
      type: string
      enum:
        - value: tavus
    TavusVoiceVoiceId0:
      type: string
      enum:
        - value: r52da2535a
    TavusVoiceVoiceId:
      oneOf:
        - $ref: '#/components/schemas/TavusVoiceVoiceId0'
        - type: string
    TavusVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/TavusVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/TavusVoiceVoiceId'
          description: This is the provider-specific ID that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        personaId:
          type: string
          description: >-
            This is the unique identifier for the persona that the replica will
            use in the conversation.
        callbackUrl:
          type: string
          description: >-
            This is the url that will receive webhooks with updates regarding
            the conversation state.
        conversationName:
          type: string
          description: This is the name for the conversation.
        conversationalContext:
          type: string
          description: >-
            This is the context that will be appended to any context provided in
            the persona, if one is provided.
        customGreeting:
          type: string
          description: >-
            This is the custom greeting that the replica will give once a
            participant joines the conversation.
        properties:
          $ref: '#/components/schemas/TavusConversationProperties'
          description: These are optional properties used to customize the conversation.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    VapiVoiceProvider:
      type: string
      enum:
        - value: vapi
    VapiVoiceVoiceId:
      type: string
      enum:
        - value: Elliot
        - value: Kylie
        - value: Rohan
        - value: Lily
        - value: Savannah
        - value: Hana
        - value: Neha
        - value: Cole
        - value: Harry
        - value: Paige
        - value: Spencer
        - value: Leah
        - value: Tara
        - value: Jess
        - value: Leo
        - value: Dan
        - value: Mia
        - value: Zac
        - value: Zoe
    VapiVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/VapiVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/VapiVoiceVoiceId'
          description: The voices provided by Vapi
        speed:
          type: number
          format: double
          default: 1
          description: |-
            This is the speed multiplier that will be used.

            @default 1
        pronunciationDictionary:
          type: array
          items:
            $ref: '#/components/schemas/VapiPronunciationDictionaryLocator'
          description: >-
            List of pronunciation dictionary locators for custom word
            pronunciations.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    SesameVoiceProvider:
      type: string
      enum:
        - value: sesame
    SesameVoiceModel:
      type: string
      enum:
        - value: csm-1b
    SesameVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/SesameVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          type: string
          description: This is the provider-specific ID that will be used.
        model:
          $ref: '#/components/schemas/SesameVoiceModel'
          description: This is the model that will be used.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
        - model
    InworldVoiceProvider:
      type: string
      enum:
        - value: inworld
    InworldVoiceVoiceId:
      type: string
      enum:
        - value: Alex
        - value: Ashley
        - value: Craig
        - value: Deborah
        - value: Dennis
        - value: Edward
        - value: Elizabeth
        - value: Hades
        - value: Julia
        - value: Pixie
        - value: Mark
        - value: Olivia
        - value: Priya
        - value: Ronald
        - value: Sarah
        - value: Shaun
        - value: Theodore
        - value: Timothy
        - value: Wendy
        - value: Dominus
        - value: Hana
        - value: Clive
        - value: Carter
        - value: Blake
        - value: Luna
        - value: Yichen
        - value: Xiaoyin
        - value: Xinyi
        - value: Jing
        - value: Erik
        - value: Katrien
        - value: Lennart
        - value: Lore
        - value: Alain
        - value: Hélène
        - value: Mathieu
        - value: Étienne
        - value: Johanna
        - value: Josef
        - value: Gianni
        - value: Orietta
        - value: Asuka
        - value: Satoshi
        - value: Hyunwoo
        - value: Minji
        - value: Seojun
        - value: Yoona
        - value: Szymon
        - value: Wojciech
        - value: Heitor
        - value: Maitê
        - value: Diego
        - value: Lupita
        - value: Miguel
        - value: Rafael
        - value: Svetlana
        - value: Elena
        - value: Dmitry
        - value: Nikolai
        - value: Riya
        - value: Manoj
        - value: Yael
        - value: Oren
        - value: Nour
        - value: Omar
    InworldVoiceModel:
      type: string
      enum:
        - value: inworld-tts-1
      default: inworld-tts-1
    InworldVoiceLanguageCode:
      type: string
      enum:
        - value: en
        - value: zh
        - value: ko
        - value: nl
        - value: fr
        - value: es
        - value: ja
        - value: de
        - value: it
        - value: pl
        - value: pt
        - value: ru
        - value: hi
        - value: he
        - value: ar
      default: en
    InworldVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/InworldVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          $ref: '#/components/schemas/InworldVoiceVoiceId'
          description: >-
            Available voices by language:

            • en: Alex, Ashley, Craig, Deborah, Dennis, Edward, Elizabeth,
            Hades, Julia, Pixie, Mark, Olivia, Priya, Ronald, Sarah, Shaun,
            Theodore, Timothy, Wendy, Dominus, Hana, Clive, Carter, Blake, Luna

            • zh: Yichen, Xiaoyin, Xinyi, Jing

            • nl: Erik, Katrien, Lennart, Lore

            • fr: Alain, Hélène, Mathieu, Étienne

            • de: Johanna, Josef

            • it: Gianni, Orietta

            • ja: Asuka, Satoshi

            • ko: Hyunwoo, Minji, Seojun, Yoona

            • pl: Szymon, Wojciech

            • pt: Heitor, Maitê

            • es: Diego, Lupita, Miguel, Rafael

            • ru: Svetlana, Elena, Dmitry, Nikolai

            • hi: Riya, Manoj

            • he: Yael, Oren

            • ar: Nour, Omar
        model:
          $ref: '#/components/schemas/InworldVoiceModel'
          description: This is the model that will be used.
        languageCode:
          $ref: '#/components/schemas/InworldVoiceLanguageCode'
          description: Language code for Inworld TTS synthesis
        temperature:
          type: number
          format: double
          default: 1.1
          description: >-
            A floating point number between 0, exclusive, and 2, inclusive. If
            equal to null or not provided, the model's default temperature of
            1.1 will be used. The temperature parameter controls variance.

            Higher values will make the output more random and can lead to more
            expressive results. Lower values will make it more deterministic.

            See
            https://docs.inworld.ai/docs/tts/capabilities/generating-audio#additional-configurations
            for more details.
        speakingRate:
          type: number
          format: double
          default: 1
          description: >-
            A floating point number between 0.5, inclusive, and 1.5, inclusive.
            If equal to null or not provided, the model's default speaking speed
            of 1.0 will be used.

            Values above 0.8 are recommended for higher quality.

            See
            https://docs.inworld.ai/docs/tts/capabilities/generating-audio#additional-configurations
            for more details.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    MinimaxVoiceProvider:
      type: string
      enum:
        - value: minimax
    MinimaxVoiceModel:
      type: string
      enum:
        - value: speech-02-hd
        - value: speech-02-turbo
        - value: speech-2.5-turbo-preview
      default: speech-02-turbo
    MinimaxVoiceRegion:
      type: string
      enum:
        - value: worldwide
        - value: china
      default: worldwide
    MinimaxVoiceLanguageBoost:
      type: string
      enum:
        - value: Chinese
        - value: Chinese,Yue
        - value: English
        - value: Arabic
        - value: Russian
        - value: Spanish
        - value: French
        - value: Portuguese
        - value: German
        - value: Turkish
        - value: Dutch
        - value: Ukrainian
        - value: Vietnamese
        - value: Indonesian
        - value: Japanese
        - value: Italian
        - value: Korean
        - value: Thai
        - value: Polish
        - value: Romanian
        - value: Greek
        - value: Czech
        - value: Finnish
        - value: Hindi
        - value: Bulgarian
        - value: Danish
        - value: Hebrew
        - value: Malay
        - value: Persian
        - value: Slovak
        - value: Swedish
        - value: Croatian
        - value: Filipino
        - value: Hungarian
        - value: Norwegian
        - value: Slovenian
        - value: Catalan
        - value: Nynorsk
        - value: Tamil
        - value: Afrikaans
        - value: auto
    MinimaxVoice:
      type: object
      properties:
        cachingEnabled:
          type: boolean
          default: true
          description: This is the flag to toggle voice caching for the assistant.
        provider:
          $ref: '#/components/schemas/MinimaxVoiceProvider'
          description: This is the voice provider that will be used.
        voiceId:
          type: string
          description: >-
            This is the provider-specific ID that will be used. Use a voice from
            MINIMAX_PREDEFINED_VOICES or a custom cloned voice ID.
        model:
          $ref: '#/components/schemas/MinimaxVoiceModel'
          description: >-
            This is the model that will be used. Options are 'speech-02-hd' and
            'speech-02-turbo'.

            speech-02-hd is optimized for high-fidelity applications like
            voiceovers and audiobooks.

            speech-02-turbo is designed for real-time applications with low
            latency.


            @default "speech-02-turbo"
        emotion:
          type: string
          description: >-
            The emotion to use for the voice. If not provided, will use
            auto-detect mode.

            Options include: 'happy', 'sad', 'angry', 'fearful', 'surprised',
            'disgusted', 'neutral'
        pitch:
          type: number
          format: double
          default: 0
          description: |-
            Voice pitch adjustment. Range from -12 to 12 semitones.
            @default 0
        speed:
          type: number
          format: double
          default: 1
          description: |-
            Voice speed adjustment. Range from 0.5 to 2.0.
            @default 1.0
        volume:
          type: number
          format: double
          default: 1
          description: |-
            Voice volume adjustment. Range from 0.5 to 2.0.
            @default 1.0
        region:
          $ref: '#/components/schemas/MinimaxVoiceRegion'
          description: The region for Minimax API. Defaults to "worldwide".
        languageBoost:
          $ref: '#/components/schemas/MinimaxVoiceLanguageBoost'
          description: >-
            Language hint for MiniMax T2A. Example: yue (Cantonese), zh
            (Chinese), en (English).
        textNormalizationEnabled:
          type: boolean
          default: true
          description: >-
            Enable MiniMax text normalization to improve number reading and
            formatting.
        chunkPlan:
          $ref: '#/components/schemas/ChunkPlan'
          description: >-
            This is the plan for chunking the model output before it is sent to
            the voice provider.
        fallbackPlan:
          $ref: '#/components/schemas/FallbackPlan'
          description: >-
            This is the plan for voice provider fallbacks in the event that the
            primary voice provider fails.
      required:
        - provider
        - voiceId
    AssistantOverridesVoice:
      oneOf:
        - $ref: '#/components/schemas/AzureVoice'
        - $ref: '#/components/schemas/CartesiaVoice'
        - $ref: '#/components/schemas/CustomVoice'
        - $ref: '#/components/schemas/DeepgramVoice'
        - $ref: '#/components/schemas/ElevenLabsVoice'
        - $ref: '#/components/schemas/HumeVoice'
        - $ref: '#/components/schemas/LMNTVoice'
        - $ref: '#/components/schemas/NeuphonicVoice'
        - $ref: '#/components/schemas/OpenAIVoice'
        - $ref: '#/components/schemas/PlayHTVoice'
        - $ref: '#/components/schemas/WellSaidVoice'
        - $ref: '#/components/schemas/RimeAIVoice'
        - $ref: '#/components/schemas/SmallestAIVoice'
        - $ref: '#/components/schemas/TavusVoice'
        - $ref: '#/components/schemas/VapiVoice'
        - $ref: '#/components/schemas/SesameVoice'
        - $ref: '#/components/schemas/InworldVoice'
        - $ref: '#/components/schemas/MinimaxVoice'
    AssistantOverridesFirstMessageMode:
      type: string
      enum:
        - value: assistant-speaks-first
        - value: assistant-speaks-first-with-model-generated-message
        - value: assistant-waits-for-user
    AssistantOverridesVoicemailDetection0:
      type: string
      enum:
        - value: 'off'
    GoogleVoicemailDetectionPlanProvider:
      type: string
      enum:
        - value: google
    VoicemailDetectionBackoffPlan:
      type: object
      properties:
        startAtSeconds:
          type: number
          format: double
          default: 5
          description: >-
            This is the number of seconds to wait before starting the first
            retry attempt.
        frequencySeconds:
          type: number
          format: double
          default: 5
          description: This is the interval in seconds between retry attempts.
        maxRetries:
          type: number
          format: double
          default: 6
          description: This is the maximum number of retry attempts before giving up.
    GoogleVoicemailDetectionPlanType:
      type: string
      enum:
        - value: audio
        - value: transcript
    GoogleVoicemailDetectionPlan:
      type: object
      properties:
        beepMaxAwaitSeconds:
          type: number
          format: double
          default: 30
          description: >-
            This is the maximum duration from the start of the call that we will
            wait for a voicemail beep, before speaking our message


            - If we detect a voicemail beep before this, we will speak the
            message at that point.


            - Setting too low a value means that the bot will start speaking its
            voicemail message too early. If it does so before the actual beep,
            it will get cut off. You should definitely tune this to your use
            case.


            @default 30

            @min 0

            @max 60
        provider:
          $ref: '#/components/schemas/GoogleVoicemailDetectionPlanProvider'
          description: This is the provider to use for voicemail detection.
        backoffPlan:
          $ref: '#/components/schemas/VoicemailDetectionBackoffPlan'
          description: This is the backoff plan for the voicemail detection.
        type:
          $ref: '#/components/schemas/GoogleVoicemailDetectionPlanType'
          description: |-
            This is the detection type to use for voicemail detection.
            - 'audio': Uses native audio models (default)
            - 'transcript': Uses ASR/transcript-based detection
            @default 'audio' (audio detection)
      required:
        - provider
    OpenAiVoicemailDetectionPlanProvider:
      type: string
      enum:
        - value: openai
    OpenAiVoicemailDetectionPlanType:
      type: string
      enum:
        - value: audio
        - value: transcript
    OpenAIVoicemailDetectionPlan:
      type: object
      properties:
        beepMaxAwaitSeconds:
          type: number
          format: double
          default: 30
          description: >-
            This is the maximum duration from the start of the call that we will
            wait for a voicemail beep, before speaking our message


            - If we detect a voicemail beep before this, we will speak the
            message at that point.


            - Setting too low a value means that the bot will start speaking its
            voicemail message too early. If it does so before the actual beep,
            it will get cut off. You should definitely tune this to your use
            case.


            @default 30

            @min 0

            @max 60
        provider:
          $ref: '#/components/schemas/OpenAiVoicemailDetectionPlanProvider'
          description: This is the provider to use for voicemail detection.
        backoffPlan:
          $ref: '#/components/schemas/VoicemailDetectionBackoffPlan'
          description: This is the backoff plan for the voicemail detection.
        type:
          $ref: '#/components/schemas/OpenAiVoicemailDetectionPlanType'
          description: |-
            This is the detection type to use for voicemail detection.
            - 'audio': Uses native audio models (default)
            - 'transcript': Uses ASR/transcript-based detection
            @default 'audio' (audio detection)
      required:
        - provider
    TwilioVoicemailDetectionPlanProvider:
      type: string
      enum:
        - value: twilio
    TwilioVoicemailDetectionPlanVoicemailDetectionTypes:
      type: string
      enum:
        - value: machine_start
        - value: human
        - value: fax
        - value: unknown
        - value: machine_end_beep
        - value: machine_end_silence
        - value: machine_end_other
    TwilioVoicemailDetectionPlan:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/TwilioVoicemailDetectionPlanProvider'
          description: This is the provider to use for voicemail detection.
        voicemailDetectionTypes:
          $ref: >-
            #/components/schemas/TwilioVoicemailDetectionPlanVoicemailDetectionTypes
          description: >-
            These are the AMD messages from Twilio that are considered as
            voicemail. Default is ['machine_end_beep', 'machine_end_silence'].


            @default {Array} ['machine_end_beep', 'machine_end_silence']
        enabled:
          type: boolean
          description: >-
            This sets whether the assistant should detect voicemail. Defaults to
            true.


            @default true
        machineDetectionTimeout:
          type: number
          format: double
          description: >-
            The number of seconds that Twilio should attempt to perform
            answering machine detection before timing out and returning
            AnsweredBy as unknown. Default is 30 seconds.


            Increasing this value will provide the engine more time to make a
            determination. This can be useful when DetectMessageEnd is provided
            in the MachineDetection parameter and there is an expectation of
            long answering machine greetings that can exceed 30 seconds.


            Decreasing this value will reduce the amount of time the engine has
            to make a determination. This can be particularly useful when the
            Enable option is provided in the MachineDetection parameter and you
            want to limit the time for initial detection.


            Check the [Twilio
            docs](https://www.twilio.com/docs/voice/answering-machine-detection#optional-api-tuning-parameters)
            for more info.


            @default 30
        machineDetectionSpeechThreshold:
          type: number
          format: double
          description: >-
            The number of milliseconds that is used as the measuring stick for
            the length of the speech activity. Durations lower than this value
            will be interpreted as a human, longer as a machine. Default is 2400
            milliseconds.


            Increasing this value will reduce the chance of a False Machine
            (detected machine, actually human) for a long human greeting (e.g.,
            a business greeting) but increase the time it takes to detect a
            machine.


            Decreasing this value will reduce the chances of a False Human
            (detected human, actually machine) for short voicemail greetings.
            The value of this parameter may need to be reduced by more than
            1000ms to detect very short voicemail greetings. A reduction of that
            significance can result in increased False Machine detections.
            Adjusting the MachineDetectionSpeechEndThreshold is likely the
            better approach for short voicemails. Decreasing
            MachineDetectionSpeechThreshold will also reduce the time it takes
            to detect a machine.


            Check the [Twilio
            docs](https://www.twilio.com/docs/voice/answering-machine-detection#optional-api-tuning-parameters)
            for more info.


            @default 2400
        machineDetectionSpeechEndThreshold:
          type: number
          format: double
          description: >-
            The number of milliseconds of silence after speech activity at which
            point the speech activity is considered complete. Default is 1200
            milliseconds.


            Increasing this value will typically be used to better address the
            short voicemail greeting scenarios. For short voicemails, there is
            typically 1000-2000ms of audio followed by 1200-2400ms of silence
            and then additional audio before the beep. Increasing the
            MachineDetectionSpeechEndThreshold to ~2500ms will treat the
            1200-2400ms of silence as a gap in the greeting but not the end of
            the greeting and will result in a machine detection. The downsides
            of such a change include:

            - Increasing the delay for human detection by the amount you
            increase this parameter, e.g., a change of 1200ms to 2500ms
            increases human detection delay by 1300ms.

            - Cases where a human has two utterances separated by a period of
            silence (e.g. a "Hello", then 2000ms of silence, and another
            "Hello") may be interpreted as a machine.


            Decreasing this value will result in faster human detection. The
            consequence is that it can lead to increased False Human (detected
            human, actually machine) detections because a silence gap in a
            voicemail greeting (not necessarily just in short voicemail
            scenarios) can be incorrectly interpreted as the end of speech.


            Check the [Twilio
            docs](https://www.twilio.com/docs/voice/answering-machine-detection#optional-api-tuning-parameters)
            for more info.


            @default 1200
        machineDetectionSilenceTimeout:
          type: number
          format: double
          description: >-
            The number of milliseconds of initial silence after which an unknown
            AnsweredBy result will be returned. Default is 5000 milliseconds.


            Increasing this value will result in waiting for a longer period of
            initial silence before returning an 'unknown' AMD result.


            Decreasing this value will result in waiting for a shorter period of
            initial silence before returning an 'unknown' AMD result.


            Check the [Twilio
            docs](https://www.twilio.com/docs/voice/answering-machine-detection#optional-api-tuning-parameters)
            for more info.


            @default 5000
      required:
        - provider
    VapiVoicemailDetectionPlanProvider:
      type: string
      enum:
        - value: vapi
    VapiVoicemailDetectionPlanType:
      type: string
      enum:
        - value: audio
        - value: transcript
    VapiVoicemailDetectionPlan:
      type: object
      properties:
        beepMaxAwaitSeconds:
          type: number
          format: double
          default: 30
          description: >-
            This is the maximum duration from the start of the call that we will
            wait for a voicemail beep, before speaking our message


            - If we detect a voicemail beep before this, we will speak the
            message at that point.


            - Setting too low a value means that the bot will start speaking its
            voicemail message too early. If it does so before the actual beep,
            it will get cut off. You should definitely tune this to your use
            case.


            @default 30

            @min 0

            @max 60
        provider:
          $ref: '#/components/schemas/VapiVoicemailDetectionPlanProvider'
          description: This is the provider to use for voicemail detection.
        backoffPlan:
          $ref: '#/components/schemas/VoicemailDetectionBackoffPlan'
          description: This is the backoff plan for the voicemail detection.
        type:
          $ref: '#/components/schemas/VapiVoicemailDetectionPlanType'
          description: |-
            This is the detection type to use for voicemail detection.
            - 'audio': Uses native audio models (default)
            - 'transcript': Uses ASR/transcript-based detection
            @default 'audio' (audio detection)
      required:
        - provider
    AssistantOverridesVoicemailDetection:
      oneOf:
        - $ref: '#/components/schemas/AssistantOverridesVoicemailDetection0'
        - $ref: '#/components/schemas/GoogleVoicemailDetectionPlan'
        - $ref: '#/components/schemas/OpenAIVoicemailDetectionPlan'
        - $ref: '#/components/schemas/TwilioVoicemailDetectionPlan'
        - $ref: '#/components/schemas/VapiVoicemailDetectionPlan'
    AssistantOverridesClientMessages:
      type: string
      enum:
        - value: conversation-update
        - value: function-call
        - value: function-call-result
        - value: hang
        - value: language-changed
        - value: metadata
        - value: model-output
        - value: speech-update
        - value: status-update
        - value: transcript
        - value: tool-calls
        - value: tool-calls-result
        - value: tool.completed
        - value: transfer-update
        - value: user-interrupted
        - value: voice-input
        - value: workflow.node.started
        - value: assistant.started
    AssistantOverridesServerMessages:
      type: string
      enum:
        - value: assistant.started
        - value: conversation-update
        - value: end-of-call-report
        - value: function-call
        - value: hang
        - value: language-changed
        - value: language-change-detected
        - value: model-output
        - value: phone-call-control
        - value: speech-update
        - value: status-update
        - value: transcript
        - value: transcript[transcriptType="final"]
        - value: tool-calls
        - value: transfer-destination-request
        - value: handoff-destination-request
        - value: transfer-update
        - value: user-interrupted
        - value: voice-input
        - value: chat.created
        - value: chat.deleted
        - value: session.created
        - value: session.updated
        - value: session.deleted
        - value: call.deleted
        - value: call.delete.failed
    AssistantOverridesBackgroundSound0:
      type: string
      enum:
        - value: 'off'
        - value: office
    AssistantOverridesBackgroundSound:
      oneOf:
        - $ref: '#/components/schemas/AssistantOverridesBackgroundSound0'
        - type: string
          format: uri
    TransportConfigurationTwilioProvider:
      type: string
      enum:
        - value: twilio
    TransportConfigurationTwilioRecordingChannels:
      type: string
      enum:
        - value: mono
        - value: dual
    TransportConfigurationTwilio:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/TransportConfigurationTwilioProvider'
        timeout:
          type: number
          format: double
          description: >-
            The integer number of seconds that we should allow the phone to ring
            before assuming there is no answer.

            The default is `60` seconds and the maximum is `600` seconds.

            For some call flows, we will add a 5-second buffer to the timeout
            value you provide.

            For this reason, a timeout value of 10 seconds could result in an
            actual timeout closer to 15 seconds.

            You can set this to a short time, such as `15` seconds, to hang up
            before reaching an answering machine or voicemail.


            @default 60
        record:
          type: boolean
          description: |-
            Whether to record the call.
            Can be `true` to record the phone call, or `false` to not.
            The default is `false`.

            @default false
        recordingChannels:
          $ref: '#/components/schemas/TransportConfigurationTwilioRecordingChannels'
          description: >-
            The number of channels in the final recording.

            Can be: `mono` or `dual`.

            The default is `mono`.

            `mono` records both legs of the call in a single channel of the
            recording file.

            `dual` records each leg to a separate channel of the recording file.

            The first channel of a dual-channel recording contains the parent
            call and the second channel contains the child call.


            @default 'mono'
      required:
        - provider
    AssistantOverridesTransportConfigurationsItems:
      oneOf:
        - $ref: '#/components/schemas/TransportConfigurationTwilio'
    LangfuseObservabilityPlanProvider:
      type: string
      enum:
        - value: langfuse
    LangfuseObservabilityPlanMetadata:
      type: object
      properties: {}
    LangfuseObservabilityPlan:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/LangfuseObservabilityPlanProvider'
        promptName:
          type: string
          description: >-
            The name of a Langfuse prompt to link generations to. This enables
            tracking which prompt version was used for each generation.
            https://langfuse.com/docs/prompt-management/features/link-to-traces
        promptVersion:
          type: number
          format: double
          description: >-
            The version number of the Langfuse prompt to link generations to.
            Used together with promptName to identify the exact prompt version.
            https://langfuse.com/docs/prompt-management/features/link-to-traces
        tags:
          type: array
          items:
            type: string
          description: >-
            This is an array of tags to be added to the Langfuse trace. Tags
            allow you to categorize and filter traces.
            https://langfuse.com/docs/tracing-features/tags
        metadata:
          $ref: '#/components/schemas/LangfuseObservabilityPlanMetadata'
          description: >-
            This is a JSON object that will be added to the Langfuse trace.
            Traces can be enriched with metadata to better understand your
            users, application, and experiments.
            https://langfuse.com/docs/tracing-features/metadata

            By default it includes the call metadata, assistant metadata, and
            assistant overrides.
      required:
        - provider
        - tags
    UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAnthropicBedrockRegion:
      type: string
      enum:
        - value: us-east-1
        - value: us-west-2
        - value: eu-west-1
        - value: eu-west-3
        - value: ap-northeast-1
        - value: ap-southeast-2
    AwsiamCredentialsAuthenticationPlanType:
      type: string
      enum:
        - value: aws-iam
    AWSIAMCredentialsAuthenticationPlan:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/AwsiamCredentialsAuthenticationPlanType'
        awsAccessKeyId:
          type: string
          description: AWS Access Key ID. This is not returned in the API.
        awsSecretAccessKey:
          type: string
          description: AWS Secret Access Key. This is not returned in the API.
      required:
        - type
        - awsAccessKeyId
        - awsSecretAccessKey
    AwsStsAuthenticationPlanType:
      type: string
      enum:
        - value: aws-sts
    AWSStsAuthenticationPlan:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/AwsStsAuthenticationPlanType'
          description: This is the type of authentication plan
        roleArn:
          type: string
          description: This is the role ARN for the AWS credential
        externalId:
          type: string
          description: >-
            Optional external ID for additional security in the role trust
            policy.
      required:
        - type
        - roleArn
    UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAnthropicBedrockAuthenticationPlan:
      oneOf:
        - $ref: '#/components/schemas/AWSIAMCredentialsAuthenticationPlan'
        - $ref: '#/components/schemas/AWSStsAuthenticationPlan'
    UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureOpenaiRegion:
      type: string
      enum:
        - value: australia
        - value: canadaeast
        - value: canadacentral
        - value: eastus2
        - value: eastus
        - value: france
        - value: germanywestcentral
        - value: india
        - value: japaneast
        - value: japanwest
        - value: northcentralus
        - value: norway
        - value: polandcentral
        - value: southcentralus
        - value: spaincentral
        - value: swedencentral
        - value: switzerland
        - value: uaenorth
        - value: uk
        - value: westeurope
        - value: westus
        - value: westus3
    UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureOpenaiModels:
      type: string
      enum:
        - value: gpt-5.2
        - value: gpt-5.2-chat
        - value: gpt-5.1
        - value: gpt-5.1-chat
        - value: gpt-5
        - value: gpt-5-mini
        - value: gpt-5-nano
        - value: gpt-4.1-2025-04-14
        - value: gpt-4.1-mini-2025-04-14
        - value: gpt-4.1-nano-2025-04-14
        - value: gpt-4o-2024-11-20
        - value: gpt-4o-2024-08-06
        - value: gpt-4o-2024-05-13
        - value: gpt-4o-mini-2024-07-18
        - value: gpt-4-turbo-2024-04-09
        - value: gpt-4-0125-preview
        - value: gpt-4-1106-preview
        - value: gpt-4-0613
        - value: gpt-35-turbo-0125
        - value: gpt-35-turbo-1106
    UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureService:
      type: string
      enum:
        - value: speech
        - value: blob_storage
      default: speech
    UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureRegion:
      type: string
      enum:
        - value: australia
        - value: canadaeast
        - value: canadacentral
        - value: eastus2
        - value: eastus
        - value: france
        - value: germanywestcentral
        - value: india
        - value: japaneast
        - value: japanwest
        - value: northcentralus
        - value: norway
        - value: polandcentral
        - value: southcentralus
        - value: spaincentral
        - value: swedencentral
        - value: switzerland
        - value: uaenorth
        - value: uk
        - value: westeurope
        - value: westus
        - value: westus3
    AzureBlobStorageBucketPlan:
      type: object
      properties:
        connectionString:
          type: string
          description: This is the blob storage connection string for the Azure resource.
        containerName:
          type: string
          description: This is the container name for the Azure blob storage.
        path:
          type: string
          description: >-
            This is the path where call artifacts will be stored.


            Usage:

            - To store call artifacts in a specific folder, set this to the full
            path. Eg. "/folder-name1/folder-name2".

            - To store call artifacts in the root of the bucket, leave this
            blank.


            @default "/"
      required:
        - connectionString
        - containerName
    SipTrunkGatewayOutboundProtocol:
      type: string
      enum:
        - value: tls/srtp
        - value: tcp
        - value: tls
        - value: udp
    SipTrunkGateway:
      type: object
      properties:
        ip:
          type: string
          description: >-
            This is the address of the gateway. It can be an IPv4 address like
            1.1.1.1 or a fully qualified domain name like
            my-sip-trunk.pstn.twilio.com.
        port:
          type: number
          format: double
          description: |-
            This is the port number of the gateway. Default is 5060.

            @default 5060
        netmask:
          type: number
          format: double
          description: |-
            This is the netmask of the gateway. Defaults to 32.

            @default 32
        inboundEnabled:
          type: boolean
          description: >-
            This is whether inbound calls are allowed from this gateway. Default
            is true.


            @default true
        outboundEnabled:
          type: boolean
          description: >-
            This is whether outbound calls should be sent to this gateway.
            Default is true.


            Note, if netmask is less than 32, it doesn't affect the outbound IPs
            that are tried. 1 attempt is made to `ip:port`.


            @default true
        outboundProtocol:
          $ref: '#/components/schemas/SipTrunkGatewayOutboundProtocol'
          description: >-
            This is the protocol to use for SIP signaling outbound calls.
            Default is udp.


            @default udp
        optionsPingEnabled:
          type: boolean
          description: >-
            This is whether to send options ping to the gateway. This can be
            used to check if the gateway is reachable. Default is false.


            This is useful for high availability setups where you want to check
            if the gateway is reachable before routing calls to it. Note, if no
            gateway for a trunk is reachable, outbound calls will be rejected.


            @default false
      required:
        - ip
    SipTrunkOutboundSipRegisterPlan:
      type: object
      properties:
        domain:
          type: string
        username:
          type: string
        realm:
          type: string
    SipTrunkOutboundAuthenticationPlan:
      type: object
      properties:
        authPassword:
          type: string
          description: This is not returned in the API.
        authUsername:
          type: string
        sipRegisterPlan:
          $ref: '#/components/schemas/SipTrunkOutboundSipRegisterPlan'
          description: >-
            This can be used to configure if SIP register is required by the SIP
            trunk. If not provided, no SIP registration will be attempted.
    SbcConfiguration:
      type: object
      properties: {}
    CreateCerebrasCredentialDtoProvider:
      type: string
      enum:
        - value: cerebras
    CloudflareR2BucketPlan:
      type: object
      properties:
        accessKeyId:
          type: string
          description: Cloudflare R2 Access key ID.
        secretAccessKey:
          type: string
          description: Cloudflare R2 access key secret. This is not returned in the API.
        url:
          type: string
          description: Cloudflare R2 base url.
        name:
          type: string
          description: This is the name of the bucket.
        path:
          type: string
          description: >-
            This is the path where call artifacts will be stored.


            Usage:

            - To store call artifacts in a specific folder, set this to the full
            path. Eg. "/folder-name1/folder-name2".

            - To store call artifacts in the root of the bucket, leave this
            blank.


            @default "/"
      required:
        - name
    OAuth2AuthenticationPlan:
      type: object
      properties:
        url:
          type: string
          description: This is the OAuth2 URL.
        clientId:
          type: string
          description: This is the OAuth2 client ID.
        clientSecret:
          type: string
          description: This is the OAuth2 client secret.
        scope:
          type: string
          description: This is the scope of the OAuth2 token.
      required:
        - url
        - clientId
        - clientSecret
    GcpKey:
      type: object
      properties:
        type:
          type: string
          description: This is the type of the key. Most likely, this is "service_account".
        projectId:
          type: string
          description: This is the ID of the Google Cloud project associated with this key.
        privateKeyId:
          type: string
          description: This is the unique identifier for the private key.
        privateKey:
          type: string
          description: |-
            This is the private key in PEM format.

            Note: This is not returned in the API.
        clientEmail:
          type: string
          description: This is the email address associated with the service account.
        clientId:
          type: string
          description: This is the unique identifier for the client.
        authUri:
          type: string
          description: This is the URI for the auth provider's authorization endpoint.
        tokenUri:
          type: string
          description: This is the URI for the auth provider's token endpoint.
        authProviderX509CertUrl:
          type: string
          description: >-
            This is the URL of the public x509 certificate for the auth
            provider.
        clientX509CertUrl:
          type: string
          description: This is the URL of the public x509 certificate for the client.
        universeDomain:
          type: string
          description: >-
            This is the domain associated with the universe this service account
            belongs to.
      required:
        - type
        - projectId
        - privateKeyId
        - privateKey
        - clientEmail
        - clientId
        - authUri
        - tokenUri
        - authProviderX509CertUrl
        - clientX509CertUrl
        - universeDomain
    BucketPlan:
      type: object
      properties:
        name:
          type: string
          description: This is the name of the bucket.
        region:
          type: string
          description: >-
            This is the region of the bucket.


            Usage:

            - If `credential.type` is `aws`, then this is required.

            - If `credential.type` is `gcp`, then this is optional since GCP
            allows buckets to be accessed without a region but region is
            required for data residency requirements. Read here:
            https://cloud.google.com/storage/docs/request-endpoints


            This overrides the `credential.region` field if it is provided.
        path:
          type: string
          description: >-
            This is the path where call artifacts will be stored.


            Usage:

            - To store call artifacts in a specific folder, set this to the full
            path. Eg. "/folder-name1/folder-name2".

            - To store call artifacts in the root of the bucket, leave this
            blank.


            @default "/"
        hmacAccessKey:
          type: string
          description: >-
            This is the HMAC access key offered by GCP for interoperability with
            S3 clients. Here is the guide on how to create:
            https://cloud.google.com/storage/docs/authentication/managing-hmackeys#console


            Usage:

            - If `credential.type` is `gcp`, then this is required.

            - If `credential.type` is `aws`, then this is not required since
            credential.awsAccessKeyId is used instead.
        hmacSecret:
          type: string
          description: >-
            This is the secret for the HMAC access key. Here is the guide on how
            to create:
            https://cloud.google.com/storage/docs/authentication/managing-hmackeys#console


            Usage:

            - If `credential.type` is `gcp`, then this is required.

            - If `credential.type` is `aws`, then this is not required since
            credential.awsSecretAccessKey is used instead.


            Note: This is not returned in the API.
      required:
        - name
    CreateGoogleCredentialDtoProvider:
      type: string
      enum:
        - value: google
    CreateInflectionAiCredentialDtoProvider:
      type: string
      enum:
        - value: inflection-ai
    SupabaseBucketPlanRegion:
      type: string
      enum:
        - value: us-west-1
        - value: us-east-1
        - value: us-east-2
        - value: ca-central-1
        - value: eu-west-1
        - value: eu-west-2
        - value: eu-west-3
        - value: eu-central-1
        - value: eu-central-2
        - value: eu-north-1
        - value: ap-south-1
        - value: ap-southeast-1
        - value: ap-northeast-1
        - value: ap-northeast-2
        - value: ap-southeast-2
        - value: sa-east-1
    SupabaseBucketPlan:
      type: object
      properties:
        region:
          $ref: '#/components/schemas/SupabaseBucketPlanRegion'
          description: >-
            This is the S3 Region. It should look like us-east-1

            It should be one of the supabase regions defined in the
            SUPABASE_REGION enum

            Check https://supabase.com/docs/guides/platform/regions for up to
            date regions
        url:
          type: string
          description: |-
            This is the S3 compatible URL for Supabase S3
            This should look like https://<project-ID>.supabase.co/storage/v1/s3
        accessKeyId:
          type: string
          description: |-
            This is the Supabase S3 Access Key ID.
            The user creates this in the Supabase project Storage settings
        secretAccessKey:
          type: string
          description: >-
            This is the Supabase S3 Secret Access Key.

            The user creates this in the Supabase project Storage settings along
            with the access key id
        name:
          type: string
          description: >-
            This is the Supabase S3 Bucket Name.

            The user must create this in Supabase under Storage > Buckets

            A bucket that does not exist will not be checked now, but file
            uploads will fail
        path:
          type: string
          description: >-
            This is the Supabase S3 Bucket Folder Path.

            The user can create this in Supabase under Storage > Buckets

            A path that does not exist will not be checked now, but file uploads
            will fail

            A Path is like a folder in the bucket

            Eg. If the bucket is called "my-bucket" and the path is "my-folder",
            the full path is "my-bucket/my-folder"
      required:
        - region
        - url
        - accessKeyId
        - secretAccessKey
        - name
    HmacAuthenticationPlanType:
      type: string
      enum:
        - value: hmac
    HmacAuthenticationPlanAlgorithm:
      type: string
      enum:
        - value: sha256
        - value: sha512
        - value: sha1
    HmacAuthenticationPlanSignatureEncoding:
      type: string
      enum:
        - value: hex
        - value: base64
    BearerAuthenticationPlanType:
      type: string
      enum:
        - value: bearer
    UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingWebhookAuthenticationPlan:
      oneOf:
        - type: object
          properties:
            type:
              type: string
              enum:
                - oauth2
              description: 'Discriminator value: oauth2'
            url:
              type: string
              description: This is the OAuth2 URL.
            clientId:
              type: string
              description: This is the OAuth2 client ID.
            clientSecret:
              type: string
              description: This is the OAuth2 client secret.
            scope:
              type: string
              description: This is the scope of the OAuth2 token.
          required:
            - type
            - url
            - clientId
            - clientSecret
          description: oauth2 variant
        - type: object
          properties:
            type:
              $ref: '#/components/schemas/HmacAuthenticationPlanType'
            secretKey:
              type: string
              description: This is the HMAC secret key used to sign requests.
            algorithm:
              $ref: '#/components/schemas/HmacAuthenticationPlanAlgorithm'
              description: This is the HMAC algorithm to use for signing.
            signatureHeader:
              type: string
              description: >-
                This is the header name where the signature will be sent.
                Defaults to 'x-signature'.
            timestampHeader:
              type: string
              description: >-
                This is the header name where the timestamp will be sent.
                Defaults to 'x-timestamp'.
            signaturePrefix:
              type: string
              description: >-
                This is the prefix for the signature. For example, 'sha256=' for
                GitHub-style signatures.
            includeTimestamp:
              type: boolean
              description: >-
                Whether to include a timestamp in the signature payload.
                Defaults to true.
            payloadFormat:
              type: string
              description: >-
                Custom payload format. Use {body} for request body, {timestamp}
                for timestamp, {method} for HTTP method, {url} for URL,
                {svix-id} for unique message ID. Defaults to
                '{timestamp}.{body}'.
            messageIdHeader:
              type: string
              description: >-
                This is the header name where the unique message ID will be
                sent. Used for Svix-style webhooks.
            signatureEncoding:
              $ref: '#/components/schemas/HmacAuthenticationPlanSignatureEncoding'
              description: The encoding format for the signature. Defaults to 'hex'.
            secretIsBase64:
              type: boolean
              description: >-
                Whether the secret key is base64-encoded and should be decoded
                before use. Defaults to false.
          required:
            - type
            - secretKey
            - algorithm
          description: hmac variant
        - type: object
          properties:
            type:
              $ref: '#/components/schemas/BearerAuthenticationPlanType'
            token:
              type: string
              description: This is the bearer token value.
            headerName:
              type: string
              description: >-
                This is the header name where the bearer token will be sent.
                Defaults to 'Authorization'.
            bearerPrefixEnabled:
              type: boolean
              description: >-
                Whether to include the 'Bearer ' prefix in the header value.
                Defaults to true.
          required:
            - type
            - token
          description: bearer variant
      discriminator:
        propertyName: type
    CreateCustomCredentialDtoProvider:
      type: string
      enum:
        - value: custom-credential
    CreateCustomCredentialDtoAuthenticationPlan:
      oneOf:
        - type: object
          properties:
            type:
              type: string
              enum:
                - oauth2
              description: 'Discriminator value: oauth2'
            url:
              type: string
              description: This is the OAuth2 URL.
            clientId:
              type: string
              description: This is the OAuth2 client ID.
            clientSecret:
              type: string
              description: This is the OAuth2 client secret.
            scope:
              type: string
              description: This is the scope of the OAuth2 token.
          required:
            - type
            - url
            - clientId
            - clientSecret
          description: oauth2 variant
        - type: object
          properties:
            type:
              $ref: '#/components/schemas/HmacAuthenticationPlanType'
            secretKey:
              type: string
              description: This is the HMAC secret key used to sign requests.
            algorithm:
              $ref: '#/components/schemas/HmacAuthenticationPlanAlgorithm'
              description: This is the HMAC algorithm to use for signing.
            signatureHeader:
              type: string
              description: >-
                This is the header name where the signature will be sent.
                Defaults to 'x-signature'.
            timestampHeader:
              type: string
              description: >-
                This is the header name where the timestamp will be sent.
                Defaults to 'x-timestamp'.
            signaturePrefix:
              type: string
              description: >-
                This is the prefix for the signature. For example, 'sha256=' for
                GitHub-style signatures.
            includeTimestamp:
              type: boolean
              description: >-
                Whether to include a timestamp in the signature payload.
                Defaults to true.
            payloadFormat:
              type: string
              description: >-
                Custom payload format. Use {body} for request body, {timestamp}
                for timestamp, {method} for HTTP method, {url} for URL,
                {svix-id} for unique message ID. Defaults to
                '{timestamp}.{body}'.
            messageIdHeader:
              type: string
              description: >-
                This is the header name where the unique message ID will be
                sent. Used for Svix-style webhooks.
            signatureEncoding:
              $ref: '#/components/schemas/HmacAuthenticationPlanSignatureEncoding'
              description: The encoding format for the signature. Defaults to 'hex'.
            secretIsBase64:
              type: boolean
              description: >-
                Whether the secret key is base64-encoded and should be decoded
                before use. Defaults to false.
          required:
            - type
            - secretKey
            - algorithm
          description: hmac variant
        - type: object
          properties:
            type:
              $ref: '#/components/schemas/BearerAuthenticationPlanType'
            token:
              type: string
              description: This is the bearer token value.
            headerName:
              type: string
              description: >-
                This is the header name where the bearer token will be sent.
                Defaults to 'Authorization'.
            bearerPrefixEnabled:
              type: boolean
              description: >-
                Whether to include the 'Bearer ' prefix in the header value.
                Defaults to true.
          required:
            - type
            - token
          description: bearer variant
      discriminator:
        propertyName: type
    PublicKeyEncryptionPlanType:
      type: string
      enum:
        - value: public-key
    PublicKeyEncryptionPlanAlgorithm:
      type: string
      enum:
        - value: RSA-OAEP-256
    SpkiPemPublicKeyConfigFormat:
      type: string
      enum:
        - value: spki-pem
    SpkiPemPublicKeyConfig:
      type: object
      properties:
        name:
          type: string
          description: Optional name of the key for identification purposes.
        format:
          $ref: '#/components/schemas/SpkiPemPublicKeyConfigFormat'
          description: The format of the public key.
        pem:
          type: string
          description: The PEM-encoded public key.
      required:
        - format
        - pem
    PublicKeyEncryptionPlan:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/PublicKeyEncryptionPlanType'
          description: The type of encryption plan.
        algorithm:
          $ref: '#/components/schemas/PublicKeyEncryptionPlanAlgorithm'
          description: The encryption algorithm to use.
        publicKey:
          $ref: '#/components/schemas/SpkiPemPublicKeyConfig'
          description: The public key configuration.
      required:
        - type
        - algorithm
        - publicKey
    CreateNeuphonicCredentialDtoProvider:
      type: string
      enum:
        - value: neuphonic
    CreateHumeCredentialDtoProvider:
      type: string
      enum:
        - value: hume
    CreateMistralCredentialDtoProvider:
      type: string
      enum:
        - value: mistral
    CreateSpeechmaticsCredentialDtoProvider:
      type: string
      enum:
        - value: speechmatics
    CreateTrieveCredentialDtoProvider:
      type: string
      enum:
        - value: trieve
    CreateGoHighLevelMcpCredentialDtoProvider:
      type: string
      enum:
        - value: ghl.oauth2-authorization
    Oauth2AuthenticationSession:
      type: object
      properties:
        accessToken:
          type: string
          description: This is the OAuth2 access token.
        expiresAt:
          type: string
          format: date-time
          description: This is the OAuth2 access token expiration.
        refreshToken:
          type: string
          description: This is the OAuth2 refresh token.
    CreateInworldCredentialDtoProvider:
      type: string
      enum:
        - value: inworld
    CreateWellSaidCredentialDtoProvider:
      type: string
      enum:
        - value: wellsaid
    AssistantOverridesCredentialsItems:
      oneOf:
        - type: object
          properties:
            provider:
              type: string
              enum:
                - 11labs
              description: 'Discriminator value: 11labs'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: 11labs variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - anthropic
              description: 'Discriminator value: anthropic'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: anthropic variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - anthropic-bedrock
              description: 'Discriminator value: anthropic-bedrock'
            region:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAnthropicBedrockRegion
              description: AWS region where Bedrock is configured.
            authenticationPlan:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAnthropicBedrockAuthenticationPlan
              description: >-
                Authentication method - either direct IAM credentials or
                cross-account role assumption.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - region
            - authenticationPlan
          description: anthropic-bedrock variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - anyscale
              description: 'Discriminator value: anyscale'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: anyscale variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - assembly-ai
              description: 'Discriminator value: assembly-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: assembly-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - azure-openai
              description: 'Discriminator value: azure-openai'
            region:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureOpenaiRegion
            models:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureOpenaiModels
            openAIKey:
              type: string
              description: This is not returned in the API.
            ocpApimSubscriptionKey:
              type: string
              description: This is not returned in the API.
            openAIEndpoint:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - region
            - models
            - openAIKey
            - openAIEndpoint
          description: azure-openai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - azure
              description: 'Discriminator value: azure'
            service:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureService
              description: This is the service being used in Azure.
            region:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureRegion
              description: This is the region of the Azure resource.
            apiKey:
              type: string
              description: This is not returned in the API.
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            bucketPlan:
              $ref: '#/components/schemas/AzureBlobStorageBucketPlan'
              description: >-
                This is the bucket plan that can be provided to store call
                artifacts in Azure Blob Storage.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - service
          description: azure variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - byo-sip-trunk
              description: 'Discriminator value: byo-sip-trunk'
            gateways:
              type: array
              items:
                $ref: '#/components/schemas/SipTrunkGateway'
              description: This is the list of SIP trunk's gateways.
            outboundAuthenticationPlan:
              $ref: '#/components/schemas/SipTrunkOutboundAuthenticationPlan'
              description: >-
                This can be used to configure the outbound authentication if
                required by the SIP trunk.
            outboundLeadingPlusEnabled:
              type: boolean
              description: >-
                This ensures the outbound origination attempts have a leading
                plus. Defaults to false to match conventional telecom behavior.


                Usage:

                - Vonage/Twilio requires leading plus for all outbound calls.
                Set this to true.


                @default false
            techPrefix:
              type: string
              description: >-
                This can be used to configure the tech prefix on outbound calls.
                This is an advanced property.
            sipDiversionHeader:
              type: string
              description: >-
                This can be used to enable the SIP diversion header for
                authenticating the calling number if the SIP trunk supports it.
                This is an advanced property.
            sbcConfiguration:
              $ref: '#/components/schemas/SbcConfiguration'
              description: >-
                This is an advanced configuration for enterprise deployments.
                This uses the onprem SBC to trunk into the SIP trunk's
                `gateways`, rather than the managed SBC provided by Vapi.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - gateways
          description: byo-sip-trunk variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - cartesia
              description: 'Discriminator value: cartesia'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: cartesia variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateCerebrasCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: cerebras variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - cloudflare
              description: 'Discriminator value: cloudflare'
            accountId:
              type: string
              description: Cloudflare Account Id.
            apiKey:
              type: string
              description: Cloudflare API Key / Token.
            accountEmail:
              type: string
              description: Cloudflare Account Email.
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            bucketPlan:
              $ref: '#/components/schemas/CloudflareR2BucketPlan'
              description: >-
                This is the bucket plan that can be provided to store call
                artifacts in R2
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
          description: cloudflare variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - custom-llm
              description: 'Discriminator value: custom-llm'
            apiKey:
              type: string
              description: This is not returned in the API.
            authenticationPlan:
              $ref: '#/components/schemas/OAuth2AuthenticationPlan'
              description: >-
                This is the authentication plan. Currently supports OAuth2 RFC
                6749. To use Bearer authentication, use apiKey
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: custom-llm variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - deepgram
              description: 'Discriminator value: deepgram'
            apiKey:
              type: string
              description: This is not returned in the API.
            apiUrl:
              type: string
              description: >-
                This can be used to point to an onprem Deepgram instance.
                Defaults to api.deepgram.com.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: deepgram variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - deepinfra
              description: 'Discriminator value: deepinfra'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: deepinfra variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - deep-seek
              description: 'Discriminator value: deep-seek'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: deep-seek variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - gcp
              description: 'Discriminator value: gcp'
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            gcpKey:
              $ref: '#/components/schemas/GcpKey'
              description: >-
                This is the GCP key. This is the JSON that can be generated in
                the Google Cloud Console at
                https://console.cloud.google.com/iam-admin/serviceaccounts/details/<service-account-id>/keys.


                The schema is identical to the JSON that GCP outputs.
            region:
              type: string
              description: This is the region of the GCP resource.
            bucketPlan:
              $ref: '#/components/schemas/BucketPlan'
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - gcpKey
          description: gcp variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - gladia
              description: 'Discriminator value: gladia'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: gladia variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - gohighlevel
              description: 'Discriminator value: gohighlevel'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: gohighlevel variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateGoogleCredentialDtoProvider'
              description: >-
                This is the key for Gemini in Google AI Studio. Get it from
                here: https://aistudio.google.com/app/apikey
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: google variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - groq
              description: 'Discriminator value: groq'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: groq variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateInflectionAiCredentialDtoProvider'
              description: >-
                This is the api key for Pi in InflectionAI's console. Get it
                from here: https://developers.inflection.ai/keys, billing will
                need to be setup
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: inflection-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - langfuse
              description: 'Discriminator value: langfuse'
            publicKey:
              type: string
              description: 'The public key for Langfuse project. Eg: pk-lf-...'
            apiKey:
              type: string
              description: >-
                The secret key for Langfuse project. Eg: sk-lf-... .This is not
                returned in the API.
            apiUrl:
              type: string
              description: >-
                The host URL for Langfuse project. Eg:
                https://cloud.langfuse.com
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - publicKey
            - apiKey
            - apiUrl
          description: langfuse variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - lmnt
              description: 'Discriminator value: lmnt'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: lmnt variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - make
              description: 'Discriminator value: make'
            teamId:
              type: string
              description: Team ID
            region:
              type: string
              description: 'Region of your application. For example: eu1, eu2, us1, us2'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - teamId
            - region
            - apiKey
          description: make variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - openai
              description: 'Discriminator value: openai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: openai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - openrouter
              description: 'Discriminator value: openrouter'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: openrouter variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - perplexity-ai
              description: 'Discriminator value: perplexity-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: perplexity-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - playht
              description: 'Discriminator value: playht'
            apiKey:
              type: string
              description: This is not returned in the API.
            userId:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
            - userId
          description: playht variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - rime-ai
              description: 'Discriminator value: rime-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: rime-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - runpod
              description: 'Discriminator value: runpod'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: runpod variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - s3
              description: 'Discriminator value: s3'
            awsAccessKeyId:
              type: string
              description: AWS access key ID.
            awsSecretAccessKey:
              type: string
              description: AWS access key secret. This is not returned in the API.
            region:
              type: string
              description: AWS region in which the S3 bucket is located.
            s3BucketName:
              type: string
              description: AWS S3 bucket name.
            s3PathPrefix:
              type: string
              description: The path prefix for the uploaded recording. Ex. "recordings/"
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - awsAccessKeyId
            - awsSecretAccessKey
            - region
            - s3BucketName
            - s3PathPrefix
          description: s3 variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - supabase
              description: 'Discriminator value: supabase'
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            bucketPlan:
              $ref: '#/components/schemas/SupabaseBucketPlan'
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
          description: supabase variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - smallest-ai
              description: 'Discriminator value: smallest-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: smallest-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - tavus
              description: 'Discriminator value: tavus'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: tavus variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - together-ai
              description: 'Discriminator value: together-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: together-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - twilio
              description: 'Discriminator value: twilio'
            authToken:
              type: string
              description: This is not returned in the API.
            apiKey:
              type: string
              description: This is not returned in the API.
            apiSecret:
              type: string
              description: This is not returned in the API.
            accountSid:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - accountSid
          description: twilio variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - vonage
              description: 'Discriminator value: vonage'
            apiSecret:
              type: string
              description: This is not returned in the API.
            apiKey:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiSecret
            - apiKey
          description: vonage variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - webhook
              description: 'Discriminator value: webhook'
            authenticationPlan:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingWebhookAuthenticationPlan
              description: >-
                This is the authentication plan. Supports OAuth2 RFC 6749, HMAC
                signing, and Bearer authentication.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authenticationPlan
          description: webhook variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateCustomCredentialDtoProvider'
            authenticationPlan:
              $ref: '#/components/schemas/CreateCustomCredentialDtoAuthenticationPlan'
              description: >-
                This is the authentication plan. Supports OAuth2 RFC 6749, HMAC
                signing, and Bearer authentication.
            encryptionPlan:
              $ref: '#/components/schemas/PublicKeyEncryptionPlan'
              description: >-
                This is the encryption plan for encrypting sensitive data.
                Currently supports public-key encryption.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authenticationPlan
          description: custom-credential variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - xai
              description: 'Discriminator value: xai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: xai variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateNeuphonicCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: neuphonic variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateHumeCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: hume variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateMistralCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: mistral variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateSpeechmaticsCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: speechmatics variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateTrieveCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: trieve variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - google.calendar.oauth2-client
              description: 'Discriminator value: google.calendar.oauth2-client'
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
          description: google.calendar.oauth2-client variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - google.calendar.oauth2-authorization
              description: 'Discriminator value: google.calendar.oauth2-authorization'
            authorizationId:
              type: string
              description: The authorization ID for the OAuth2 authorization
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authorizationId
          description: google.calendar.oauth2-authorization variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - google.sheets.oauth2-authorization
              description: 'Discriminator value: google.sheets.oauth2-authorization'
            authorizationId:
              type: string
              description: The authorization ID for the OAuth2 authorization
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authorizationId
          description: google.sheets.oauth2-authorization variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - slack.oauth2-authorization
              description: 'Discriminator value: slack.oauth2-authorization'
            authorizationId:
              type: string
              description: The authorization ID for the OAuth2 authorization
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authorizationId
          description: slack.oauth2-authorization variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateGoHighLevelMcpCredentialDtoProvider'
            authenticationSession:
              $ref: '#/components/schemas/Oauth2AuthenticationSession'
              description: This is the authentication session for the credential.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authenticationSession
          description: ghl.oauth2-authorization variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateInworldCredentialDtoProvider'
            apiKey:
              type: string
              description: >-
                This is the Inworld Basic (Base64) authentication token. This is
                not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: inworld variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - minimax
              description: 'Discriminator value: minimax'
            apiKey:
              type: string
              description: This is not returned in the API.
            groupId:
              type: string
              description: This is the Minimax Group ID.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
            - groupId
          description: minimax variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateWellSaidCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: wellsaid variant
      discriminator:
        propertyName: provider
    CallHookCallEndingOn:
      type: string
      enum:
        - value: call.ending
    ToolCallHookActionType:
      type: string
      enum:
        - value: tool
    ToolCallHookActionTool:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    ToolCallHookAction:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/ToolCallHookActionType'
          description: This is the type of action - must be "tool"
        tool:
          $ref: '#/components/schemas/ToolCallHookActionTool'
          description: >-
            This is the tool to call. To use an existing tool, send `toolId`
            instead.
        toolId:
          type: string
          description: >-
            This is the tool to call. To use a transient tool, send `tool`
            instead.
      required:
        - type
    MessageAddHookActionType:
      type: string
      enum:
        - value: message.add
    MessageAddHookAction:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/MessageAddHookActionType'
          description: This is the type of action - must be "message.add"
        message:
          $ref: '#/components/schemas/OpenAIMessage'
          description: The message to add to the conversation in OpenAI format
        triggerResponseEnabled:
          type: boolean
          default: true
          description: Whether to trigger an assistant response after adding the message
      required:
        - type
        - message
    CallHookCallEndingDoItems:
      oneOf:
        - $ref: '#/components/schemas/ToolCallHookAction'
        - $ref: '#/components/schemas/MessageAddHookAction'
    CallHookFilterType:
      type: string
      enum:
        - value: oneOf
    CallHookFilter:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/CallHookFilterType'
          description: This is the type of filter - currently only "oneOf" is supported
        key:
          type: string
          description: This is the key to filter on (e.g. "call.endedReason")
        oneOf:
          type: array
          items:
            type: string
          description: This is the array of possible values to match against
      required:
        - type
        - key
        - oneOf
    CallHookCallEnding:
      type: object
      properties:
        'on':
          $ref: '#/components/schemas/CallHookCallEndingOn'
          description: This is the event that triggers this hook
        do:
          type: array
          items:
            $ref: '#/components/schemas/CallHookCallEndingDoItems'
          description: This is the set of actions to perform when the hook triggers
        filters:
          type: array
          items:
            $ref: '#/components/schemas/CallHookFilter'
          description: This is the set of filters that must match for the hook to trigger
      required:
        - 'on'
        - do
    CallHookAssistantSpeechInterruptedOn:
      type: string
      enum:
        - value: assistant.speech.interrupted
    SayHookActionType:
      type: string
      enum:
        - value: say
    SystemMessage:
      type: object
      properties:
        role:
          type: string
          description: The role of the system in the conversation.
        message:
          type: string
          description: The message content from the system.
        time:
          type: number
          format: double
          description: The timestamp when the message was sent.
        secondsFromStart:
          type: number
          format: double
          description: The number of seconds from the start of the conversation.
      required:
        - role
        - message
        - time
        - secondsFromStart
    UserMessageMetadata:
      type: object
      properties: {}
    UserMessage:
      type: object
      properties:
        role:
          type: string
          description: The role of the user in the conversation.
        message:
          type: string
          description: The message content from the user.
        time:
          type: number
          format: double
          description: The timestamp when the message was sent.
        endTime:
          type: number
          format: double
          description: The timestamp when the message ended.
        secondsFromStart:
          type: number
          format: double
          description: The number of seconds from the start of the conversation.
        duration:
          type: number
          format: double
          description: The duration of the message in seconds.
        isFiltered:
          type: boolean
          description: Indicates if the message was filtered for security reasons.
        detectedThreats:
          type: array
          items:
            type: string
          description: List of detected security threats if the message was filtered.
        originalMessage:
          type: string
          description: >-
            The original message before filtering (only included if content was
            filtered).
        metadata:
          $ref: '#/components/schemas/UserMessageMetadata'
          description: >-
            The metadata associated with the message. Currently used to store
            the transcriber's word level confidence.
        speakerLabel:
          type: string
          description: Stable speaker label for diarized user speakers (e.g., "Speaker 1").
      required:
        - role
        - message
        - time
        - endTime
        - secondsFromStart
    AssistantMessageRole:
      type: string
      enum:
        - value: assistant
      default: assistant
    ToolCallFunction:
      type: object
      properties:
        arguments:
          type: string
          description: This is the arguments to call the function with
        name:
          type: string
          description: This is the name of the function to call
      required:
        - arguments
        - name
    ToolCall:
      type: object
      properties:
        id:
          type: string
          description: This is the ID of the tool call
        type:
          type: string
          description: This is the type of tool
        function:
          $ref: '#/components/schemas/ToolCallFunction'
          description: This is the function that was called
      required:
        - id
        - type
        - function
    AssistantMessageMetadata:
      type: object
      properties: {}
    AssistantMessage:
      type: object
      properties:
        role:
          $ref: '#/components/schemas/AssistantMessageRole'
          description: This is the role of the message author
        content:
          type: string
          description: This is the content of the assistant message
        refusal:
          type: string
          description: This is the refusal message generated by the model
        tool_calls:
          type: array
          items:
            $ref: '#/components/schemas/ToolCall'
          description: This is the tool calls generated by the model
        name:
          type: string
          description: This is an optional name for the participant
        metadata:
          $ref: '#/components/schemas/AssistantMessageMetadata'
          description: This is an optional metadata for the message
      required:
        - role
    ToolMessageRole:
      type: string
      enum:
        - value: tool
      default: tool
    ToolMessageMetadata:
      type: object
      properties: {}
    ToolMessage:
      type: object
      properties:
        role:
          $ref: '#/components/schemas/ToolMessageRole'
          description: This is the role of the message author
        content:
          type: string
          description: This is the content of the tool message
        tool_call_id:
          type: string
          description: This is the ID of the tool call this message is responding to
        name:
          type: string
          description: This is an optional name for the participant
        metadata:
          $ref: '#/components/schemas/ToolMessageMetadata'
          description: This is an optional metadata for the message
      required:
        - role
        - content
        - tool_call_id
    DeveloperMessageRole:
      type: string
      enum:
        - value: developer
      default: developer
    DeveloperMessageMetadata:
      type: object
      properties: {}
    DeveloperMessage:
      type: object
      properties:
        role:
          $ref: '#/components/schemas/DeveloperMessageRole'
          description: This is the role of the message author
        content:
          type: string
          description: This is the content of the developer message
        name:
          type: string
          description: This is an optional name for the participant
        metadata:
          $ref: '#/components/schemas/DeveloperMessageMetadata'
          description: This is an optional metadata for the message
      required:
        - role
        - content
    SayHookActionPromptOneOf1Items:
      oneOf:
        - $ref: '#/components/schemas/SystemMessage'
        - $ref: '#/components/schemas/UserMessage'
        - $ref: '#/components/schemas/AssistantMessage'
        - $ref: '#/components/schemas/ToolMessage'
        - $ref: '#/components/schemas/DeveloperMessage'
    SayHookActionPrompt1:
      type: array
      items:
        $ref: '#/components/schemas/SayHookActionPromptOneOf1Items'
    SayHookActionPrompt:
      oneOf:
        - type: string
        - $ref: '#/components/schemas/SayHookActionPrompt1'
    SayHookActionExact:
      type: object
      properties: {}
    SayHookAction:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/SayHookActionType'
          description: This is the type of action - must be "say"
        prompt:
          $ref: '#/components/schemas/SayHookActionPrompt'
          description: >-
            This is the prompt for the assistant to generate a response based on
            existing conversation.

            Can be a string or an array of chat messages.
        exact:
          $ref: '#/components/schemas/SayHookActionExact'
          description: This is the message to say
      required:
        - type
    CallHookAssistantSpeechInterruptedDoItems:
      oneOf:
        - $ref: '#/components/schemas/SayHookAction'
        - $ref: '#/components/schemas/ToolCallHookAction'
        - $ref: '#/components/schemas/MessageAddHookAction'
    CallHookAssistantSpeechInterrupted:
      type: object
      properties:
        'on':
          $ref: '#/components/schemas/CallHookAssistantSpeechInterruptedOn'
          description: This is the event that triggers this hook
        do:
          type: array
          items:
            $ref: '#/components/schemas/CallHookAssistantSpeechInterruptedDoItems'
          description: This is the set of actions to perform when the hook triggers
      required:
        - 'on'
        - do
    CallHookCustomerSpeechInterruptedOn:
      type: string
      enum:
        - value: customer.speech.interrupted
    CallHookCustomerSpeechInterruptedDoItems:
      oneOf:
        - $ref: '#/components/schemas/SayHookAction'
        - $ref: '#/components/schemas/ToolCallHookAction'
        - $ref: '#/components/schemas/MessageAddHookAction'
    CallHookCustomerSpeechInterrupted:
      type: object
      properties:
        'on':
          $ref: '#/components/schemas/CallHookCustomerSpeechInterruptedOn'
          description: This is the event that triggers this hook
        do:
          type: array
          items:
            $ref: '#/components/schemas/CallHookCustomerSpeechInterruptedDoItems'
          description: This is the set of actions to perform when the hook triggers
      required:
        - 'on'
        - do
    CallHookCustomerSpeechTimeoutDoItems:
      oneOf:
        - $ref: '#/components/schemas/SayHookAction'
        - $ref: '#/components/schemas/ToolCallHookAction'
        - $ref: '#/components/schemas/MessageAddHookAction'
    CustomerSpeechTimeoutOptionsTriggerResetMode:
      type: object
      properties: {}
    CustomerSpeechTimeoutOptions:
      type: object
      properties:
        timeoutSeconds:
          type: number
          format: double
          description: >-
            This is the timeout in seconds before action is triggered.

            The clock starts when the assistant finishes speaking and remains
            active until the user speaks.


            @default 7.5
        triggerMaxCount:
          type: number
          format: double
          description: |-
            This is the maximum number of times the hook will trigger in a call.

            @default 3
        triggerResetMode:
          $ref: '#/components/schemas/CustomerSpeechTimeoutOptionsTriggerResetMode'
          description: |-
            This is whether the counter for hook trigger resets the user speaks.

            @default never
      required:
        - timeoutSeconds
    CallHookCustomerSpeechTimeout:
      type: object
      properties:
        'on':
          type: string
          description: >-
            Must be either "customer.speech.timeout" or match the pattern
            "customer.speech.timeout[property=value]"
        do:
          type: array
          items:
            $ref: '#/components/schemas/CallHookCustomerSpeechTimeoutDoItems'
          description: This is the set of actions to perform when the hook triggers
        options:
          $ref: '#/components/schemas/CustomerSpeechTimeoutOptions'
          description: This is the set of filters that must match for the hook to trigger
        name:
          type: string
          description: >-
            This is the name of the hook, it can be set by the user to identify
            the hook.

            If no name is provided, the hook will be auto generated as UUID.


            @default UUID
      required:
        - 'on'
        - do
    AssistantOverridesHooksItems:
      oneOf:
        - $ref: '#/components/schemas/CallHookCallEnding'
        - $ref: '#/components/schemas/CallHookAssistantSpeechInterrupted'
        - $ref: '#/components/schemas/CallHookCustomerSpeechInterrupted'
        - $ref: '#/components/schemas/CallHookCustomerSpeechTimeout'
    AssistantOverridesToolsAppendItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    AssistantOverridesVariableValues:
      type: object
      properties: {}
    SecurityFilterBase:
      type: object
      properties: {}
    SecurityFilterPlanMode:
      type: string
      enum:
        - value: sanitize
        - value: reject
        - value: replace
      default: sanitize
    SecurityFilterPlan:
      type: object
      properties:
        enabled:
          type: boolean
          default: false
          description: |-
            Whether the security filter is enabled.
            @default false
        filters:
          type: array
          items:
            $ref: '#/components/schemas/SecurityFilterBase'
          description: |-
            Array of security filter types to apply.
            If array is not empty, only those security filters are run.
        mode:
          $ref: '#/components/schemas/SecurityFilterPlanMode'
          description: |-
            Mode of operation when a security threat is detected.
            - 'sanitize': Remove or replace the threatening content
            - 'reject': Replace the entire transcript with replacement text
            - 'replace': Replace threatening patterns with replacement text
            @default 'sanitize'
        replacementText:
          type: string
          default: '[FILTERED]'
          description: |-
            Text to use when replacing filtered content.
            @default '[FILTERED]'
    CompliancePlanRecordingConsentPlanDiscriminatorMappingStayOnLineVoice:
      oneOf:
        - $ref: '#/components/schemas/AzureVoice'
        - $ref: '#/components/schemas/CartesiaVoice'
        - $ref: '#/components/schemas/CustomVoice'
        - $ref: '#/components/schemas/DeepgramVoice'
        - $ref: '#/components/schemas/ElevenLabsVoice'
        - $ref: '#/components/schemas/HumeVoice'
        - $ref: '#/components/schemas/LMNTVoice'
        - $ref: '#/components/schemas/NeuphonicVoice'
        - $ref: '#/components/schemas/OpenAIVoice'
        - $ref: '#/components/schemas/PlayHTVoice'
        - $ref: '#/components/schemas/WellSaidVoice'
        - $ref: '#/components/schemas/RimeAIVoice'
        - $ref: '#/components/schemas/SmallestAIVoice'
        - $ref: '#/components/schemas/TavusVoice'
        - $ref: '#/components/schemas/VapiVoice'
        - $ref: '#/components/schemas/SesameVoice'
        - $ref: '#/components/schemas/InworldVoice'
        - $ref: '#/components/schemas/MinimaxVoice'
    CompliancePlanRecordingConsentPlanDiscriminatorMappingVerbalVoice:
      oneOf:
        - $ref: '#/components/schemas/AzureVoice'
        - $ref: '#/components/schemas/CartesiaVoice'
        - $ref: '#/components/schemas/CustomVoice'
        - $ref: '#/components/schemas/DeepgramVoice'
        - $ref: '#/components/schemas/ElevenLabsVoice'
        - $ref: '#/components/schemas/HumeVoice'
        - $ref: '#/components/schemas/LMNTVoice'
        - $ref: '#/components/schemas/NeuphonicVoice'
        - $ref: '#/components/schemas/OpenAIVoice'
        - $ref: '#/components/schemas/PlayHTVoice'
        - $ref: '#/components/schemas/WellSaidVoice'
        - $ref: '#/components/schemas/RimeAIVoice'
        - $ref: '#/components/schemas/SmallestAIVoice'
        - $ref: '#/components/schemas/TavusVoice'
        - $ref: '#/components/schemas/VapiVoice'
        - $ref: '#/components/schemas/SesameVoice'
        - $ref: '#/components/schemas/InworldVoice'
        - $ref: '#/components/schemas/MinimaxVoice'
    CompliancePlanRecordingConsentPlanDiscriminatorMappingVerbalDeclineTool:
      type: object
      properties: {}
    CompliancePlanRecordingConsentPlan:
      oneOf:
        - type: object
          properties:
            type:
              type: string
              enum:
                - stay-on-line
              description: 'Discriminator value: stay-on-line'
            message:
              type: string
              description: >-
                This is the message asking for consent to record the call.

                If the type is `stay-on-line`, the message should ask the user
                to hang up if they do not consent.

                If the type is `verbal`, the message should ask the user to
                verbally consent or decline.
            voice:
              $ref: >-
                #/components/schemas/CompliancePlanRecordingConsentPlanDiscriminatorMappingStayOnLineVoice
              description: >-
                This is the voice to use for the consent message. If not
                specified, inherits from the assistant's voice.

                Use a different voice for the consent message for a better user
                experience.
            waitSeconds:
              type: number
              format: double
              default: 3
              description: >-
                Number of seconds to wait before transferring to the assistant
                if user stays on the call
          required:
            - type
            - message
          description: stay-on-line variant
        - type: object
          properties:
            type:
              type: string
              enum:
                - verbal
              description: 'Discriminator value: verbal'
            message:
              type: string
              description: >-
                This is the message asking for consent to record the call.

                If the type is `stay-on-line`, the message should ask the user
                to hang up if they do not consent.

                If the type is `verbal`, the message should ask the user to
                verbally consent or decline.
            voice:
              $ref: >-
                #/components/schemas/CompliancePlanRecordingConsentPlanDiscriminatorMappingVerbalVoice
              description: >-
                This is the voice to use for the consent message. If not
                specified, inherits from the assistant's voice.

                Use a different voice for the consent message for a better user
                experience.
            declineTool:
              $ref: >-
                #/components/schemas/CompliancePlanRecordingConsentPlanDiscriminatorMappingVerbalDeclineTool
              description: Tool to execute if user verbally declines recording consent
            declineToolId:
              type: string
              description: >-
                ID of existing tool to execute if user verbally declines
                recording consent
          required:
            - type
            - message
          description: verbal variant
      discriminator:
        propertyName: type
    CompliancePlan:
      type: object
      properties:
        hipaaEnabled:
          type: boolean
          description: >-
            When this is enabled, no logs, recordings, or transcriptions will be
            stored.

            At the end of the call, you will still receive an end-of-call-report
            message to store on your server. Defaults to false.
        pciEnabled:
          type: boolean
          description: >-
            When this is enabled, the user will be restricted to use
            PCI-compliant providers, and no logs or transcripts are stored.

            At the end of the call, you will receive an end-of-call-report
            message to store on your server. Defaults to false.
        securityFilterPlan:
          $ref: '#/components/schemas/SecurityFilterPlan'
          description: >-
            This is the security filter plan for the assistant. It allows
            filtering of transcripts for security threats before sending to LLM.
        recordingConsentPlan:
          $ref: '#/components/schemas/CompliancePlanRecordingConsentPlan'
    AssistantOverridesMetadata:
      type: object
      properties: {}
    SmartDenoisingPlan:
      type: object
      properties:
        enabled:
          type: boolean
          default: false
          description: Whether smart denoising using Krisp is enabled.
    FourierDenoisingPlan:
      type: object
      properties:
        enabled:
          type: boolean
          default: false
          description: >-
            Whether Fourier denoising is enabled. Note that this is experimental
            and may not work as expected.
        mediaDetectionEnabled:
          type: boolean
          default: true
          description: >-
            Whether automatic media detection is enabled. When enabled, the
            filter will automatically

            detect consistent background TV/music/radio and switch to more
            aggressive filtering settings.

            Only applies when enabled is true.
        staticThreshold:
          type: number
          format: double
          default: -35
          description: >-
            Static threshold in dB used as fallback when no baseline is
            established.
        baselineOffsetDb:
          type: number
          format: double
          default: -15
          description: >-
            How far below the rolling baseline to filter audio, in dB.

            Lower values (e.g., -10) are more aggressive, higher values (e.g.,
            -20) are more conservative.
        windowSizeMs:
          type: number
          format: double
          default: 3000
          description: >-
            Rolling window size in milliseconds for calculating the audio
            baseline.

            Larger windows adapt more slowly but are more stable.
        baselinePercentile:
          type: number
          format: double
          default: 85
          description: >-
            Percentile to use for baseline calculation (1-99).

            Higher percentiles (e.g., 85) focus on louder speech, lower
            percentiles (e.g., 50) include quieter speech.
    BackgroundSpeechDenoisingPlan:
      type: object
      properties:
        smartDenoisingPlan:
          $ref: '#/components/schemas/SmartDenoisingPlan'
          description: Whether smart denoising using Krisp is enabled.
        fourierDenoisingPlan:
          $ref: '#/components/schemas/FourierDenoisingPlan'
          description: >-
            Whether Fourier denoising is enabled. Note that this is experimental
            and may not work as expected.


            This can be combined with smart denoising, and will be run
            afterwards.
    StructuredDataPlanMessagesItems:
      type: object
      properties: {}
    StructuredDataPlan:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/StructuredDataPlanMessagesItems'
          description: >-
            These are the messages used to generate the structured data.


            @default: ```

            [
              {
                "role": "system",
                "content": "You are an expert data extractor. You will be given a transcript of a call. Extract structured data per the JSON Schema. DO NOT return anything except the structured data.\n\nJson Schema:\\n{{schema}}\n\nOnly respond with the JSON."
              },
              {
                "role": "user",
                "content": "Here is the transcript:\n\n{{transcript}}\n\n. Here is the ended reason of the call:\n\n{{endedReason}}\n\n"
              }
            ]```


            You can customize by providing any messages you want.


            Here are the template variables available:

            - {{transcript}}: the transcript of the call from
            `call.artifact.transcript`- {{systemPrompt}}: the system prompt of
            the call from `assistant.model.messages[type=system].content`-
            {{messages}}: the messages of the call from
            `assistant.model.messages`- {{schema}}: the schema of the structured
            data from `structuredDataPlan.schema`- {{endedReason}}: the ended
            reason of the call from `call.endedReason`
        enabled:
          type: boolean
          description: >-
            This determines whether structured data is generated and stored in
            `call.analysis.structuredData`. Defaults to false.


            Usage:

            - If you want to extract structured data, set this to true and
            provide a `schema`.


            @default false
        schema:
          $ref: '#/components/schemas/JsonSchema'
          description: >-
            This is the schema of the structured data. The output is stored in
            `call.analysis.structuredData`.


            Complete guide on JSON Schema can be found
            [here](https://ajv.js.org/json-schema.html#json-data-type).
        timeoutSeconds:
          type: number
          format: double
          description: >-
            This is how long the request is tried before giving up. When request
            times out, `call.analysis.structuredData` will be empty.


            Usage:

            - To guarantee the structured data is generated, set this value
            high. Note, this will delay the end of call report in cases where
            model is slow to respond.


            @default 5 seconds
    StructuredDataMultiPlan:
      type: object
      properties:
        key:
          type: string
          description: This is the key of the structured data plan in the catalog.
        plan:
          $ref: '#/components/schemas/StructuredDataPlan'
          description: This is an individual structured data plan in the catalog.
      required:
        - key
        - plan
    SuccessEvaluationPlanRubric:
      type: string
      enum:
        - value: NumericScale
        - value: DescriptiveScale
        - value: Checklist
        - value: Matrix
        - value: PercentageScale
        - value: LikertScale
        - value: AutomaticRubric
        - value: PassFail
    SuccessEvaluationPlanMessagesItems:
      type: object
      properties: {}
    SuccessEvaluationPlan:
      type: object
      properties:
        rubric:
          $ref: '#/components/schemas/SuccessEvaluationPlanRubric'
          description: >-
            This enforces the rubric of the evaluation. The output is stored in
            `call.analysis.successEvaluation`.


            Options include:

            - 'NumericScale': A scale of 1 to 10.

            - 'DescriptiveScale': A scale of Excellent, Good, Fair, Poor.

            - 'Checklist': A checklist of criteria and their status.

            - 'Matrix': A grid that evaluates multiple criteria across different
            performance levels.

            - 'PercentageScale': A scale of 0% to 100%.

            - 'LikertScale': A scale of Strongly Agree, Agree, Neutral,
            Disagree, Strongly Disagree.

            - 'AutomaticRubric': Automatically break down evaluation into
            several criteria, each with its own score.

            - 'PassFail': A simple 'true' if call passed, 'false' if not.


            Default is 'PassFail'.
        messages:
          type: array
          items:
            $ref: '#/components/schemas/SuccessEvaluationPlanMessagesItems'
          description: >-
            These are the messages used to generate the success evaluation.


            @default: ```

            [
              {
                "role": "system",
                "content": "You are an expert call evaluator. You will be given a transcript of a call and the system prompt of the AI participant. Determine if the call was successful based on the objectives inferred from the system prompt. DO NOT return anything except the result.\n\nRubric:\\n{{rubric}}\n\nOnly respond with the result."
              },
              {
                "role": "user",
                "content": "Here is the transcript:\n\n{{transcript}}\n\n"
              },
              {
                "role": "user",
                "content": "Here was the system prompt of the call:\n\n{{systemPrompt}}\n\n. Here is the ended reason of the call:\n\n{{endedReason}}\n\n"
              }
            ]```


            You can customize by providing any messages you want.


            Here are the template variables available:

            - {{transcript}}: the transcript of the call from
            `call.artifact.transcript`- {{systemPrompt}}: the system prompt of
            the call from `assistant.model.messages[type=system].content`-
            {{messages}}: the messages of the call from
            `assistant.model.messages`- {{rubric}}: the rubric of the success
            evaluation from `successEvaluationPlan.rubric`- {{endedReason}}: the
            ended reason of the call from `call.endedReason`
        enabled:
          type: boolean
          description: >-
            This determines whether a success evaluation is generated and stored
            in `call.analysis.successEvaluation`. Defaults to true.


            Usage:

            - If you want to disable the success evaluation, set this to false.


            @default true
        timeoutSeconds:
          type: number
          format: double
          description: >-
            This is how long the request is tried before giving up. When request
            times out, `call.analysis.successEvaluation` will be empty.


            Usage:

            - To guarantee the success evaluation is generated, set this value
            high. Note, this will delay the end of call report in cases where
            model is slow to respond.


            @default 5 seconds
    AnalysisPlan:
      type: object
      properties:
        minMessagesThreshold:
          type: number
          format: double
          description: >-
            The minimum number of messages required to run the analysis plan.

            If the number of messages is less than this, analysis will be
            skipped.

            @default 2
        summaryPlan:
          $ref: '#/components/schemas/SummaryPlan'
          description: >-
            This is the plan for generating the summary of the call. This
            outputs to `call.analysis.summary`.
        structuredDataPlan:
          $ref: '#/components/schemas/StructuredDataPlan'
          description: >-
            This is the plan for generating the structured data from the call.
            This outputs to `call.analysis.structuredData`.
        structuredDataMultiPlan:
          type: array
          items:
            $ref: '#/components/schemas/StructuredDataMultiPlan'
          description: >-
            This is an array of structured data plan catalogs. Each entry
            includes a `key` and a `plan` for generating the structured data
            from the call. This outputs to `call.analysis.structuredDataMulti`.
        successEvaluationPlan:
          $ref: '#/components/schemas/SuccessEvaluationPlan'
          description: >-
            This is the plan for generating the success evaluation of the call.
            This outputs to `call.analysis.successEvaluation`.
        outcomeIds:
          type: array
          items:
            type: string
          description: >-
            This is an array of outcome UUIDs to be calculated during analysis.

            The outcomes will be calculated and stored in
            `call.analysis.outcomes`.
    ArtifactPlanRecordingFormat:
      type: string
      enum:
        - value: wav;l16
        - value: mp3
    TranscriptPlan:
      type: object
      properties:
        enabled:
          type: boolean
          description: >-
            This determines whether the transcript is stored in
            `call.artifact.transcript`. Defaults to true.


            @default true
        assistantName:
          type: string
          description: >-
            This is the name of the assistant in the transcript. Defaults to
            'AI'.


            Usage:

            - If you want to change the name of the assistant in the transcript,
            set this. Example, here is what the transcript would look like with
            `assistantName` set to 'Buyer':

            ```

            User: Hello, how are you?

            Buyer: I'm fine.

            User: Do you want to buy a car?

            Buyer: No.

            ```


            @default 'AI'
        userName:
          type: string
          description: >-
            This is the name of the user in the transcript. Defaults to 'User'.


            Usage:

            - If you want to change the name of the user in the transcript, set
            this. Example, here is what the transcript would look like with
            `userName` set to 'Seller':

            ```

            Seller: Hello, how are you?

            AI: I'm fine.

            Seller: Do you want to buy a car?

            AI: No.

            ```


            @default 'User'
    WorkflowOpenAiModelProvider:
      type: string
      enum:
        - value: openai
    WorkflowOpenAiModelModel:
      type: string
      enum:
        - value: gpt-5.2
        - value: gpt-5.2-chat-latest
        - value: gpt-5.1
        - value: gpt-5.1-chat-latest
        - value: gpt-5
        - value: gpt-5-chat-latest
        - value: gpt-5-mini
        - value: gpt-5-nano
        - value: gpt-4.1-2025-04-14
        - value: gpt-4.1-mini-2025-04-14
        - value: gpt-4.1-nano-2025-04-14
        - value: gpt-4.1
        - value: gpt-4.1-mini
        - value: gpt-4.1-nano
        - value: chatgpt-4o-latest
        - value: o3
        - value: o3-mini
        - value: o4-mini
        - value: o1-mini
        - value: o1-mini-2024-09-12
        - value: gpt-4o-mini-2024-07-18
        - value: gpt-4o-mini
        - value: gpt-4o
        - value: gpt-4o-2024-05-13
        - value: gpt-4o-2024-08-06
        - value: gpt-4o-2024-11-20
        - value: gpt-4-turbo
        - value: gpt-4-turbo-2024-04-09
        - value: gpt-4-turbo-preview
        - value: gpt-4-0125-preview
        - value: gpt-4-1106-preview
        - value: gpt-4
        - value: gpt-4-0613
        - value: gpt-3.5-turbo
        - value: gpt-3.5-turbo-0125
        - value: gpt-3.5-turbo-1106
        - value: gpt-3.5-turbo-16k
        - value: gpt-3.5-turbo-0613
        - value: gpt-4.1-2025-04-14:westus
        - value: gpt-4.1-2025-04-14:eastus2
        - value: gpt-4.1-2025-04-14:eastus
        - value: gpt-4.1-2025-04-14:westus3
        - value: gpt-4.1-2025-04-14:northcentralus
        - value: gpt-4.1-2025-04-14:southcentralus
        - value: gpt-4.1-2025-04-14:westeurope
        - value: gpt-4.1-2025-04-14:germanywestcentral
        - value: gpt-4.1-2025-04-14:polandcentral
        - value: gpt-4.1-2025-04-14:spaincentral
        - value: gpt-4.1-mini-2025-04-14:westus
        - value: gpt-4.1-mini-2025-04-14:eastus2
        - value: gpt-4.1-mini-2025-04-14:eastus
        - value: gpt-4.1-mini-2025-04-14:westus3
        - value: gpt-4.1-mini-2025-04-14:northcentralus
        - value: gpt-4.1-mini-2025-04-14:southcentralus
        - value: gpt-4.1-mini-2025-04-14:westeurope
        - value: gpt-4.1-mini-2025-04-14:germanywestcentral
        - value: gpt-4.1-mini-2025-04-14:polandcentral
        - value: gpt-4.1-mini-2025-04-14:spaincentral
        - value: gpt-4.1-nano-2025-04-14:westus
        - value: gpt-4.1-nano-2025-04-14:eastus2
        - value: gpt-4.1-nano-2025-04-14:westus3
        - value: gpt-4.1-nano-2025-04-14:northcentralus
        - value: gpt-4.1-nano-2025-04-14:southcentralus
        - value: gpt-4o-2024-11-20:swedencentral
        - value: gpt-4o-2024-11-20:westus
        - value: gpt-4o-2024-11-20:eastus2
        - value: gpt-4o-2024-11-20:eastus
        - value: gpt-4o-2024-11-20:westus3
        - value: gpt-4o-2024-11-20:southcentralus
        - value: gpt-4o-2024-11-20:westeurope
        - value: gpt-4o-2024-11-20:germanywestcentral
        - value: gpt-4o-2024-11-20:polandcentral
        - value: gpt-4o-2024-11-20:spaincentral
        - value: gpt-4o-2024-08-06:westus
        - value: gpt-4o-2024-08-06:westus3
        - value: gpt-4o-2024-08-06:eastus
        - value: gpt-4o-2024-08-06:eastus2
        - value: gpt-4o-2024-08-06:northcentralus
        - value: gpt-4o-2024-08-06:southcentralus
        - value: gpt-4o-mini-2024-07-18:westus
        - value: gpt-4o-mini-2024-07-18:westus3
        - value: gpt-4o-mini-2024-07-18:eastus
        - value: gpt-4o-mini-2024-07-18:eastus2
        - value: gpt-4o-mini-2024-07-18:northcentralus
        - value: gpt-4o-mini-2024-07-18:southcentralus
        - value: gpt-4o-2024-05-13:eastus2
        - value: gpt-4o-2024-05-13:eastus
        - value: gpt-4o-2024-05-13:northcentralus
        - value: gpt-4o-2024-05-13:southcentralus
        - value: gpt-4o-2024-05-13:westus3
        - value: gpt-4o-2024-05-13:westus
        - value: gpt-4-turbo-2024-04-09:eastus2
        - value: gpt-4-0125-preview:eastus
        - value: gpt-4-0125-preview:northcentralus
        - value: gpt-4-0125-preview:southcentralus
        - value: gpt-4-1106-preview:australia
        - value: gpt-4-1106-preview:canadaeast
        - value: gpt-4-1106-preview:france
        - value: gpt-4-1106-preview:india
        - value: gpt-4-1106-preview:norway
        - value: gpt-4-1106-preview:swedencentral
        - value: gpt-4-1106-preview:uk
        - value: gpt-4-1106-preview:westus
        - value: gpt-4-1106-preview:westus3
        - value: gpt-4-0613:canadaeast
        - value: gpt-3.5-turbo-0125:canadaeast
        - value: gpt-3.5-turbo-0125:northcentralus
        - value: gpt-3.5-turbo-0125:southcentralus
        - value: gpt-3.5-turbo-1106:canadaeast
        - value: gpt-3.5-turbo-1106:westus
    WorkflowOpenAIModel:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/WorkflowOpenAiModelProvider'
          description: This is the provider of the model (`openai`).
        model:
          $ref: '#/components/schemas/WorkflowOpenAiModelModel'
          description: >-
            This is the OpenAI model that will be used.


            When using Vapi OpenAI or your own Azure Credentials, you have the
            option to specify the region for the selected model. This shouldn't
            be specified unless you have a specific reason to do so. Vapi will
            automatically find the fastest region that make sense.

            This is helpful when you are required to comply with Data Residency
            rules. Learn more about Azure regions here
            https://azure.microsoft.com/en-us/explore/global-infrastructure/data-residency/.
        temperature:
          type: number
          format: double
          description: This is the temperature of the model.
        maxTokens:
          type: number
          format: double
          description: This is the max tokens of the model.
      required:
        - provider
        - model
    WorkflowAnthropicModelProvider:
      type: string
      enum:
        - value: anthropic
    WorkflowAnthropicModelModel:
      type: string
      enum:
        - value: claude-3-opus-20240229
        - value: claude-3-sonnet-20240229
        - value: claude-3-haiku-20240307
        - value: claude-3-5-sonnet-20240620
        - value: claude-3-5-sonnet-20241022
        - value: claude-3-5-haiku-20241022
        - value: claude-3-7-sonnet-20250219
        - value: claude-opus-4-20250514
        - value: claude-opus-4-5-20251101
        - value: claude-opus-4-6
        - value: claude-sonnet-4-20250514
        - value: claude-sonnet-4-5-20250929
        - value: claude-haiku-4-5-20251001
    WorkflowAnthropicModel:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/WorkflowAnthropicModelProvider'
          description: This is the provider of the model (`anthropic`).
        model:
          $ref: '#/components/schemas/WorkflowAnthropicModelModel'
          description: This is the specific model that will be used.
        thinking:
          $ref: '#/components/schemas/AnthropicThinkingConfig'
          description: >-
            This is the optional configuration for Anthropic's thinking feature.


            - If provided, `maxTokens` must be greater than
            `thinking.budgetTokens`.
        temperature:
          type: number
          format: double
          description: This is the temperature of the model.
        maxTokens:
          type: number
          format: double
          description: This is the max tokens of the model.
      required:
        - provider
        - model
    WorkflowAnthropicBedrockModelProvider:
      type: string
      enum:
        - value: anthropic-bedrock
    WorkflowAnthropicBedrockModelModel:
      type: string
      enum:
        - value: claude-3-opus-20240229
        - value: claude-3-sonnet-20240229
        - value: claude-3-haiku-20240307
        - value: claude-3-5-sonnet-20240620
        - value: claude-3-5-sonnet-20241022
        - value: claude-3-5-haiku-20241022
        - value: claude-3-7-sonnet-20250219
        - value: claude-opus-4-20250514
        - value: claude-opus-4-5-20251101
        - value: claude-opus-4-6
        - value: claude-sonnet-4-20250514
        - value: claude-sonnet-4-5-20250929
        - value: claude-haiku-4-5-20251001
    WorkflowAnthropicBedrockModel:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/WorkflowAnthropicBedrockModelProvider'
          description: This is the provider of the model (`anthropic-bedrock`).
        model:
          $ref: '#/components/schemas/WorkflowAnthropicBedrockModelModel'
          description: This is the specific model that will be used.
        thinking:
          $ref: '#/components/schemas/AnthropicThinkingConfig'
          description: >-
            This is the optional configuration for Anthropic's thinking feature.


            - If provided, `maxTokens` must be greater than
            `thinking.budgetTokens`.
        temperature:
          type: number
          format: double
          description: This is the temperature of the model.
        maxTokens:
          type: number
          format: double
          description: This is the max tokens of the model.
      required:
        - provider
        - model
    WorkflowGoogleModelProvider:
      type: string
      enum:
        - value: google
    WorkflowGoogleModelModel:
      type: string
      enum:
        - value: gemini-3-flash-preview
        - value: gemini-2.5-pro
        - value: gemini-2.5-flash
        - value: gemini-2.5-flash-lite
        - value: gemini-2.0-flash-thinking-exp
        - value: gemini-2.0-pro-exp-02-05
        - value: gemini-2.0-flash
        - value: gemini-2.0-flash-lite
        - value: gemini-2.0-flash-exp
        - value: gemini-2.0-flash-realtime-exp
        - value: gemini-1.5-flash
        - value: gemini-1.5-flash-002
        - value: gemini-1.5-pro
        - value: gemini-1.5-pro-002
        - value: gemini-1.0-pro
    WorkflowGoogleModel:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/WorkflowGoogleModelProvider'
          description: This is the provider of the model (`google`).
        model:
          $ref: '#/components/schemas/WorkflowGoogleModelModel'
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        temperature:
          type: number
          format: double
          description: This is the temperature of the model.
        maxTokens:
          type: number
          format: double
          description: This is the max tokens of the model.
      required:
        - provider
        - model
    WorkflowCustomModelProvider:
      type: string
      enum:
        - value: custom-llm
    WorkflowCustomModelMetadataSendMode:
      type: string
      enum:
        - value: 'off'
        - value: variable
        - value: destructured
    WorkflowCustomModelHeaders:
      type: object
      properties: {}
    WorkflowCustomModel:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/WorkflowCustomModelProvider'
          description: This is the provider of the model (`custom-llm`).
        metadataSendMode:
          $ref: '#/components/schemas/WorkflowCustomModelMetadataSendMode'
          description: >-
            This determines whether metadata is sent in requests to the custom
            provider.


            - `off` will not send any metadata. payload will look like `{
            messages }`

            - `variable` will send `assistant.metadata` as a variable on the
            payload. payload will look like `{ messages, metadata }`

            - `destructured` will send `assistant.metadata` fields directly on
            the payload. payload will look like `{ messages, ...metadata }`


            Further, `variable` and `destructured` will send `call`,
            `phoneNumber`, and `customer` objects in the payload.


            Default is `variable`.
        url:
          type: string
          description: >-
            These is the URL we'll use for the OpenAI client's `baseURL`. Ex.
            https://openrouter.ai/api/v1
        headers:
          $ref: '#/components/schemas/WorkflowCustomModelHeaders'
          description: These are the headers we'll use for the OpenAI client's `headers`.
        timeoutSeconds:
          type: number
          format: double
          description: >-
            This sets the timeout for the connection to the custom provider
            without needing to stream any tokens back. Default is 20 seconds.
        model:
          type: string
          description: >-
            This is the name of the model. Ex.
            cognitivecomputations/dolphin-mixtral-8x7b
        temperature:
          type: number
          format: double
          description: This is the temperature of the model.
        maxTokens:
          type: number
          format: double
          description: This is the max tokens of the model.
      required:
        - provider
        - url
        - model
    CreateStructuredOutputDtoModel:
      oneOf:
        - $ref: '#/components/schemas/WorkflowOpenAIModel'
        - $ref: '#/components/schemas/WorkflowAnthropicModel'
        - $ref: '#/components/schemas/WorkflowAnthropicBedrockModel'
        - $ref: '#/components/schemas/WorkflowGoogleModel'
        - $ref: '#/components/schemas/WorkflowCustomModel'
    ComplianceOverride:
      type: object
      properties:
        forceStoreOnHipaaEnabled:
          type: boolean
          description: >-
            Force storage for this output under HIPAA. Only enable if output
            contains no sensitive data.
    CreateStructuredOutputDTO:
      type: object
      properties:
        model:
          $ref: '#/components/schemas/CreateStructuredOutputDtoModel'
          description: >-
            This is the model that will be used to extract the structured
            output.


            To provide your own custom system and user prompts for structured
            output extraction, populate the messages array with your system and
            user messages. You can specify liquid templating in your system and
            user messages.

            Between the system or user messages, you must reference either
            'transcript' or 'messages' with the '{{}}' syntax to access the
            conversation history.

            Between the system or user messages, you must reference a variation
            of the structured output with the '{{}}' syntax to access the
            structured output definition.

            i.e.:

            {{structuredOutput}}

            {{structuredOutput.name}}

            {{structuredOutput.description}}

            {{structuredOutput.schema}}


            If model is not specified, GPT-4.1 will be used by default for
            extraction, utilizing default system and user prompts.

            If messages or required fields are not specified, the default system
            and user prompts will be used.
        compliancePlan:
          $ref: '#/components/schemas/ComplianceOverride'
          description: >-
            Compliance configuration for this output. Only enable overrides if
            no sensitive data will be stored.
        name:
          type: string
          description: This is the name of the structured output.
        schema:
          $ref: '#/components/schemas/JsonSchema'
          description: >-
            This is the JSON Schema definition for the structured output.


            This is required when creating a structured output. Defines the
            structure and validation rules for the data that will be extracted.
            Supports all JSON Schema features including:

            - Objects and nested properties

            - Arrays and array validation

            - String, number, boolean, and null types

            - Enums and const values

            - Validation constraints (min/max, patterns, etc.)

            - Composition with allOf, anyOf, oneOf
        description:
          type: string
          description: >-
            This is the description of what the structured output extracts.


            Use this to provide context about what data will be extracted and
            how it will be used.
        assistantIds:
          type: array
          items:
            type: string
          description: >-
            These are the assistant IDs that this structured output is linked
            to.


            When linked to assistants, this structured output will be available
            for extraction during those assistant's calls.
        workflowIds:
          type: array
          items:
            type: string
          description: >-
            These are the workflow IDs that this structured output is linked to.


            When linked to workflows, this structured output will be available
            for extraction during those workflow's execution.
      required:
        - name
        - schema
    ScorecardMetricConditionsItems:
      type: object
      properties: {}
    ScorecardMetric:
      type: object
      properties:
        structuredOutputId:
          type: string
          description: >-
            This is the unique identifier for the structured output that will be
            used to evaluate the scorecard.

            The structured output must be of type number or boolean only for
            now.
        conditions:
          type: array
          items:
            $ref: '#/components/schemas/ScorecardMetricConditionsItems'
          description: >-
            These are the conditions that will be used to evaluate the
            scorecard.

            Each condition will have a comparator, value, and points that will
            be used to calculate the final score.

            The points will be added to the overall score if the condition is
            met.

            The overall score will be normalized to a 100 point scale to ensure
            uniformity across different scorecards.
      required:
        - structuredOutputId
        - conditions
    CreateScorecardDTO:
      type: object
      properties:
        name:
          type: string
          description: >-
            This is the name of the scorecard. It is only for user reference and
            will not be used for any evaluation.
        description:
          type: string
          description: >-
            This is the description of the scorecard. It is only for user
            reference and will not be used for any evaluation.
        metrics:
          type: array
          items:
            $ref: '#/components/schemas/ScorecardMetric'
          description: >-
            These are the metrics that will be used to evaluate the scorecard.

            Each metric will have a set of conditions and points that will be
            used to generate the score.
        assistantIds:
          type: array
          items:
            type: string
          description: >-
            These are the assistant IDs that this scorecard is linked to.

            When linked to assistants, this scorecard will be available for
            evaluation during those assistants' calls.
      required:
        - metrics
    ArtifactPlan:
      type: object
      properties:
        recordingEnabled:
          type: boolean
          description: >-
            This determines whether assistant's calls are recorded. Defaults to
            true.


            Usage:

            - If you don't want to record the calls, set this to false.

            - If you want to record the calls when `assistant.hipaaEnabled`
            (deprecated) or `assistant.compliancePlan.hipaaEnabled` explicity
            set this to true and make sure to provide S3 or GCP credentials on
            the Provider Credentials page in the Dashboard.


            You can find the recording at `call.artifact.recordingUrl` and
            `call.artifact.stereoRecordingUrl` after the call is ended.


            @default true
        recordingFormat:
          $ref: '#/components/schemas/ArtifactPlanRecordingFormat'
          description: |-
            This determines the format of the recording. Defaults to `wav;l16`.

            @default 'wav;l16'
        recordingUseCustomStorageEnabled:
          type: boolean
          description: >-
            This determines whether to use custom storage (S3 or GCP) for call
            recordings when storage credentials are configured.


            When set to false, recordings will be stored on Vapi's storage
            instead of your custom storage, even if you have custom storage
            credentials configured.


            Usage:

            - Set to false if you have custom storage configured but want to
            store recordings on Vapi's storage for this assistant.

            - Set to true (or leave unset) to use your custom storage for
            recordings when available.


            @default true
        videoRecordingEnabled:
          type: boolean
          description: >-
            This determines whether the video is recorded during the call.
            Defaults to false. Only relevant for `webCall` type.


            You can find the video recording at
            `call.artifact.videoRecordingUrl` after the call is ended.


            @default false
        fullMessageHistoryEnabled:
          type: boolean
          description: >-
            This determines whether the artifact contains the full message
            history, even after handoff context engineering. Defaults to false.
        pcapEnabled:
          type: boolean
          description: >-
            This determines whether the SIP packet capture is enabled. Defaults
            to true. Only relevant for `phone` type calls where phone number's
            provider is `vapi` or `byo-phone-number`.


            You can find the packet capture at `call.artifact.pcapUrl` after the
            call is ended.


            @default true
        pcapS3PathPrefix:
          type: string
          description: >-
            This is the path where the SIP packet capture will be uploaded. This
            is only used if you have provided S3 or GCP credentials on the
            Provider Credentials page in the Dashboard.


            If credential.s3PathPrefix or credential.bucketPlan.path is set,
            this will append to it.


            Usage:

            - If you want to upload the packet capture to a specific path, set
            this to the path. Example: `/my-assistant-captures`.

            - If you want to upload the packet capture to the root of the
            bucket, set this to `/`.


            @default '/'
        pcapUseCustomStorageEnabled:
          type: boolean
          description: >-
            This determines whether to use custom storage (S3 or GCP) for SIP
            packet captures when storage credentials are configured.


            When set to false, packet captures will be stored on Vapi's storage
            instead of your custom storage, even if you have custom storage
            credentials configured.


            Usage:

            - Set to false if you have custom storage configured but want to
            store packet captures on Vapi's storage for this assistant.

            - Set to true (or leave unset) to use your custom storage for packet
            captures when available.


            @default true
        loggingEnabled:
          type: boolean
          description: |-
            This determines whether the call logs are enabled. Defaults to true.

            @default true
        loggingUseCustomStorageEnabled:
          type: boolean
          description: >-
            This determines whether to use custom storage (S3 or GCP) for call
            logs when storage credentials are configured.


            When set to false, logs will be stored on Vapi's storage instead of
            your custom storage, even if you have custom storage credentials
            configured.


            Usage:

            - Set to false if you have custom storage configured but want to
            store logs on Vapi's storage for this assistant.

            - Set to true (or leave unset) to use your custom storage for logs
            when available.


            @default true
        transcriptPlan:
          $ref: '#/components/schemas/TranscriptPlan'
          description: >-
            This is the plan for `call.artifact.transcript`. To disable, set
            `transcriptPlan.enabled` to false.
        recordingPath:
          type: string
          description: >-
            This is the path where the recording will be uploaded. This is only
            used if you have provided S3 or GCP credentials on the Provider
            Credentials page in the Dashboard.


            If credential.s3PathPrefix or credential.bucketPlan.path is set,
            this will append to it.


            Usage:

            - If you want to upload the recording to a specific path, set this
            to the path. Example: `/my-assistant-recordings`.

            - If you want to upload the recording to the root of the bucket, set
            this to `/`.


            @default '/'
        structuredOutputIds:
          type: array
          items:
            type: string
          description: >-
            This is an array of structured output IDs to be calculated during
            the call.

            The outputs will be extracted and stored in
            `call.artifact.structuredOutputs` after the call is ended.
        structuredOutputs:
          type: array
          items:
            $ref: '#/components/schemas/CreateStructuredOutputDTO'
          description: >-
            This is an array of transient structured outputs to be calculated
            during the call.

            The outputs will be extracted and stored in
            `call.artifact.structuredOutputs` after the call is ended.

            Use this to provide inline structured output configurations instead
            of referencing existing ones via structuredOutputIds.
        scorecardIds:
          type: array
          items:
            type: string
          description: >-
            This is an array of scorecard IDs that will be evaluated based on
            the structured outputs extracted during the call.

            The scorecards will be evaluated and the results will be stored in
            `call.artifact.scorecards` after the call has ended.
        scorecards:
          type: array
          items:
            $ref: '#/components/schemas/CreateScorecardDTO'
          description: >-
            This is the array of scorecards that will be evaluated based on the
            structured outputs extracted during the call.

            The scorecards will be evaluated and the results will be stored in
            `call.artifact.scorecards` after the call has ended.
        loggingPath:
          type: string
          description: >-
            This is the path where the call logs will be uploaded. This is only
            used if you have provided S3 or GCP credentials on the Provider
            Credentials page in the Dashboard.


            If credential.s3PathPrefix or credential.bucketPlan.path is set,
            this will append to it.


            Usage:

            - If you want to upload the call logs to a specific path, set this
            to the path. Example: `/my-assistant-logs`.

            - If you want to upload the call logs to the root of the bucket, set
            this to `/`.


            @default '/'
    StartSpeakingPlanSmartEndpointingEnabled1:
      type: string
      enum:
        - value: livekit
    StartSpeakingPlanSmartEndpointingEnabled:
      oneOf:
        - type: boolean
        - $ref: '#/components/schemas/StartSpeakingPlanSmartEndpointingEnabled1'
    VapiSmartEndpointingPlanProvider:
      type: string
      enum:
        - value: vapi
        - value: livekit
        - value: custom-endpointing-model
    VapiSmartEndpointingPlan:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/VapiSmartEndpointingPlanProvider'
          description: This is the provider for the smart endpointing plan.
      required:
        - provider
    LivekitSmartEndpointingPlanProvider:
      type: string
      enum:
        - value: vapi
        - value: livekit
        - value: custom-endpointing-model
    LivekitSmartEndpointingPlan:
      type: object
      properties:
        provider:
          $ref: '#/components/schemas/LivekitSmartEndpointingPlanProvider'
          description: This is the provider for the smart endpointing plan.
        waitFunction:
          type: string
          description: >-
            This expression describes how long the bot will wait to start
            speaking based on the likelihood that the user has reached an
            endpoint.


            This is a millisecond valued function. It maps probabilities (real
            numbers on [0,1]) to milliseconds that the bot should wait before
            speaking ([0, \infty]). Any negative values that are returned are
            set to zero (the bot can't start talking in the past).


            A probability of zero represents very high confidence that the
            caller has stopped speaking, and would like the bot to speak to
            them. A probability of one represents very high confidence that the
            caller is still speaking.


            Under the hood, this is parsed into a mathjs expression. Whatever
            you use to write your expression needs to be valid with respect to
            mathjs


            @default "20 + 500 * sqrt(x) + 2500 * x^3"
      required:
        - provider
    CustomEndpointingModelSmartEndpointingPlanProvider:
      type: string
      enum:
        - value: vapi
        - value: livekit
        - value: custom-endpointing-model
    CustomEndpointingModelSmartEndpointingPlan:
      type: object
      properties:
        provider:
          $ref: >-
            #/components/schemas/CustomEndpointingModelSmartEndpointingPlanProvider
          description: >-
            This is the provider for the smart endpointing plan. Use
            `custom-endpointing-model` for custom endpointing providers that are
            not natively supported.
        server:
          $ref: '#/components/schemas/Server'
          description: >-
            This is where the endpointing request will be sent. If not provided,
            will be sent to `assistant.server`. If that does not exist either,
            will be sent to `org.server`.


            Request Example:


            POST https://{server.url}

            Content-Type: application/json


            {
              "message": {
                "type": "call.endpointing.request",
                "messages": [
                  {
                    "role": "user",
                    "message": "Hello, how are you?",
                    "time": 1234567890,
                    "secondsFromStart": 0
                  }
                ],
                ...other metadata about the call...
              }
            }


            Response Expected:

            {
              "timeoutSeconds": 0.5
            }


            The timeout is the number of seconds to wait before considering the
            user's speech as finished. The endpointing timeout is automatically
            reset each time a new transcript is received (and another
            `call.endpointing.request` is sent).
      required:
        - provider
    StartSpeakingPlanSmartEndpointingPlan:
      oneOf:
        - $ref: '#/components/schemas/VapiSmartEndpointingPlan'
        - $ref: '#/components/schemas/LivekitSmartEndpointingPlan'
        - $ref: '#/components/schemas/CustomEndpointingModelSmartEndpointingPlan'
    AssistantCustomEndpointingRuleType:
      type: string
      enum:
        - value: assistant
    AssistantCustomEndpointingRule:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/AssistantCustomEndpointingRuleType'
          description: >-
            This endpointing rule is based on the last assistant message before
            customer started speaking.


            Flow:

            - Assistant speaks

            - Customer starts speaking

            - Customer transcription comes in

            - This rule is evaluated on the last assistant message

            - If a match is found based on `regex`, the endpointing timeout is
            set to `timeoutSeconds`


            Usage:

            - If you have yes/no questions in your use case like "are you
            interested in a loan?", you can set a shorter timeout.

            - If you have questions where the customer may pause to look up
            information like "what's my account number?", you can set a longer
            timeout.
        regex:
          type: string
          description: >-
            This is the regex pattern to match.


            Note:

            - This works by using the `RegExp.test` method in Node.JS. Eg.
            `/hello/.test("hello there")` will return `true`.


            Hot tip:

            - In JavaScript, escape `\` when sending the regex pattern. Eg.
            `"hello\sthere"` will be sent over the wire as `"hellosthere"`. Send
            `"hello\\sthere"` instead.

            - `RegExp.test` does substring matching, so `/cat/.test("I love
            cats")` will return `true`. To do full string matching, send
            "^cat$".
        regexOptions:
          type: array
          items:
            $ref: '#/components/schemas/RegexOption'
          description: |-
            These are the options for the regex match. Defaults to all disabled.

            @default []
        timeoutSeconds:
          type: number
          format: double
          description: This is the endpointing timeout in seconds, if the rule is matched.
      required:
        - type
        - regex
        - timeoutSeconds
    CustomerCustomEndpointingRuleType:
      type: string
      enum:
        - value: customer
    CustomerCustomEndpointingRule:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/CustomerCustomEndpointingRuleType'
          description: >-
            This endpointing rule is based on current customer message as they
            are speaking.


            Flow:

            - Assistant speaks

            - Customer starts speaking

            - Customer transcription comes in

            - This rule is evaluated on the current customer transcription

            - If a match is found based on `regex`, the endpointing timeout is
            set to `timeoutSeconds`


            Usage:

            - If you want to wait longer while customer is speaking numbers, you
            can set a longer timeout.
        regex:
          type: string
          description: >-
            This is the regex pattern to match.


            Note:

            - This works by using the `RegExp.test` method in Node.JS. Eg.
            `/hello/.test("hello there")` will return `true`.


            Hot tip:

            - In JavaScript, escape `\` when sending the regex pattern. Eg.
            `"hello\sthere"` will be sent over the wire as `"hellosthere"`. Send
            `"hello\\sthere"` instead.

            - `RegExp.test` does substring matching, so `/cat/.test("I love
            cats")` will return `true`. To do full string matching, send
            "^cat$".
        regexOptions:
          type: array
          items:
            $ref: '#/components/schemas/RegexOption'
          description: |-
            These are the options for the regex match. Defaults to all disabled.

            @default []
        timeoutSeconds:
          type: number
          format: double
          description: This is the endpointing timeout in seconds, if the rule is matched.
      required:
        - type
        - regex
        - timeoutSeconds
    BothCustomEndpointingRuleType:
      type: string
      enum:
        - value: both
    BothCustomEndpointingRule:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/BothCustomEndpointingRuleType'
          description: >-
            This endpointing rule is based on both the last assistant message
            and the current customer message as they are speaking.


            Flow:

            - Assistant speaks

            - Customer starts speaking

            - Customer transcription comes in

            - This rule is evaluated on the last assistant message and the
            current customer transcription

            - If assistant message matches `assistantRegex` AND customer message
            matches `customerRegex`, the endpointing timeout is set to
            `timeoutSeconds`


            Usage:

            - If you want to wait longer while customer is speaking numbers, you
            can set a longer timeout.
        assistantRegex:
          type: string
          description: >-
            This is the regex pattern to match the assistant's message.


            Note:

            - This works by using the `RegExp.test` method in Node.JS. Eg.
            `/hello/.test("hello there")` will return `true`.


            Hot tip:

            - In JavaScript, escape `\` when sending the regex pattern. Eg.
            `"hello\sthere"` will be sent over the wire as `"hellosthere"`. Send
            `"hello\\sthere"` instead.

            - `RegExp.test` does substring matching, so `/cat/.test("I love
            cats")` will return `true`. To do full string matching, send
            "^cat$".
        assistantRegexOptions:
          type: array
          items:
            $ref: '#/components/schemas/RegexOption'
          description: >-
            These are the options for the assistant's message regex match.
            Defaults to all disabled.


            @default []
        customerRegex:
          type: string
        customerRegexOptions:
          type: array
          items:
            $ref: '#/components/schemas/RegexOption'
          description: >-
            These are the options for the customer's message regex match.
            Defaults to all disabled.


            @default []
        timeoutSeconds:
          type: number
          format: double
          description: This is the endpointing timeout in seconds, if the rule is matched.
      required:
        - type
        - assistantRegex
        - customerRegex
        - timeoutSeconds
    StartSpeakingPlanCustomEndpointingRulesItems:
      oneOf:
        - $ref: '#/components/schemas/AssistantCustomEndpointingRule'
        - $ref: '#/components/schemas/CustomerCustomEndpointingRule'
        - $ref: '#/components/schemas/BothCustomEndpointingRule'
    TranscriptionEndpointingPlan:
      type: object
      properties:
        onPunctuationSeconds:
          type: number
          format: double
          description: >-
            The minimum number of seconds to wait after transcription ending
            with punctuation before sending a request to the model. Defaults to
            0.1.


            This setting exists because the transcriber punctuates the
            transcription when it's more confident that customer has completed a
            thought.


            @default 0.1
        onNoPunctuationSeconds:
          type: number
          format: double
          description: >-
            The minimum number of seconds to wait after transcription ending
            without punctuation before sending a request to the model. Defaults
            to 1.5.


            This setting exists to catch the cases where the transcriber was not
            confident enough to punctuate the transcription, but the customer is
            done and has been silent for a long time.


            @default 1.5
        onNumberSeconds:
          type: number
          format: double
          description: >-
            The minimum number of seconds to wait after transcription ending
            with a number before sending a request to the model. Defaults to
            0.4.


            This setting exists because the transcriber will sometimes punctuate
            the transcription ending with a number, even though the customer
            hasn't uttered the full number. This happens commonly for long
            numbers when the customer reads the number in chunks.


            @default 0.5
    StartSpeakingPlan:
      type: object
      properties:
        waitSeconds:
          type: number
          format: double
          description: >-
            This is how long assistant waits before speaking. Defaults to 0.4.


            This is the minimum it will wait but if there is latency is the
            pipeline, this minimum will be exceeded. This is intended as a
            stopgap in case the pipeline is moving too fast.


            Example:

            - If model generates tokens and voice generates bytes within 100ms,
            the pipeline still waits 300ms before outputting speech.


            Usage:

            - If the customer is taking long pauses, set this to a higher value.

            - If the assistant is accidentally jumping in too much, set this to
            a higher value.


            @default 0.4
        smartEndpointingEnabled:
          $ref: '#/components/schemas/StartSpeakingPlanSmartEndpointingEnabled'
        smartEndpointingPlan:
          $ref: '#/components/schemas/StartSpeakingPlanSmartEndpointingPlan'
          description: >-
            This is the plan for smart endpointing. Pick between Vapi smart
            endpointing, LiveKit, or custom endpointing model (or nothing). We
            strongly recommend using livekit endpointing when working in
            English. LiveKit endpointing is not supported in other languages,
            yet.


            If this is set, it will override and take precedence over
            `transcriptionEndpointingPlan`.

            This plan will still be overridden by any matching
            `customEndpointingRules`.


            If this is not set, the system will automatically use the
            transcriber's built-in endpointing capabilities if available.
        customEndpointingRules:
          type: array
          items:
            $ref: '#/components/schemas/StartSpeakingPlanCustomEndpointingRulesItems'
          description: >-
            These are the custom endpointing rules to set an endpointing timeout
            based on a regex on the customer's speech or the assistant's last
            message.


            Usage:

            - If you have yes/no questions like "are you interested in a loan?",
            you can set a shorter timeout.

            - If you have questions where the customer may pause to look up
            information like "what's my account number?", you can set a longer
            timeout.

            - If you want to wait longer while customer is enumerating a list of
            numbers, you can set a longer timeout.


            These rules have the highest precedence and will override both
            `smartEndpointingPlan` and `transcriptionEndpointingPlan` when a
            rule is matched.


            The rules are evaluated in order and the first one that matches will
            be used.


            Order of precedence for endpointing:

            1. customEndpointingRules (if any match)

            2. smartEndpointingPlan (if set)

            3. transcriptionEndpointingPlan


            @default []
        transcriptionEndpointingPlan:
          $ref: '#/components/schemas/TranscriptionEndpointingPlan'
          description: >-
            This determines how a customer speech is considered done
            (endpointing) using the transcription of customer's speech.


            Once an endpoint is triggered, the request is sent to
            `assistant.model`.


            Note: This plan is only used if `smartEndpointingPlan` is not set
            and transcriber does not have built-in endpointing capabilities. If
            both are provided, `smartEndpointingPlan` takes precedence.

            This plan will also be overridden by any matching
            `customEndpointingRules`.
    StopSpeakingPlan:
      type: object
      properties:
        numWords:
          type: number
          format: double
          description: >-
            This is the number of words that the customer has to say before the
            assistant will stop talking.


            Words like "stop", "actually", "no", etc. will always interrupt
            immediately regardless of this value.


            Words like "okay", "yeah", "right" will never interrupt.


            When set to 0, `voiceSeconds` is used in addition to the
            transcriptions to determine the customer has started speaking.


            Defaults to 0.


            @default 0
        voiceSeconds:
          type: number
          format: double
          description: >-
            This is the seconds customer has to speak before the assistant stops
            talking. This uses the VAD (Voice Activity Detection) spike to
            determine if the customer has started speaking.


            Considerations:

            - A lower value might be more responsive but could potentially pick
            up non-speech sounds.

            - A higher value reduces false positives but might slightly delay
            the detection of speech onset.


            This is only used if `numWords` is set to 0.


            Defaults to 0.2


            @default 0.2
        backoffSeconds:
          type: number
          format: double
          description: >-
            This is the seconds to wait before the assistant will start talking
            again after being interrupted.


            Defaults to 1.


            @default 1
        acknowledgementPhrases:
          type: array
          items:
            type: string
          description: >-
            These are the phrases that will never interrupt the assistant, even
            if numWords threshold is met.

            These are typically acknowledgement or backchanneling phrases.
        interruptionPhrases:
          type: array
          items:
            type: string
          description: >-
            These are the phrases that will always interrupt the assistant
            immediately, regardless of numWords.

            These are typically phrases indicating disagreement or desire to
            stop.
    MonitorPlan:
      type: object
      properties:
        listenEnabled:
          type: boolean
          description: >-
            This determines whether the assistant's calls allow live listening.
            Defaults to true.


            Fetch `call.monitor.listenUrl` to get the live listening URL.


            @default true
        listenAuthenticationEnabled:
          type: boolean
          description: >-
            This enables authentication on the `call.monitor.listenUrl`.


            If `listenAuthenticationEnabled` is `true`, the
            `call.monitor.listenUrl` will require an `Authorization: Bearer
            <vapi-public-api-key>` header.


            @default false
        controlEnabled:
          type: boolean
          description: >-
            This determines whether the assistant's calls allow live control.
            Defaults to true.


            Fetch `call.monitor.controlUrl` to get the live control URL.


            To use, send any control message via a POST request to
            `call.monitor.controlUrl`. Here are the types of controls supported:
            https://docs.vapi.ai/api-reference/messages/client-inbound-message


            @default true
        controlAuthenticationEnabled:
          type: boolean
          description: >-
            This enables authentication on the `call.monitor.controlUrl`.


            If `controlAuthenticationEnabled` is `true`, the
            `call.monitor.controlUrl` will require an `Authorization: Bearer
            <vapi-public-api-key>` header.


            @default false
        monitorIds:
          type: array
          items:
            type: string
          description: >-
            This the set of monitor ids that are attached to the assistant.

            The source of truth for the monitor ids is the assistant_monitor
            join table.

            This field can be used for transient assistants and to update
            assistants with new monitor ids.


            @default []
    KeypadInputPlanDelimiters:
      type: string
      enum:
        - value: '#'
        - value: '*'
        - value: ''
    KeypadInputPlan:
      type: object
      properties:
        enabled:
          type: boolean
          description: |-
            This keeps track of whether the user has enabled keypad input.
            By default, it is off.

            @default false
        timeoutSeconds:
          type: number
          format: double
          description: >-
            This is the time in seconds to wait before processing the input.

            If the input is not received within this time, the input will be
            ignored.

            If set to "off", the input will be processed when the user enters a
            delimiter or immediately if no delimiter is used.


            @default 2
        delimiters:
          $ref: '#/components/schemas/KeypadInputPlanDelimiters'
          description: |-
            This is the delimiter(s) that will be used to process the input.
            Can be '#', '*', or an empty array.
    AssistantOverrides:
      type: object
      properties:
        transcriber:
          $ref: '#/components/schemas/AssistantOverridesTranscriber'
          description: These are the options for the assistant's transcriber.
        model:
          $ref: '#/components/schemas/AssistantOverridesModel'
          description: These are the options for the assistant's LLM.
        voice:
          $ref: '#/components/schemas/AssistantOverridesVoice'
          description: These are the options for the assistant's voice.
        firstMessage:
          type: string
          description: >-
            This is the first message that the assistant will say. This can also
            be a URL to a containerized audio file (mp3, wav, etc.).


            If unspecified, assistant will wait for user to speak and use the
            model to respond once they speak.
        firstMessageInterruptionsEnabled:
          type: boolean
          default: false
        firstMessageMode:
          $ref: '#/components/schemas/AssistantOverridesFirstMessageMode'
          description: >-
            This is the mode for the first message. Default is
            'assistant-speaks-first'.


            Use:

            - 'assistant-speaks-first' to have the assistant speak first.

            - 'assistant-waits-for-user' to have the assistant wait for the user
            to speak first.

            - 'assistant-speaks-first-with-model-generated-message' to have the
            assistant speak first with a message generated by the model based on
            the conversation state. (`assistant.model.messages` at call start,
            `call.messages` at squad transfer points).


            @default 'assistant-speaks-first'
        voicemailDetection:
          $ref: '#/components/schemas/AssistantOverridesVoicemailDetection'
          description: >-
            These are the settings to configure or disable voicemail detection.
            Alternatively, voicemail detection can be configured using the
            model.tools=[VoicemailTool].

            By default, voicemail detection is disabled.
        clientMessages:
          $ref: '#/components/schemas/AssistantOverridesClientMessages'
          description: >-
            These are the messages that will be sent to your Client SDKs.
            Default is
            conversation-update,function-call,hang,model-output,speech-update,status-update,transfer-update,transcript,tool-calls,user-interrupted,voice-input,workflow.node.started,assistant.started.
            You can check the shape of the messages in ClientMessage schema.
        serverMessages:
          $ref: '#/components/schemas/AssistantOverridesServerMessages'
          description: >-
            These are the messages that will be sent to your Server URL. Default
            is
            conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,handoff-destination-request,user-interrupted,assistant.started.
            You can check the shape of the messages in ServerMessage schema.
        maxDurationSeconds:
          type: number
          format: double
          description: >-
            This is the maximum number of seconds that the call will last. When
            the call reaches this duration, it will be ended.


            @default 600 (10 minutes)
        backgroundSound:
          $ref: '#/components/schemas/AssistantOverridesBackgroundSound'
          description: >-
            This is the background sound in the call. Default for phone calls is
            'office' and default for web calls is 'off'.

            You can also provide a custom sound by providing a URL to an audio
            file.
        modelOutputInMessagesEnabled:
          type: boolean
          description: >-
            This determines whether the model's output is used in conversation
            history rather than the transcription of assistant's speech.


            Default `false` while in beta.


            @default false
        transportConfigurations:
          type: array
          items:
            $ref: >-
              #/components/schemas/AssistantOverridesTransportConfigurationsItems
          description: >-
            These are the configurations to be passed to the transport providers
            of assistant's calls, like Twilio. You can store multiple
            configurations for different transport providers. For a call, only
            the configuration matching the call transport provider is used.
        observabilityPlan:
          $ref: '#/components/schemas/LangfuseObservabilityPlan'
          description: |-
            This is the plan for observability of assistant's calls.

            Currently, only Langfuse is supported.
        credentials:
          type: array
          items:
            $ref: '#/components/schemas/AssistantOverridesCredentialsItems'
          description: >-
            These are dynamic credentials that will be used for the assistant
            calls. By default, all the credentials are available for use in the
            call but you can supplement an additional credentials using this.
            Dynamic credentials override existing credentials.
        hooks:
          type: array
          items:
            $ref: '#/components/schemas/AssistantOverridesHooksItems'
          description: This is a set of actions that will be performed on certain events.
        tools:append:
          type: array
          items:
            $ref: '#/components/schemas/AssistantOverridesToolsAppendItems'
        variableValues:
          $ref: '#/components/schemas/AssistantOverridesVariableValues'
          description: >-
            These are values that will be used to replace the template variables
            in the assistant messages and other text-based fields.

            This uses LiquidJS syntax.
            https://liquidjs.com/tutorials/intro-to-liquid.html


            So for example, `{{ name }}` will be replaced with the value of
            `name` in `variableValues`.

            `{{"now" | date: "%b %d, %Y, %I:%M %p", "America/New_York"}}` will
            be replaced with the current date and time in New York.
             Some VAPI reserved defaults:
             - *customer* - the customer object
        name:
          type: string
          description: >-
            This is the name of the assistant.


            This is required when you want to transfer between assistants in a
            call.
        voicemailMessage:
          type: string
          description: >-
            This is the message that the assistant will say if the call is
            forwarded to voicemail.


            If unspecified, it will hang up.
        endCallMessage:
          type: string
          description: |-
            This is the message that the assistant will say if it ends the call.

            If unspecified, it will hang up without saying anything.
        endCallPhrases:
          type: array
          items:
            type: string
          description: >-
            This list contains phrases that, if spoken by the assistant, will
            trigger the call to be hung up. Case insensitive.
        compliancePlan:
          $ref: '#/components/schemas/CompliancePlan'
        metadata:
          $ref: '#/components/schemas/AssistantOverridesMetadata'
          description: This is for metadata you want to store on the assistant.
        backgroundSpeechDenoisingPlan:
          $ref: '#/components/schemas/BackgroundSpeechDenoisingPlan'
          description: >-
            This enables filtering of noise and background speech while the user
            is talking.


            Features:

            - Smart denoising using Krisp

            - Fourier denoising


            Smart denoising can be combined with or used independently of
            Fourier denoising.


            Order of precedence:

            - Smart denoising

            - Fourier denoising
        analysisPlan:
          $ref: '#/components/schemas/AnalysisPlan'
          description: >-
            This is the plan for analysis of assistant's calls. Stored in
            `call.analysis`.
        artifactPlan:
          $ref: '#/components/schemas/ArtifactPlan'
          description: >-
            This is the plan for artifacts generated during assistant's calls.
            Stored in `call.artifact`.
        startSpeakingPlan:
          $ref: '#/components/schemas/StartSpeakingPlan'
          description: >-
            This is the plan for when the assistant should start talking.


            You should configure this if you're running into these issues:

            - The assistant is too slow to start talking after the customer is
            done speaking.

            - The assistant is too fast to start talking after the customer is
            done speaking.

            - The assistant is so fast that it's actually interrupting the
            customer.
        stopSpeakingPlan:
          $ref: '#/components/schemas/StopSpeakingPlan'
          description: >-
            This is the plan for when assistant should stop talking on customer
            interruption.


            You should configure this if you're running into these issues:

            - The assistant is too slow to recognize customer's interruption.

            - The assistant is too fast to recognize customer's interruption.

            - The assistant is getting interrupted by phrases that are just
            acknowledgments.

            - The assistant is getting interrupted by background noises.

            - The assistant is not properly stopping -- it starts talking right
            after getting interrupted.
        monitorPlan:
          $ref: '#/components/schemas/MonitorPlan'
          description: >-
            This is the plan for real-time monitoring of the assistant's calls.


            Usage:

            - To enable live listening of the assistant's calls, set
            `monitorPlan.listenEnabled` to `true`.

            - To enable live control of the assistant's calls, set
            `monitorPlan.controlEnabled` to `true`.

            - To attach monitors to the assistant, set `monitorPlan.monitorIds`
            to the set of monitor ids.
        credentialIds:
          type: array
          items:
            type: string
          description: >-
            These are the credentials that will be used for the assistant calls.
            By default, all the credentials are available for use in the call
            but you can provide a subset using this.
        server:
          $ref: '#/components/schemas/Server'
          description: >-
            This is where Vapi will send webhooks. You can find all webhooks
            available along with their shape in ServerMessage schema.


            The order of precedence is:


            1. assistant.server.url

            2. phoneNumber.serverUrl

            3. org.serverUrl
        keypadInputPlan:
          $ref: '#/components/schemas/KeypadInputPlan'
    HandoffDestinationAssistant:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/HandoffDestinationAssistantType'
        contextEngineeringPlan:
          $ref: >-
            #/components/schemas/HandoffDestinationAssistantContextEngineeringPlan
          description: >-
            This is the plan for manipulating the message context before handing
            off the call to the next assistant.
        assistantName:
          type: string
          description: >-
            This is the assistant to transfer the call to. You must provide
            either assistantName or assistantId.
        assistantId:
          type: string
          description: >-
            This is the assistant id to transfer the call to. You must provide
            either assistantName or assistantId.
        assistant:
          $ref: '#/components/schemas/CreateAssistantDTO'
          description: >-
            This is a transient assistant to transfer the call to. You may
            provide a transient assistant in the response 
            `handoff-destination-request` in a dynamic handoff.
        variableExtractionPlan:
          $ref: '#/components/schemas/VariableExtractionPlan'
          description: This is the variable extraction plan for the handoff tool.
        assistantOverrides:
          $ref: '#/components/schemas/AssistantOverrides'
          description: >-
            These are the assistant overrides to apply to the destination
            assistant.
        description:
          type: string
          description: >-
            This is the description of the destination, used by the AI to choose
            when and how to transfer the call.
      required:
        - type
    HandoffDestinationDynamicType:
      type: string
      enum:
        - value: dynamic
    HandoffDestinationDynamic:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/HandoffDestinationDynamicType'
        server:
          $ref: '#/components/schemas/Server'
          description: >-
            This is where Vapi will send the handoff-destination-request webhook
            in a dynamic handoff.


            The order of precedence is:


            1. tool.server.url

            2. assistant.server.url

            3. phoneNumber.server.url

            4. org.server.url
        description:
          type: string
          description: >-
            This is the description of the destination, used by the AI to choose
            when and how to transfer the call.
      required:
        - type
    HandoffDestinationSquadType:
      type: string
      enum:
        - value: squad
    HandoffDestinationSquadContextEngineeringPlan:
      oneOf:
        - $ref: '#/components/schemas/ContextEngineeringPlanLastNMessages'
        - $ref: '#/components/schemas/ContextEngineeringPlanNone'
        - $ref: '#/components/schemas/ContextEngineeringPlanAll'
        - $ref: '#/components/schemas/ContextEngineeringPlanUserAndAssistantMessages'
    SquadMemberDtoAssistantDestinationsItems:
      oneOf:
        - $ref: '#/components/schemas/TransferDestinationAssistant'
        - $ref: '#/components/schemas/HandoffDestinationAssistant'
    SquadMemberDTO:
      type: object
      properties:
        assistantDestinations:
          type: array
          items:
            $ref: '#/components/schemas/SquadMemberDtoAssistantDestinationsItems'
        assistantId:
          type:
            - string
            - 'null'
          description: >-
            This is the assistant that will be used for the call. To use a
            transient assistant, use `assistant` instead.
        assistant:
          $ref: '#/components/schemas/CreateAssistantDTO'
          description: >-
            This is the assistant that will be used for the call. To use an
            existing assistant, use `assistantId` instead.
        assistantOverrides:
          $ref: '#/components/schemas/AssistantOverrides'
          description: >-
            This can be used to override the assistant's settings and provide
            values for it's template variables.
    CreateSquadDTO:
      type: object
      properties:
        name:
          type: string
          description: This is the name of the squad.
        members:
          type: array
          items:
            $ref: '#/components/schemas/SquadMemberDTO'
          description: |-
            This is the list of assistants that make up the squad.

            The call will start with the first assistant in the list.
        membersOverrides:
          $ref: '#/components/schemas/AssistantOverrides'
          description: >-
            This can be used to override all the assistants' settings and
            provide values for their template variables.


            Both `membersOverrides` and `members[n].assistantOverrides` can be
            used together. First, `members[n].assistantOverrides` is applied.
            Then, `membersOverrides` is applied as a global override.
      required:
        - members
    HandoffDestinationSquad:
      type: object
      properties:
        type:
          $ref: '#/components/schemas/HandoffDestinationSquadType'
        contextEngineeringPlan:
          $ref: '#/components/schemas/HandoffDestinationSquadContextEngineeringPlan'
          description: >-
            This is the plan for manipulating the message context before handing
            off the call to the squad.
        squadId:
          type: string
          description: This is the squad id to transfer the call to.
        squad:
          $ref: '#/components/schemas/CreateSquadDTO'
          description: This is a transient squad to transfer the call to.
        entryAssistantName:
          type: string
          description: >-
            This is the name of the entry assistant to start with when handing
            off to the squad.

            If not provided, the first member of the squad will be used.
        variableExtractionPlan:
          $ref: '#/components/schemas/VariableExtractionPlan'
          description: This is the variable extraction plan for the handoff tool.
        squadOverrides:
          $ref: '#/components/schemas/AssistantOverrides'
          description: |-
            These are the overrides to apply to the squad configuration.
            Maps to squad-level membersOverrides.
        description:
          type: string
          description: >-
            This is the description of the destination, used by the AI to choose
            when and how to transfer the call.
      required:
        - type
    ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingHandoffDestinationsItems:
      oneOf:
        - $ref: '#/components/schemas/HandoffDestinationAssistant'
        - $ref: '#/components/schemas/HandoffDestinationDynamic'
        - $ref: '#/components/schemas/HandoffDestinationSquad'
    CreateHandoffToolDTO:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingHandoffMessagesItems
          description: >-
            These are the messages that will be spoken to the user as the tool
            is running.


            For some tools, this is auto-filled based on special fields like
            `tool.destinations`. For others like the function tool, these can be
            custom configured.
        destinations:
          type: array
          items:
            $ref: >-
              #/components/schemas/ToolPostRequestBodyContentApplicationJsonSchemaDiscriminatorMappingHandoffDestinationsItems
          description: >-
            These are the destinations that the call can be handed off to.


            Usage:

            1. Single destination


            Use `assistantId` to handoff the call to a saved assistant, or
            `assistantName` to handoff the call to an assistant in the same
            squad.


            ```json

            {
              "tools": [
                {
                  "type": "handoff",
                  "destinations": [
                    {
                      "type": "assistant",
                      "assistantId": "assistant-123", // or "assistantName": "Assistant123"
                      "description": "customer wants to be handed off to assistant-123",
                      "contextEngineeringPlan": {
                        "type": "all"
                      }
                    }
                  ],
                }
              ]
            }

            ```


            2. Multiple destinations


            2.1. Multiple Tools, Each With One Destination (OpenAI recommended)


            ```json

            {
              "tools": [
                {
                  "type": "handoff",
                  "destinations": [
                    {
                      "type": "assistant",
                      "assistantId": "assistant-123",
                      "description": "customer wants to be handed off to assistant-123",
                      "contextEngineeringPlan": {
                        "type": "all"
                      }
                    },
                  ],
                },
                {
                  "type": "handoff",
                  "destinations": [
                    {
                      "type": "assistant",
                      "assistantId": "assistant-456",
                      "description": "customer wants to be handed off to assistant-456",
                      "contextEngineeringPlan": {
                        "type": "all"
                      }
                    }
                  ],
                }
              ]
            }

            ```


            2.2. One Tool, Multiple Destinations (Anthropic recommended)


            ```json

            {
              "tools": [
                {
                  "type": "handoff",
                  "destinations": [
                    {
                      "type": "assistant",
                      "assistantId": "assistant-123",
                      "description": "customer wants to be handed off to assistant-123",
                      "contextEngineeringPlan": {
                        "type": "all"
                      }
                    },
                    {
                      "type": "assistant",
                      "assistantId": "assistant-456",
                      "description": "customer wants to be handed off to assistant-456",
                      "contextEngineeringPlan": {
                        "type": "all"
                      }
                    }
                  ],
                }
              ]
            }

            ```


            3. Dynamic destination


            3.1 To determine the destination dynamically, supply a `dynamic`
            handoff destination type and a `server` object.
                VAPI will send a handoff-destination-request webhook to the `server.url`.
                The response from the server will be used as the destination (if valid).

            ```json

            {
              "tools": [
                {
                  "type": "handoff",
                  "destinations": [
                    {
                      "type": "dynamic",
                      "server": {
                        "url": "https://example.com"
                      }
                    }
                  ],
                }
              ]
            }

            ```


            3.2. To pass custom parameters to the server, you can use the
            `function` object.


            ```json

            {
              "tools": [
                {
                  "type": "handoff",
                  "destinations": [
                    {
                      "type": "dynamic",
                      "server": {
                        "url": "https://example.com"
                      },
                    }
                  ],
                  "function": {
                    "name": "handoff",
                    "description": "Call this function when the customer is ready to be handed off to the next assistant",
                    "parameters": {
                      "type": "object",
                      "properties": {
                        "destination": {
                          "type": "string",
                          "description": "Use dynamic when customer is ready to be handed off to the next assistant",
                          "enum": ["dynamic"]
                        },
                        "customerAreaCode": {
                          "type": "number",
                          "description": "Area code of the customer"
                        },
                        "customerIntent": {
                          "type": "string",
                          "enum": ["new-customer", "existing-customer"],
                          "description": "Use new-customer when customer is a new customer, existing-customer when customer is an existing customer"
                        },
                        "customerSentiment": {
                          "type": "string",
                          "enum": ["positive", "negative", "neutral"],
                          "description": "Use positive when customer is happy, negative when customer is unhappy, neutral when customer is neutral"
                        }
                      }
                    }
                  }
                }
              ]
            }

            ```


            The properties `customerAreaCode`, `customerIntent`, and
            `customerSentiment` will be passed to the server in the webhook
            request body.
        function:
          $ref: '#/components/schemas/OpenAIFunction'
          description: >-
            This is the optional function definition that will be passed to the
            LLM.

            If this is not defined, we will construct this based on the other
            properties.


            For example, given the following tools definition:

            ```json

            {
              "tools": [
                {
                  "type": "handoff",
                  "destinations": [
                    {
                      "type": "assistant",
                      "assistantId": "assistant-123",
                      "description": "customer wants to be handed off to assistant-123",
                      "contextEngineeringPlan": {
                        "type": "all"
                      }
                    },
                    {
                      "type": "assistant",
                      "assistantId": "assistant-456",
                      "description": "customer wants to be handed off to assistant-456",
                      "contextEngineeringPlan": {
                        "type": "all"
                      }
                    }
                  ],
                }
              ]
            }

            ```


            We will construct the following function definition:

            ```json

            {
              "function": {
                "name": "handoff_to_assistant-123",
                "description": "
                     Use this function to handoff the call to the next assistant.
                     Only use it when instructions explicitly ask you to use the handoff_to_assistant function.
                     DO NOT call this function unless you are instructed to do so.
                     Here are the destinations you can handoff the call to:
                     1. assistant-123. When: customer wants to be handed off to assistant-123
                     2. assistant-456. When: customer wants to be handed off to assistant-456
                ",
                "parameters": {
                  "type": "object",
                  "properties": {
                    "destination": {
                      "type": "string",
                      "description": "Options: assistant-123 (customer wants to be handed off to assistant-123), assistant-456 (customer wants to be handed off to assistant-456)",
                      "enum": ["assistant-123", "assistant-456"]
                    },
                  },
                  "required": ["destination"]
                }
              }
            }

            ```


            To override this function, please provide an OpenAI function
            definition and refer to it in the system prompt.

            You may override parts of the function definition (i.e. you may only
            want to change the function name for your prompt).

            If you choose to override the function parameters, it must include
            `destination` as a required parameter, and it must evaluate to
            either an assistantId, assistantName, or a the string literal
            `dynamic`.


            To pass custom parameters to the server in a dynamic handoff, you
            can use the function parameters, with `dynamic` as the destination.

            ```json

            {
              "function": {
                "name": "dynamic_handoff",
                "description": "
                     Call this function when the customer is ready to be handed off to the next assistant
                ",
                "parameters": {
                  "type": "object",
                  "properties": {
                    "destination": {
                      "type": "string",
                      "enum": ["dynamic"]
                    },
                    "customerAreaCode": {
                      "type": "number",
                      "description": "Area code of the customer"
                    },
                    "customerIntent": {
                      "type": "string",
                      "enum": ["new-customer", "existing-customer"],
                      "description": "Use new-customer when customer is a new customer, existing-customer when customer is an existing customer"
                    },
                    "customerSentiment": {
                      "type": "string",
                      "enum": ["positive", "negative", "neutral"],
                      "description": "Use positive when customer is happy, negative when customer is unhappy, neutral when customer is neutral"
                    }
                  },
                  "required": ["destination", "customerAreaCode", "customerIntent", "customerSentiment"]
                }
              }
            }

            ```
        rejectionPlan:
          $ref: '#/components/schemas/ToolRejectionPlan'
          description: >-
            This is the plan to reject a tool call based on the conversation
            state.


            // Example 1: Reject endCall if user didn't say goodbye

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '(?i)\\b(bye|goodbye|farewell|see you later|take care)\\b',
                target: { position: -1, role: 'user' },
                negate: true  // Reject if pattern does NOT match
              }]
            }

            ```


            // Example 2: Reject transfer if user is actually asking a question

            ```json

            {
              conditions: [{
                type: 'regex',
                regex: '\\?',
                target: { position: -1, role: 'user' }
              }]
            }

            ```


            // Example 3: Reject transfer if user didn't mention transfer
            recently

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 5 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' %}

            {% assign mentioned = false %}

            {% for msg in userMessages %}
              {% if msg.content contains 'transfer' or msg.content contains 'connect' or msg.content contains 'speak to' %}
                {% assign mentioned = true %}
                {% break %}
              {% endif %}
            {% endfor %}

            {% if mentioned %}
              false
            {% else %}
              true
            {% endif %}`
              }]
            }

            ```


            // Example 4: Reject endCall if the bot is looping and trying to
            exit

            ```json

            {
              conditions: [{
                type: 'liquid',
                liquid: `{% assign recentMessages = messages | last: 6 %}
            {% assign userMessages = recentMessages | where: 'role', 'user' |
            reverse %}

            {% if userMessages.size < 3 %}
              false
            {% else %}
              {% assign msg1 = userMessages[0].content | downcase %}
              {% assign msg2 = userMessages[1].content | downcase %}
              {% assign msg3 = userMessages[2].content | downcase %}
              {% comment %} Check for repetitive messages {% endcomment %}
              {% if msg1 == msg2 or msg1 == msg3 or msg2 == msg3 %}
                true
              {% comment %} Check for common loop phrases {% endcomment %}
              {% elsif msg1 contains 'cool thanks' or msg2 contains 'cool thanks' or msg3 contains 'cool thanks' %}
                true
              {% elsif msg1 contains 'okay thanks' or msg2 contains 'okay thanks' or msg3 contains 'okay thanks' %}
                true
              {% elsif msg1 contains 'got it' or msg2 contains 'got it' or msg3 contains 'got it' %}
                true
              {% else %}
                false
              {% endif %}
            {% endif %}`
              }]
            }

            ```
    AnthropicModelToolsItems:
      oneOf:
        - $ref: '#/components/schemas/CreateApiRequestToolDTO'
        - $ref: '#/components/schemas/CreateBashToolDTO'
        - $ref: '#/components/schemas/CreateCodeToolDTO'
        - $ref: '#/components/schemas/CreateComputerToolDTO'
        - $ref: '#/components/schemas/CreateDtmfToolDTO'
        - $ref: '#/components/schemas/CreateEndCallToolDTO'
        - $ref: '#/components/schemas/CreateFunctionToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelCalendarEventCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactCreateToolDTO'
        - $ref: '#/components/schemas/CreateGoHighLevelContactGetToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCheckAvailabilityToolDTO'
        - $ref: '#/components/schemas/CreateGoogleCalendarCreateEventToolDTO'
        - $ref: '#/components/schemas/CreateGoogleSheetsRowAppendToolDTO'
        - $ref: '#/components/schemas/CreateHandoffToolDTO'
        - $ref: '#/components/schemas/CreateMcpToolDTO'
        - $ref: '#/components/schemas/CreateQueryToolDTO'
        - $ref: '#/components/schemas/CreateSlackSendMessageToolDTO'
        - $ref: '#/components/schemas/CreateSmsToolDTO'
        - $ref: '#/components/schemas/CreateTextEditorToolDTO'
        - $ref: '#/components/schemas/CreateTransferCallToolDTO'
        - $ref: '#/components/schemas/CreateSipRequestToolDTO'
        - $ref: '#/components/schemas/CreateVoicemailToolDTO'
    AnthropicModelKnowledgeBase:
      oneOf:
        - $ref: '#/components/schemas/CreateCustomKnowledgeBaseDTO'
    AnthropicModelModel:
      type: string
      enum:
        - value: claude-3-opus-20240229
        - value: claude-3-sonnet-20240229
        - value: claude-3-haiku-20240307
        - value: claude-3-5-sonnet-20240620
        - value: claude-3-5-sonnet-20241022
        - value: claude-3-5-haiku-20241022
        - value: claude-3-7-sonnet-20250219
        - value: claude-opus-4-20250514
        - value: claude-opus-4-5-20251101
        - value: claude-opus-4-6
        - value: claude-sonnet-4-20250514
        - value: claude-sonnet-4-5-20250929
        - value: claude-haiku-4-5-20251001
    AnthropicModelProvider:
      type: string
      enum:
        - value: anthropic
    AnthropicModel:
      type: object
      properties:
        messages:
          type: array
          items:
            $ref: '#/components/schemas/OpenAIMessage'
          description: This is the starting state for the conversation.
        tools:
          type: array
          items:
            $ref: '#/components/schemas/AnthropicModelToolsItems'
          description: >-
            These are the tools that the assistant can use during the call. To
            use existing tools, use `toolIds`.


            Both `tools` and `toolIds` can be used together.
        toolIds:
          type: array
          items:
            type: string
          description: >-
            These are the tools that the assistant can use during the call. To
            use transient tools, use `tools`.


            Both `tools` and `toolIds` can be used together.
        knowledgeBase:
          $ref: '#/components/schemas/AnthropicModelKnowledgeBase'
          description: These are the options for the knowledge base.
        model:
          $ref: '#/components/schemas/AnthropicModelModel'
          description: The specific Anthropic/Claude model that will be used.
        provider:
          $ref: '#/components/schemas/AnthropicModelProvider'
          description: The provider identifier for Anthropic.
        thinking:
          $ref: '#/components/schemas/AnthropicThinkingConfig'
          description: |-
            Optional configuration for Anthropic's thinking feature.
            Only applicable for claude-3-7-sonnet-20250219 model.
            If provided, maxTokens must be greater than thinking.budgetTokens.
        temperature:
          type: number
          format: double
          description: >-
            This is the temperature that will be used for calls. Default is 0 to
            leverage caching for lower latency.
        maxTokens:
          type: number
          format: double
          description: >-
            This is the max number of tokens that the assistant will be allowed
            to generate in each turn of the conversation. Default is 250.
        emotionRecognitionEnabled:
          type: boolean
          description: >-
            This determines whether we detect user's emotion while they speak
            and send it as an additional info to model.


            Default `false` because the model is usually are good at
            understanding the user's emotion from text.


            @default false
        numFastTurns:
          type: number
          format: double
          description: >-
            This sets how many turns at the start of the conversation to use a
            smaller, faster model from the same provider before switching to the
            primary model. Example, gpt-3.5-turbo if provider is openai.


            Default is 0.


            @default 0
      required:
        - model
        - provider
    CreateAssistantDtoModel:
      oneOf:
        - $ref: '#/components/schemas/AnthropicModel'
        - $ref: '#/components/schemas/AnthropicBedrockModel'
        - $ref: '#/components/schemas/AnyscaleModel'
        - $ref: '#/components/schemas/CerebrasModel'
        - $ref: '#/components/schemas/CustomLLMModel'
        - $ref: '#/components/schemas/DeepInfraModel'
        - $ref: '#/components/schemas/DeepSeekModel'
        - $ref: '#/components/schemas/GoogleModel'
        - $ref: '#/components/schemas/GroqModel'
        - $ref: '#/components/schemas/InflectionAIModel'
        - $ref: '#/components/schemas/OpenAIModel'
        - $ref: '#/components/schemas/OpenRouterModel'
        - $ref: '#/components/schemas/PerplexityAIModel'
        - $ref: '#/components/schemas/TogetherAIModel'
        - $ref: '#/components/schemas/XaiModel'
    CreateAssistantDtoVoice:
      oneOf:
        - $ref: '#/components/schemas/AzureVoice'
        - $ref: '#/components/schemas/CartesiaVoice'
        - $ref: '#/components/schemas/CustomVoice'
        - $ref: '#/components/schemas/DeepgramVoice'
        - $ref: '#/components/schemas/ElevenLabsVoice'
        - $ref: '#/components/schemas/HumeVoice'
        - $ref: '#/components/schemas/LMNTVoice'
        - $ref: '#/components/schemas/NeuphonicVoice'
        - $ref: '#/components/schemas/OpenAIVoice'
        - $ref: '#/components/schemas/PlayHTVoice'
        - $ref: '#/components/schemas/WellSaidVoice'
        - $ref: '#/components/schemas/RimeAIVoice'
        - $ref: '#/components/schemas/SmallestAIVoice'
        - $ref: '#/components/schemas/TavusVoice'
        - $ref: '#/components/schemas/VapiVoice'
        - $ref: '#/components/schemas/SesameVoice'
        - $ref: '#/components/schemas/InworldVoice'
        - $ref: '#/components/schemas/MinimaxVoice'
    CreateAssistantDtoFirstMessageMode:
      type: string
      enum:
        - value: assistant-speaks-first
        - value: assistant-speaks-first-with-model-generated-message
        - value: assistant-waits-for-user
    CreateAssistantDtoVoicemailDetection0:
      type: string
      enum:
        - value: 'off'
    CreateAssistantDtoVoicemailDetection:
      oneOf:
        - $ref: '#/components/schemas/CreateAssistantDtoVoicemailDetection0'
        - $ref: '#/components/schemas/GoogleVoicemailDetectionPlan'
        - $ref: '#/components/schemas/OpenAIVoicemailDetectionPlan'
        - $ref: '#/components/schemas/TwilioVoicemailDetectionPlan'
        - $ref: '#/components/schemas/VapiVoicemailDetectionPlan'
    CreateAssistantDtoClientMessages:
      type: string
      enum:
        - value: conversation-update
        - value: function-call
        - value: function-call-result
        - value: hang
        - value: language-changed
        - value: metadata
        - value: model-output
        - value: speech-update
        - value: status-update
        - value: transcript
        - value: tool-calls
        - value: tool-calls-result
        - value: tool.completed
        - value: transfer-update
        - value: user-interrupted
        - value: voice-input
        - value: workflow.node.started
        - value: assistant.started
    CreateAssistantDtoServerMessages:
      type: string
      enum:
        - value: assistant.started
        - value: conversation-update
        - value: end-of-call-report
        - value: function-call
        - value: hang
        - value: language-changed
        - value: language-change-detected
        - value: model-output
        - value: phone-call-control
        - value: speech-update
        - value: status-update
        - value: transcript
        - value: transcript[transcriptType="final"]
        - value: tool-calls
        - value: transfer-destination-request
        - value: handoff-destination-request
        - value: transfer-update
        - value: user-interrupted
        - value: voice-input
        - value: chat.created
        - value: chat.deleted
        - value: session.created
        - value: session.updated
        - value: session.deleted
        - value: call.deleted
        - value: call.delete.failed
    CreateAssistantDtoBackgroundSound0:
      type: string
      enum:
        - value: 'off'
        - value: office
    CreateAssistantDtoBackgroundSound:
      oneOf:
        - $ref: '#/components/schemas/CreateAssistantDtoBackgroundSound0'
        - type: string
          format: uri
    CreateAssistantDtoTransportConfigurationsItems:
      oneOf:
        - $ref: '#/components/schemas/TransportConfigurationTwilio'
    CreateAssistantDtoCredentialsItems:
      oneOf:
        - type: object
          properties:
            provider:
              type: string
              enum:
                - 11labs
              description: 'Discriminator value: 11labs'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: 11labs variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - anthropic
              description: 'Discriminator value: anthropic'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: anthropic variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - anthropic-bedrock
              description: 'Discriminator value: anthropic-bedrock'
            region:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAnthropicBedrockRegion
              description: AWS region where Bedrock is configured.
            authenticationPlan:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAnthropicBedrockAuthenticationPlan
              description: >-
                Authentication method - either direct IAM credentials or
                cross-account role assumption.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - region
            - authenticationPlan
          description: anthropic-bedrock variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - anyscale
              description: 'Discriminator value: anyscale'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: anyscale variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - assembly-ai
              description: 'Discriminator value: assembly-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: assembly-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - azure-openai
              description: 'Discriminator value: azure-openai'
            region:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureOpenaiRegion
            models:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureOpenaiModels
            openAIKey:
              type: string
              description: This is not returned in the API.
            ocpApimSubscriptionKey:
              type: string
              description: This is not returned in the API.
            openAIEndpoint:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - region
            - models
            - openAIKey
            - openAIEndpoint
          description: azure-openai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - azure
              description: 'Discriminator value: azure'
            service:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureService
              description: This is the service being used in Azure.
            region:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureRegion
              description: This is the region of the Azure resource.
            apiKey:
              type: string
              description: This is not returned in the API.
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            bucketPlan:
              $ref: '#/components/schemas/AzureBlobStorageBucketPlan'
              description: >-
                This is the bucket plan that can be provided to store call
                artifacts in Azure Blob Storage.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - service
          description: azure variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - byo-sip-trunk
              description: 'Discriminator value: byo-sip-trunk'
            gateways:
              type: array
              items:
                $ref: '#/components/schemas/SipTrunkGateway'
              description: This is the list of SIP trunk's gateways.
            outboundAuthenticationPlan:
              $ref: '#/components/schemas/SipTrunkOutboundAuthenticationPlan'
              description: >-
                This can be used to configure the outbound authentication if
                required by the SIP trunk.
            outboundLeadingPlusEnabled:
              type: boolean
              description: >-
                This ensures the outbound origination attempts have a leading
                plus. Defaults to false to match conventional telecom behavior.


                Usage:

                - Vonage/Twilio requires leading plus for all outbound calls.
                Set this to true.


                @default false
            techPrefix:
              type: string
              description: >-
                This can be used to configure the tech prefix on outbound calls.
                This is an advanced property.
            sipDiversionHeader:
              type: string
              description: >-
                This can be used to enable the SIP diversion header for
                authenticating the calling number if the SIP trunk supports it.
                This is an advanced property.
            sbcConfiguration:
              $ref: '#/components/schemas/SbcConfiguration'
              description: >-
                This is an advanced configuration for enterprise deployments.
                This uses the onprem SBC to trunk into the SIP trunk's
                `gateways`, rather than the managed SBC provided by Vapi.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - gateways
          description: byo-sip-trunk variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - cartesia
              description: 'Discriminator value: cartesia'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: cartesia variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateCerebrasCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: cerebras variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - cloudflare
              description: 'Discriminator value: cloudflare'
            accountId:
              type: string
              description: Cloudflare Account Id.
            apiKey:
              type: string
              description: Cloudflare API Key / Token.
            accountEmail:
              type: string
              description: Cloudflare Account Email.
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            bucketPlan:
              $ref: '#/components/schemas/CloudflareR2BucketPlan'
              description: >-
                This is the bucket plan that can be provided to store call
                artifacts in R2
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
          description: cloudflare variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - custom-llm
              description: 'Discriminator value: custom-llm'
            apiKey:
              type: string
              description: This is not returned in the API.
            authenticationPlan:
              $ref: '#/components/schemas/OAuth2AuthenticationPlan'
              description: >-
                This is the authentication plan. Currently supports OAuth2 RFC
                6749. To use Bearer authentication, use apiKey
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: custom-llm variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - deepgram
              description: 'Discriminator value: deepgram'
            apiKey:
              type: string
              description: This is not returned in the API.
            apiUrl:
              type: string
              description: >-
                This can be used to point to an onprem Deepgram instance.
                Defaults to api.deepgram.com.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: deepgram variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - deepinfra
              description: 'Discriminator value: deepinfra'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: deepinfra variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - deep-seek
              description: 'Discriminator value: deep-seek'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: deep-seek variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - gcp
              description: 'Discriminator value: gcp'
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            gcpKey:
              $ref: '#/components/schemas/GcpKey'
              description: >-
                This is the GCP key. This is the JSON that can be generated in
                the Google Cloud Console at
                https://console.cloud.google.com/iam-admin/serviceaccounts/details/<service-account-id>/keys.


                The schema is identical to the JSON that GCP outputs.
            region:
              type: string
              description: This is the region of the GCP resource.
            bucketPlan:
              $ref: '#/components/schemas/BucketPlan'
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - gcpKey
          description: gcp variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - gladia
              description: 'Discriminator value: gladia'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: gladia variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - gohighlevel
              description: 'Discriminator value: gohighlevel'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: gohighlevel variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateGoogleCredentialDtoProvider'
              description: >-
                This is the key for Gemini in Google AI Studio. Get it from
                here: https://aistudio.google.com/app/apikey
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: google variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - groq
              description: 'Discriminator value: groq'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: groq variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateInflectionAiCredentialDtoProvider'
              description: >-
                This is the api key for Pi in InflectionAI's console. Get it
                from here: https://developers.inflection.ai/keys, billing will
                need to be setup
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: inflection-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - langfuse
              description: 'Discriminator value: langfuse'
            publicKey:
              type: string
              description: 'The public key for Langfuse project. Eg: pk-lf-...'
            apiKey:
              type: string
              description: >-
                The secret key for Langfuse project. Eg: sk-lf-... .This is not
                returned in the API.
            apiUrl:
              type: string
              description: >-
                The host URL for Langfuse project. Eg:
                https://cloud.langfuse.com
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - publicKey
            - apiKey
            - apiUrl
          description: langfuse variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - lmnt
              description: 'Discriminator value: lmnt'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: lmnt variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - make
              description: 'Discriminator value: make'
            teamId:
              type: string
              description: Team ID
            region:
              type: string
              description: 'Region of your application. For example: eu1, eu2, us1, us2'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - teamId
            - region
            - apiKey
          description: make variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - openai
              description: 'Discriminator value: openai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: openai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - openrouter
              description: 'Discriminator value: openrouter'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: openrouter variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - perplexity-ai
              description: 'Discriminator value: perplexity-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: perplexity-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - playht
              description: 'Discriminator value: playht'
            apiKey:
              type: string
              description: This is not returned in the API.
            userId:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
            - userId
          description: playht variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - rime-ai
              description: 'Discriminator value: rime-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: rime-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - runpod
              description: 'Discriminator value: runpod'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: runpod variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - s3
              description: 'Discriminator value: s3'
            awsAccessKeyId:
              type: string
              description: AWS access key ID.
            awsSecretAccessKey:
              type: string
              description: AWS access key secret. This is not returned in the API.
            region:
              type: string
              description: AWS region in which the S3 bucket is located.
            s3BucketName:
              type: string
              description: AWS S3 bucket name.
            s3PathPrefix:
              type: string
              description: The path prefix for the uploaded recording. Ex. "recordings/"
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - awsAccessKeyId
            - awsSecretAccessKey
            - region
            - s3BucketName
            - s3PathPrefix
          description: s3 variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - supabase
              description: 'Discriminator value: supabase'
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            bucketPlan:
              $ref: '#/components/schemas/SupabaseBucketPlan'
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
          description: supabase variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - smallest-ai
              description: 'Discriminator value: smallest-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: smallest-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - tavus
              description: 'Discriminator value: tavus'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: tavus variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - together-ai
              description: 'Discriminator value: together-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: together-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - twilio
              description: 'Discriminator value: twilio'
            authToken:
              type: string
              description: This is not returned in the API.
            apiKey:
              type: string
              description: This is not returned in the API.
            apiSecret:
              type: string
              description: This is not returned in the API.
            accountSid:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - accountSid
          description: twilio variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - vonage
              description: 'Discriminator value: vonage'
            apiSecret:
              type: string
              description: This is not returned in the API.
            apiKey:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiSecret
            - apiKey
          description: vonage variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - webhook
              description: 'Discriminator value: webhook'
            authenticationPlan:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingWebhookAuthenticationPlan
              description: >-
                This is the authentication plan. Supports OAuth2 RFC 6749, HMAC
                signing, and Bearer authentication.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authenticationPlan
          description: webhook variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateCustomCredentialDtoProvider'
            authenticationPlan:
              $ref: '#/components/schemas/CreateCustomCredentialDtoAuthenticationPlan'
              description: >-
                This is the authentication plan. Supports OAuth2 RFC 6749, HMAC
                signing, and Bearer authentication.
            encryptionPlan:
              $ref: '#/components/schemas/PublicKeyEncryptionPlan'
              description: >-
                This is the encryption plan for encrypting sensitive data.
                Currently supports public-key encryption.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authenticationPlan
          description: custom-credential variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - xai
              description: 'Discriminator value: xai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: xai variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateNeuphonicCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: neuphonic variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateHumeCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: hume variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateMistralCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: mistral variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateSpeechmaticsCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: speechmatics variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateTrieveCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: trieve variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - google.calendar.oauth2-client
              description: 'Discriminator value: google.calendar.oauth2-client'
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
          description: google.calendar.oauth2-client variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - google.calendar.oauth2-authorization
              description: 'Discriminator value: google.calendar.oauth2-authorization'
            authorizationId:
              type: string
              description: The authorization ID for the OAuth2 authorization
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authorizationId
          description: google.calendar.oauth2-authorization variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - google.sheets.oauth2-authorization
              description: 'Discriminator value: google.sheets.oauth2-authorization'
            authorizationId:
              type: string
              description: The authorization ID for the OAuth2 authorization
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authorizationId
          description: google.sheets.oauth2-authorization variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - slack.oauth2-authorization
              description: 'Discriminator value: slack.oauth2-authorization'
            authorizationId:
              type: string
              description: The authorization ID for the OAuth2 authorization
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authorizationId
          description: slack.oauth2-authorization variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateGoHighLevelMcpCredentialDtoProvider'
            authenticationSession:
              $ref: '#/components/schemas/Oauth2AuthenticationSession'
              description: This is the authentication session for the credential.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authenticationSession
          description: ghl.oauth2-authorization variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateInworldCredentialDtoProvider'
            apiKey:
              type: string
              description: >-
                This is the Inworld Basic (Base64) authentication token. This is
                not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: inworld variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - minimax
              description: 'Discriminator value: minimax'
            apiKey:
              type: string
              description: This is not returned in the API.
            groupId:
              type: string
              description: This is the Minimax Group ID.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
            - groupId
          description: minimax variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateWellSaidCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: wellsaid variant
      discriminator:
        propertyName: provider
    CreateAssistantDtoHooksItems:
      oneOf:
        - $ref: '#/components/schemas/CallHookCallEnding'
        - $ref: '#/components/schemas/CallHookAssistantSpeechInterrupted'
        - $ref: '#/components/schemas/CallHookCustomerSpeechInterrupted'
        - $ref: '#/components/schemas/CallHookCustomerSpeechTimeout'
    CreateAssistantDtoMetadata:
      type: object
      properties: {}
    CreateAssistantDTO:
      type: object
      properties:
        transcriber:
          $ref: '#/components/schemas/CreateAssistantDtoTranscriber'
          description: These are the options for the assistant's transcriber.
        model:
          $ref: '#/components/schemas/CreateAssistantDtoModel'
          description: These are the options for the assistant's LLM.
        voice:
          $ref: '#/components/schemas/CreateAssistantDtoVoice'
          description: These are the options for the assistant's voice.
        firstMessage:
          type: string
          description: >-
            This is the first message that the assistant will say. This can also
            be a URL to a containerized audio file (mp3, wav, etc.).


            If unspecified, assistant will wait for user to speak and use the
            model to respond once they speak.
        firstMessageInterruptionsEnabled:
          type: boolean
          default: false
        firstMessageMode:
          $ref: '#/components/schemas/CreateAssistantDtoFirstMessageMode'
          description: >-
            This is the mode for the first message. Default is
            'assistant-speaks-first'.


            Use:

            - 'assistant-speaks-first' to have the assistant speak first.

            - 'assistant-waits-for-user' to have the assistant wait for the user
            to speak first.

            - 'assistant-speaks-first-with-model-generated-message' to have the
            assistant speak first with a message generated by the model based on
            the conversation state. (`assistant.model.messages` at call start,
            `call.messages` at squad transfer points).


            @default 'assistant-speaks-first'
        voicemailDetection:
          $ref: '#/components/schemas/CreateAssistantDtoVoicemailDetection'
          description: >-
            These are the settings to configure or disable voicemail detection.
            Alternatively, voicemail detection can be configured using the
            model.tools=[VoicemailTool].

            By default, voicemail detection is disabled.
        clientMessages:
          $ref: '#/components/schemas/CreateAssistantDtoClientMessages'
          description: >-
            These are the messages that will be sent to your Client SDKs.
            Default is
            conversation-update,function-call,hang,model-output,speech-update,status-update,transfer-update,transcript,tool-calls,user-interrupted,voice-input,workflow.node.started,assistant.started.
            You can check the shape of the messages in ClientMessage schema.
        serverMessages:
          $ref: '#/components/schemas/CreateAssistantDtoServerMessages'
          description: >-
            These are the messages that will be sent to your Server URL. Default
            is
            conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,handoff-destination-request,user-interrupted,assistant.started.
            You can check the shape of the messages in ServerMessage schema.
        maxDurationSeconds:
          type: number
          format: double
          description: >-
            This is the maximum number of seconds that the call will last. When
            the call reaches this duration, it will be ended.


            @default 600 (10 minutes)
        backgroundSound:
          $ref: '#/components/schemas/CreateAssistantDtoBackgroundSound'
          description: >-
            This is the background sound in the call. Default for phone calls is
            'office' and default for web calls is 'off'.

            You can also provide a custom sound by providing a URL to an audio
            file.
        modelOutputInMessagesEnabled:
          type: boolean
          description: >-
            This determines whether the model's output is used in conversation
            history rather than the transcription of assistant's speech.


            Default `false` while in beta.


            @default false
        transportConfigurations:
          type: array
          items:
            $ref: >-
              #/components/schemas/CreateAssistantDtoTransportConfigurationsItems
          description: >-
            These are the configurations to be passed to the transport providers
            of assistant's calls, like Twilio. You can store multiple
            configurations for different transport providers. For a call, only
            the configuration matching the call transport provider is used.
        observabilityPlan:
          $ref: '#/components/schemas/LangfuseObservabilityPlan'
          description: |-
            This is the plan for observability of assistant's calls.

            Currently, only Langfuse is supported.
        credentials:
          type: array
          items:
            $ref: '#/components/schemas/CreateAssistantDtoCredentialsItems'
          description: >-
            These are dynamic credentials that will be used for the assistant
            calls. By default, all the credentials are available for use in the
            call but you can supplement an additional credentials using this.
            Dynamic credentials override existing credentials.
        hooks:
          type: array
          items:
            $ref: '#/components/schemas/CreateAssistantDtoHooksItems'
          description: This is a set of actions that will be performed on certain events.
        name:
          type: string
          description: >-
            This is the name of the assistant.


            This is required when you want to transfer between assistants in a
            call.
        voicemailMessage:
          type: string
          description: >-
            This is the message that the assistant will say if the call is
            forwarded to voicemail.


            If unspecified, it will hang up.
        endCallMessage:
          type: string
          description: |-
            This is the message that the assistant will say if it ends the call.

            If unspecified, it will hang up without saying anything.
        endCallPhrases:
          type: array
          items:
            type: string
          description: >-
            This list contains phrases that, if spoken by the assistant, will
            trigger the call to be hung up. Case insensitive.
        compliancePlan:
          $ref: '#/components/schemas/CompliancePlan'
        metadata:
          $ref: '#/components/schemas/CreateAssistantDtoMetadata'
          description: This is for metadata you want to store on the assistant.
        backgroundSpeechDenoisingPlan:
          $ref: '#/components/schemas/BackgroundSpeechDenoisingPlan'
          description: >-
            This enables filtering of noise and background speech while the user
            is talking.


            Features:

            - Smart denoising using Krisp

            - Fourier denoising


            Smart denoising can be combined with or used independently of
            Fourier denoising.


            Order of precedence:

            - Smart denoising

            - Fourier denoising
        analysisPlan:
          $ref: '#/components/schemas/AnalysisPlan'
          description: >-
            This is the plan for analysis of assistant's calls. Stored in
            `call.analysis`.
        artifactPlan:
          $ref: '#/components/schemas/ArtifactPlan'
          description: >-
            This is the plan for artifacts generated during assistant's calls.
            Stored in `call.artifact`.
        startSpeakingPlan:
          $ref: '#/components/schemas/StartSpeakingPlan'
          description: >-
            This is the plan for when the assistant should start talking.


            You should configure this if you're running into these issues:

            - The assistant is too slow to start talking after the customer is
            done speaking.

            - The assistant is too fast to start talking after the customer is
            done speaking.

            - The assistant is so fast that it's actually interrupting the
            customer.
        stopSpeakingPlan:
          $ref: '#/components/schemas/StopSpeakingPlan'
          description: >-
            This is the plan for when assistant should stop talking on customer
            interruption.


            You should configure this if you're running into these issues:

            - The assistant is too slow to recognize customer's interruption.

            - The assistant is too fast to recognize customer's interruption.

            - The assistant is getting interrupted by phrases that are just
            acknowledgments.

            - The assistant is getting interrupted by background noises.

            - The assistant is not properly stopping -- it starts talking right
            after getting interrupted.
        monitorPlan:
          $ref: '#/components/schemas/MonitorPlan'
          description: >-
            This is the plan for real-time monitoring of the assistant's calls.


            Usage:

            - To enable live listening of the assistant's calls, set
            `monitorPlan.listenEnabled` to `true`.

            - To enable live control of the assistant's calls, set
            `monitorPlan.controlEnabled` to `true`.

            - To attach monitors to the assistant, set `monitorPlan.monitorIds`
            to the set of monitor ids.
        credentialIds:
          type: array
          items:
            type: string
          description: >-
            These are the credentials that will be used for the assistant calls.
            By default, all the credentials are available for use in the call
            but you can provide a subset using this.
        server:
          $ref: '#/components/schemas/Server'
          description: >-
            This is where Vapi will send webhooks. You can find all webhooks
            available along with their shape in ServerMessage schema.


            The order of precedence is:


            1. assistant.server.url

            2. phoneNumber.serverUrl

            3. org.serverUrl
        keypadInputPlan:
          $ref: '#/components/schemas/KeypadInputPlan'
    AssistantTranscriber:
      oneOf:
        - $ref: '#/components/schemas/AssemblyAITranscriber'
        - $ref: '#/components/schemas/AzureSpeechTranscriber'
        - $ref: '#/components/schemas/CustomTranscriber'
        - $ref: '#/components/schemas/DeepgramTranscriber'
        - $ref: '#/components/schemas/ElevenLabsTranscriber'
        - $ref: '#/components/schemas/GladiaTranscriber'
        - $ref: '#/components/schemas/GoogleTranscriber'
        - $ref: '#/components/schemas/SpeechmaticsTranscriber'
        - $ref: '#/components/schemas/TalkscriberTranscriber'
        - $ref: '#/components/schemas/OpenAITranscriber'
        - $ref: '#/components/schemas/CartesiaTranscriber'
    AssistantModel:
      oneOf:
        - $ref: '#/components/schemas/AnthropicModel'
        - $ref: '#/components/schemas/AnthropicBedrockModel'
        - $ref: '#/components/schemas/AnyscaleModel'
        - $ref: '#/components/schemas/CerebrasModel'
        - $ref: '#/components/schemas/CustomLLMModel'
        - $ref: '#/components/schemas/DeepInfraModel'
        - $ref: '#/components/schemas/DeepSeekModel'
        - $ref: '#/components/schemas/GoogleModel'
        - $ref: '#/components/schemas/GroqModel'
        - $ref: '#/components/schemas/InflectionAIModel'
        - $ref: '#/components/schemas/OpenAIModel'
        - $ref: '#/components/schemas/OpenRouterModel'
        - $ref: '#/components/schemas/PerplexityAIModel'
        - $ref: '#/components/schemas/TogetherAIModel'
        - $ref: '#/components/schemas/XaiModel'
    AssistantVoice:
      oneOf:
        - $ref: '#/components/schemas/AzureVoice'
        - $ref: '#/components/schemas/CartesiaVoice'
        - $ref: '#/components/schemas/CustomVoice'
        - $ref: '#/components/schemas/DeepgramVoice'
        - $ref: '#/components/schemas/ElevenLabsVoice'
        - $ref: '#/components/schemas/HumeVoice'
        - $ref: '#/components/schemas/LMNTVoice'
        - $ref: '#/components/schemas/NeuphonicVoice'
        - $ref: '#/components/schemas/OpenAIVoice'
        - $ref: '#/components/schemas/PlayHTVoice'
        - $ref: '#/components/schemas/WellSaidVoice'
        - $ref: '#/components/schemas/RimeAIVoice'
        - $ref: '#/components/schemas/SmallestAIVoice'
        - $ref: '#/components/schemas/TavusVoice'
        - $ref: '#/components/schemas/VapiVoice'
        - $ref: '#/components/schemas/SesameVoice'
        - $ref: '#/components/schemas/InworldVoice'
        - $ref: '#/components/schemas/MinimaxVoice'
    AssistantFirstMessageMode:
      type: string
      enum:
        - value: assistant-speaks-first
        - value: assistant-speaks-first-with-model-generated-message
        - value: assistant-waits-for-user
    AssistantVoicemailDetection0:
      type: string
      enum:
        - value: 'off'
    AssistantVoicemailDetection:
      oneOf:
        - $ref: '#/components/schemas/AssistantVoicemailDetection0'
        - $ref: '#/components/schemas/GoogleVoicemailDetectionPlan'
        - $ref: '#/components/schemas/OpenAIVoicemailDetectionPlan'
        - $ref: '#/components/schemas/TwilioVoicemailDetectionPlan'
        - $ref: '#/components/schemas/VapiVoicemailDetectionPlan'
    AssistantClientMessages:
      type: string
      enum:
        - value: conversation-update
        - value: function-call
        - value: function-call-result
        - value: hang
        - value: language-changed
        - value: metadata
        - value: model-output
        - value: speech-update
        - value: status-update
        - value: transcript
        - value: tool-calls
        - value: tool-calls-result
        - value: tool.completed
        - value: transfer-update
        - value: user-interrupted
        - value: voice-input
        - value: workflow.node.started
        - value: assistant.started
    AssistantServerMessages:
      type: string
      enum:
        - value: assistant.started
        - value: conversation-update
        - value: end-of-call-report
        - value: function-call
        - value: hang
        - value: language-changed
        - value: language-change-detected
        - value: model-output
        - value: phone-call-control
        - value: speech-update
        - value: status-update
        - value: transcript
        - value: transcript[transcriptType="final"]
        - value: tool-calls
        - value: transfer-destination-request
        - value: handoff-destination-request
        - value: transfer-update
        - value: user-interrupted
        - value: voice-input
        - value: chat.created
        - value: chat.deleted
        - value: session.created
        - value: session.updated
        - value: session.deleted
        - value: call.deleted
        - value: call.delete.failed
    AssistantBackgroundSound0:
      type: string
      enum:
        - value: 'off'
        - value: office
    AssistantBackgroundSound:
      oneOf:
        - $ref: '#/components/schemas/AssistantBackgroundSound0'
        - type: string
          format: uri
    AssistantTransportConfigurationsItems:
      oneOf:
        - $ref: '#/components/schemas/TransportConfigurationTwilio'
    AssistantCredentialsItems:
      oneOf:
        - type: object
          properties:
            provider:
              type: string
              enum:
                - 11labs
              description: 'Discriminator value: 11labs'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: 11labs variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - anthropic
              description: 'Discriminator value: anthropic'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: anthropic variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - anthropic-bedrock
              description: 'Discriminator value: anthropic-bedrock'
            region:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAnthropicBedrockRegion
              description: AWS region where Bedrock is configured.
            authenticationPlan:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAnthropicBedrockAuthenticationPlan
              description: >-
                Authentication method - either direct IAM credentials or
                cross-account role assumption.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - region
            - authenticationPlan
          description: anthropic-bedrock variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - anyscale
              description: 'Discriminator value: anyscale'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: anyscale variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - assembly-ai
              description: 'Discriminator value: assembly-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: assembly-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - azure-openai
              description: 'Discriminator value: azure-openai'
            region:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureOpenaiRegion
            models:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureOpenaiModels
            openAIKey:
              type: string
              description: This is not returned in the API.
            ocpApimSubscriptionKey:
              type: string
              description: This is not returned in the API.
            openAIEndpoint:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - region
            - models
            - openAIKey
            - openAIEndpoint
          description: azure-openai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - azure
              description: 'Discriminator value: azure'
            service:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureService
              description: This is the service being used in Azure.
            region:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingAzureRegion
              description: This is the region of the Azure resource.
            apiKey:
              type: string
              description: This is not returned in the API.
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            bucketPlan:
              $ref: '#/components/schemas/AzureBlobStorageBucketPlan'
              description: >-
                This is the bucket plan that can be provided to store call
                artifacts in Azure Blob Storage.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - service
          description: azure variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - byo-sip-trunk
              description: 'Discriminator value: byo-sip-trunk'
            gateways:
              type: array
              items:
                $ref: '#/components/schemas/SipTrunkGateway'
              description: This is the list of SIP trunk's gateways.
            outboundAuthenticationPlan:
              $ref: '#/components/schemas/SipTrunkOutboundAuthenticationPlan'
              description: >-
                This can be used to configure the outbound authentication if
                required by the SIP trunk.
            outboundLeadingPlusEnabled:
              type: boolean
              description: >-
                This ensures the outbound origination attempts have a leading
                plus. Defaults to false to match conventional telecom behavior.


                Usage:

                - Vonage/Twilio requires leading plus for all outbound calls.
                Set this to true.


                @default false
            techPrefix:
              type: string
              description: >-
                This can be used to configure the tech prefix on outbound calls.
                This is an advanced property.
            sipDiversionHeader:
              type: string
              description: >-
                This can be used to enable the SIP diversion header for
                authenticating the calling number if the SIP trunk supports it.
                This is an advanced property.
            sbcConfiguration:
              $ref: '#/components/schemas/SbcConfiguration'
              description: >-
                This is an advanced configuration for enterprise deployments.
                This uses the onprem SBC to trunk into the SIP trunk's
                `gateways`, rather than the managed SBC provided by Vapi.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - gateways
          description: byo-sip-trunk variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - cartesia
              description: 'Discriminator value: cartesia'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: cartesia variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateCerebrasCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: cerebras variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - cloudflare
              description: 'Discriminator value: cloudflare'
            accountId:
              type: string
              description: Cloudflare Account Id.
            apiKey:
              type: string
              description: Cloudflare API Key / Token.
            accountEmail:
              type: string
              description: Cloudflare Account Email.
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            bucketPlan:
              $ref: '#/components/schemas/CloudflareR2BucketPlan'
              description: >-
                This is the bucket plan that can be provided to store call
                artifacts in R2
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
          description: cloudflare variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - custom-llm
              description: 'Discriminator value: custom-llm'
            apiKey:
              type: string
              description: This is not returned in the API.
            authenticationPlan:
              $ref: '#/components/schemas/OAuth2AuthenticationPlan'
              description: >-
                This is the authentication plan. Currently supports OAuth2 RFC
                6749. To use Bearer authentication, use apiKey
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: custom-llm variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - deepgram
              description: 'Discriminator value: deepgram'
            apiKey:
              type: string
              description: This is not returned in the API.
            apiUrl:
              type: string
              description: >-
                This can be used to point to an onprem Deepgram instance.
                Defaults to api.deepgram.com.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: deepgram variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - deepinfra
              description: 'Discriminator value: deepinfra'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: deepinfra variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - deep-seek
              description: 'Discriminator value: deep-seek'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: deep-seek variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - gcp
              description: 'Discriminator value: gcp'
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            gcpKey:
              $ref: '#/components/schemas/GcpKey'
              description: >-
                This is the GCP key. This is the JSON that can be generated in
                the Google Cloud Console at
                https://console.cloud.google.com/iam-admin/serviceaccounts/details/<service-account-id>/keys.


                The schema is identical to the JSON that GCP outputs.
            region:
              type: string
              description: This is the region of the GCP resource.
            bucketPlan:
              $ref: '#/components/schemas/BucketPlan'
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - gcpKey
          description: gcp variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - gladia
              description: 'Discriminator value: gladia'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: gladia variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - gohighlevel
              description: 'Discriminator value: gohighlevel'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: gohighlevel variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateGoogleCredentialDtoProvider'
              description: >-
                This is the key for Gemini in Google AI Studio. Get it from
                here: https://aistudio.google.com/app/apikey
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: google variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - groq
              description: 'Discriminator value: groq'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: groq variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateInflectionAiCredentialDtoProvider'
              description: >-
                This is the api key for Pi in InflectionAI's console. Get it
                from here: https://developers.inflection.ai/keys, billing will
                need to be setup
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: inflection-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - langfuse
              description: 'Discriminator value: langfuse'
            publicKey:
              type: string
              description: 'The public key for Langfuse project. Eg: pk-lf-...'
            apiKey:
              type: string
              description: >-
                The secret key for Langfuse project. Eg: sk-lf-... .This is not
                returned in the API.
            apiUrl:
              type: string
              description: >-
                The host URL for Langfuse project. Eg:
                https://cloud.langfuse.com
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - publicKey
            - apiKey
            - apiUrl
          description: langfuse variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - lmnt
              description: 'Discriminator value: lmnt'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: lmnt variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - make
              description: 'Discriminator value: make'
            teamId:
              type: string
              description: Team ID
            region:
              type: string
              description: 'Region of your application. For example: eu1, eu2, us1, us2'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - teamId
            - region
            - apiKey
          description: make variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - openai
              description: 'Discriminator value: openai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: openai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - openrouter
              description: 'Discriminator value: openrouter'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: openrouter variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - perplexity-ai
              description: 'Discriminator value: perplexity-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: perplexity-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - playht
              description: 'Discriminator value: playht'
            apiKey:
              type: string
              description: This is not returned in the API.
            userId:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
            - userId
          description: playht variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - rime-ai
              description: 'Discriminator value: rime-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: rime-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - runpod
              description: 'Discriminator value: runpod'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: runpod variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - s3
              description: 'Discriminator value: s3'
            awsAccessKeyId:
              type: string
              description: AWS access key ID.
            awsSecretAccessKey:
              type: string
              description: AWS access key secret. This is not returned in the API.
            region:
              type: string
              description: AWS region in which the S3 bucket is located.
            s3BucketName:
              type: string
              description: AWS S3 bucket name.
            s3PathPrefix:
              type: string
              description: The path prefix for the uploaded recording. Ex. "recordings/"
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - awsAccessKeyId
            - awsSecretAccessKey
            - region
            - s3BucketName
            - s3PathPrefix
          description: s3 variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - supabase
              description: 'Discriminator value: supabase'
            fallbackIndex:
              type: number
              format: double
              description: >-
                This is the order in which this storage provider is tried during
                upload retries. Lower numbers are tried first in increasing
                order.
            bucketPlan:
              $ref: '#/components/schemas/SupabaseBucketPlan'
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
          description: supabase variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - smallest-ai
              description: 'Discriminator value: smallest-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: smallest-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - tavus
              description: 'Discriminator value: tavus'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: tavus variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - together-ai
              description: 'Discriminator value: together-ai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: together-ai variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - twilio
              description: 'Discriminator value: twilio'
            authToken:
              type: string
              description: This is not returned in the API.
            apiKey:
              type: string
              description: This is not returned in the API.
            apiSecret:
              type: string
              description: This is not returned in the API.
            accountSid:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - accountSid
          description: twilio variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - vonage
              description: 'Discriminator value: vonage'
            apiSecret:
              type: string
              description: This is not returned in the API.
            apiKey:
              type: string
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiSecret
            - apiKey
          description: vonage variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - webhook
              description: 'Discriminator value: webhook'
            authenticationPlan:
              $ref: >-
                #/components/schemas/UpdateWorkflowDtoCredentialsItemsDiscriminatorMappingWebhookAuthenticationPlan
              description: >-
                This is the authentication plan. Supports OAuth2 RFC 6749, HMAC
                signing, and Bearer authentication.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authenticationPlan
          description: webhook variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateCustomCredentialDtoProvider'
            authenticationPlan:
              $ref: '#/components/schemas/CreateCustomCredentialDtoAuthenticationPlan'
              description: >-
                This is the authentication plan. Supports OAuth2 RFC 6749, HMAC
                signing, and Bearer authentication.
            encryptionPlan:
              $ref: '#/components/schemas/PublicKeyEncryptionPlan'
              description: >-
                This is the encryption plan for encrypting sensitive data.
                Currently supports public-key encryption.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authenticationPlan
          description: custom-credential variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - xai
              description: 'Discriminator value: xai'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: xai variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateNeuphonicCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: neuphonic variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateHumeCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: hume variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateMistralCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: mistral variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateSpeechmaticsCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: speechmatics variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateTrieveCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: trieve variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - google.calendar.oauth2-client
              description: 'Discriminator value: google.calendar.oauth2-client'
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
          description: google.calendar.oauth2-client variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - google.calendar.oauth2-authorization
              description: 'Discriminator value: google.calendar.oauth2-authorization'
            authorizationId:
              type: string
              description: The authorization ID for the OAuth2 authorization
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authorizationId
          description: google.calendar.oauth2-authorization variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - google.sheets.oauth2-authorization
              description: 'Discriminator value: google.sheets.oauth2-authorization'
            authorizationId:
              type: string
              description: The authorization ID for the OAuth2 authorization
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authorizationId
          description: google.sheets.oauth2-authorization variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - slack.oauth2-authorization
              description: 'Discriminator value: slack.oauth2-authorization'
            authorizationId:
              type: string
              description: The authorization ID for the OAuth2 authorization
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authorizationId
          description: slack.oauth2-authorization variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateGoHighLevelMcpCredentialDtoProvider'
            authenticationSession:
              $ref: '#/components/schemas/Oauth2AuthenticationSession'
              description: This is the authentication session for the credential.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - authenticationSession
          description: ghl.oauth2-authorization variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateInworldCredentialDtoProvider'
            apiKey:
              type: string
              description: >-
                This is the Inworld Basic (Base64) authentication token. This is
                not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: inworld variant
        - type: object
          properties:
            provider:
              type: string
              enum:
                - minimax
              description: 'Discriminator value: minimax'
            apiKey:
              type: string
              description: This is not returned in the API.
            groupId:
              type: string
              description: This is the Minimax Group ID.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
            - groupId
          description: minimax variant
        - type: object
          properties:
            provider:
              $ref: '#/components/schemas/CreateWellSaidCredentialDtoProvider'
            apiKey:
              type: string
              description: This is not returned in the API.
            name:
              type: string
              description: This is the name of credential. This is just for your reference.
          required:
            - provider
            - apiKey
          description: wellsaid variant
      discriminator:
        propertyName: provider
    AssistantHooksItems:
      oneOf:
        - $ref: '#/components/schemas/CallHookCallEnding'
        - $ref: '#/components/schemas/CallHookAssistantSpeechInterrupted'
        - $ref: '#/components/schemas/CallHookCustomerSpeechInterrupted'
        - $ref: '#/components/schemas/CallHookCustomerSpeechTimeout'
    AssistantMetadata:
      type: object
      properties: {}
    Assistant:
      type: object
      properties:
        transcriber:
          $ref: '#/components/schemas/AssistantTranscriber'
          description: These are the options for the assistant's transcriber.
        model:
          $ref: '#/components/schemas/AssistantModel'
          description: These are the options for the assistant's LLM.
        voice:
          $ref: '#/components/schemas/AssistantVoice'
          description: These are the options for the assistant's voice.
        firstMessage:
          type: string
          description: >-
            This is the first message that the assistant will say. This can also
            be a URL to a containerized audio file (mp3, wav, etc.).


            If unspecified, assistant will wait for user to speak and use the
            model to respond once they speak.
        firstMessageInterruptionsEnabled:
          type: boolean
          default: false
        firstMessageMode:
          $ref: '#/components/schemas/AssistantFirstMessageMode'
          description: >-
            This is the mode for the first message. Default is
            'assistant-speaks-first'.


            Use:

            - 'assistant-speaks-first' to have the assistant speak first.

            - 'assistant-waits-for-user' to have the assistant wait for the user
            to speak first.

            - 'assistant-speaks-first-with-model-generated-message' to have the
            assistant speak first with a message generated by the model based on
            the conversation state. (`assistant.model.messages` at call start,
            `call.messages` at squad transfer points).


            @default 'assistant-speaks-first'
        voicemailDetection:
          $ref: '#/components/schemas/AssistantVoicemailDetection'
          description: >-
            These are the settings to configure or disable voicemail detection.
            Alternatively, voicemail detection can be configured using the
            model.tools=[VoicemailTool].

            By default, voicemail detection is disabled.
        clientMessages:
          $ref: '#/components/schemas/AssistantClientMessages'
          description: >-
            These are the messages that will be sent to your Client SDKs.
            Default is
            conversation-update,function-call,hang,model-output,speech-update,status-update,transfer-update,transcript,tool-calls,user-interrupted,voice-input,workflow.node.started,assistant.started.
            You can check the shape of the messages in ClientMessage schema.
        serverMessages:
          $ref: '#/components/schemas/AssistantServerMessages'
          description: >-
            These are the messages that will be sent to your Server URL. Default
            is
            conversation-update,end-of-call-report,function-call,hang,speech-update,status-update,tool-calls,transfer-destination-request,handoff-destination-request,user-interrupted,assistant.started.
            You can check the shape of the messages in ServerMessage schema.
        maxDurationSeconds:
          type: number
          format: double
          description: >-
            This is the maximum number of seconds that the call will last. When
            the call reaches this duration, it will be ended.


            @default 600 (10 minutes)
        backgroundSound:
          $ref: '#/components/schemas/AssistantBackgroundSound'
          description: >-
            This is the background sound in the call. Default for phone calls is
            'office' and default for web calls is 'off'.

            You can also provide a custom sound by providing a URL to an audio
            file.
        modelOutputInMessagesEnabled:
          type: boolean
          description: >-
            This determines whether the model's output is used in conversation
            history rather than the transcription of assistant's speech.


            Default `false` while in beta.


            @default false
        transportConfigurations:
          type: array
          items:
            $ref: '#/components/schemas/AssistantTransportConfigurationsItems'
          description: >-
            These are the configurations to be passed to the transport providers
            of assistant's calls, like Twilio. You can store multiple
            configurations for different transport providers. For a call, only
            the configuration matching the call transport provider is used.
        observabilityPlan:
          $ref: '#/components/schemas/LangfuseObservabilityPlan'
          description: |-
            This is the plan for observability of assistant's calls.

            Currently, only Langfuse is supported.
        credentials:
          type: array
          items:
            $ref: '#/components/schemas/AssistantCredentialsItems'
          description: >-
            These are dynamic credentials that will be used for the assistant
            calls. By default, all the credentials are available for use in the
            call but you can supplement an additional credentials using this.
            Dynamic credentials override existing credentials.
        hooks:
          type: array
          items:
            $ref: '#/components/schemas/AssistantHooksItems'
          description: This is a set of actions that will be performed on certain events.
        name:
          type: string
          description: >-
            This is the name of the assistant.


            This is required when you want to transfer between assistants in a
            call.
        voicemailMessage:
          type: string
          description: >-
            This is the message that the assistant will say if the call is
            forwarded to voicemail.


            If unspecified, it will hang up.
        endCallMessage:
          type: string
          description: |-
            This is the message that the assistant will say if it ends the call.

            If unspecified, it will hang up without saying anything.
        endCallPhrases:
          type: array
          items:
            type: string
          description: >-
            This list contains phrases that, if spoken by the assistant, will
            trigger the call to be hung up. Case insensitive.
        compliancePlan:
          $ref: '#/components/schemas/CompliancePlan'
        metadata:
          $ref: '#/components/schemas/AssistantMetadata'
          description: This is for metadata you want to store on the assistant.
        backgroundSpeechDenoisingPlan:
          $ref: '#/components/schemas/BackgroundSpeechDenoisingPlan'
          description: >-
            This enables filtering of noise and background speech while the user
            is talking.


            Features:

            - Smart denoising using Krisp

            - Fourier denoising


            Smart denoising can be combined with or used independently of
            Fourier denoising.


            Order of precedence:

            - Smart denoising

            - Fourier denoising
        analysisPlan:
          $ref: '#/components/schemas/AnalysisPlan'
          description: >-
            This is the plan for analysis of assistant's calls. Stored in
            `call.analysis`.
        artifactPlan:
          $ref: '#/components/schemas/ArtifactPlan'
          description: >-
            This is the plan for artifacts generated during assistant's calls.
            Stored in `call.artifact`.
        startSpeakingPlan:
          $ref: '#/components/schemas/StartSpeakingPlan'
          description: >-
            This is the plan for when the assistant should start talking.


            You should configure this if you're running into these issues:

            - The assistant is too slow to start talking after the customer is
            done speaking.

            - The assistant is too fast to start talking after the customer is
            done speaking.

            - The assistant is so fast that it's actually interrupting the
            customer.
        stopSpeakingPlan:
          $ref: '#/components/schemas/StopSpeakingPlan'
          description: >-
            This is the plan for when assistant should stop talking on customer
            interruption.


            You should configure this if you're running into these issues:

            - The assistant is too slow to recognize customer's interruption.

            - The assistant is too fast to recognize customer's interruption.

            - The assistant is getting interrupted by phrases that are just
            acknowledgments.

            - The assistant is getting interrupted by background noises.

            - The assistant is not properly stopping -- it starts talking right
            after getting interrupted.
        monitorPlan:
          $ref: '#/components/schemas/MonitorPlan'
          description: >-
            This is the plan for real-time monitoring of the assistant's calls.


            Usage:

            - To enable live listening of the assistant's calls, set
            `monitorPlan.listenEnabled` to `true`.

            - To enable live control of the assistant's calls, set
            `monitorPlan.controlEnabled` to `true`.

            - To attach monitors to the assistant, set `monitorPlan.monitorIds`
            to the set of monitor ids.
        credentialIds:
          type: array
          items:
            type: string
          description: >-
            These are the credentials that will be used for the assistant calls.
            By default, all the credentials are available for use in the call
            but you can provide a subset using this.
        server:
          $ref: '#/components/schemas/Server'
          description: >-
            This is where Vapi will send webhooks. You can find all webhooks
            available along with their shape in ServerMessage schema.


            The order of precedence is:


            1. assistant.server.url

            2. phoneNumber.serverUrl

            3. org.serverUrl
        keypadInputPlan:
          $ref: '#/components/schemas/KeypadInputPlan'
        id:
          type: string
          description: This is the unique identifier for the assistant.
        orgId:
          type: string
          description: >-
            This is the unique identifier for the org that this assistant
            belongs to.
        createdAt:
          type: string
          format: date-time
          description: >-
            This is the ISO 8601 date-time string of when the assistant was
            created.
        updatedAt:
          type: string
          format: date-time
          description: >-
            This is the ISO 8601 date-time string of when the assistant was last
            updated.
      required:
        - id
        - orgId
        - createdAt
        - updatedAt

```

## SDK Code Examples

```python
from vapi import Vapi

client = Vapi(
    token="YOUR_TOKEN_HERE"
)

client.assistants.create()

```